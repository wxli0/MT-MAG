x shape is: (2577, 16384)
features shape is: (2577, 16384)
subtypes shape is: (2577,)
x shape is: (2577, 16384)
y shape is: (2577,)
y_dict is: {'c__Clostridia': 0, 'c__Clostridia_eval': 0, 'c__Clostridia_test': 0, 'c__Clostridia_A': 1, 'c__Clostridia_A_eval': 1, 'c__Clostridia_A_test': 1, 'c__Thermoanaerobacteria': 2, 'c__Thermoanaerobacteria_eval': 2, 'c__Thermoanaerobacteria_test': 2}
x.shape is: (2577, 5) y.len is: 2577
***** iteration: 0 ******
loss: 80.36885833740234
***** iteration: 1 ******
loss: 75.97050476074219
***** iteration: 2 ******
loss: 71.57955169677734
***** iteration: 3 ******
loss: 67.19587707519531
***** iteration: 4 ******
loss: 62.81915283203125
***** iteration: 5 ******
loss: 58.448814392089844
***** iteration: 6 ******
loss: 54.08430480957031
***** iteration: 7 ******
loss: 49.724918365478516
***** iteration: 8 ******
loss: 45.36996841430664
***** iteration: 9 ******
loss: 41.01880645751953
***** iteration: 10 ******
loss: 36.672821044921875
***** iteration: 11 ******
loss: 36.023494720458984
***** iteration: 12 ******
loss: 30.227231979370117
***** iteration: 13 ******
loss: 25.152185440063477
***** iteration: 14 ******
loss: 21.59861946105957
***** iteration: 15 ******
loss: 17.960140228271484
***** iteration: 16 ******
loss: 14.248307228088379
***** iteration: 17 ******
loss: 10.472391128540039
***** iteration: 18 ******
loss: 6.640042781829834
***** iteration: 19 ******
loss: 2.7787630558013916
***** iteration: 20 ******
loss: 2.194272518157959
***** iteration: 21 ******
loss: 3.082689046859741
***** iteration: 22 ******
loss: 3.6820030212402344
***** iteration: 23 ******
loss: 4.032731056213379
***** iteration: 24 ******
loss: 4.161444664001465
***** iteration: 25 ******
loss: 4.094123363494873
***** iteration: 26 ******
loss: 3.8638217449188232
***** iteration: 27 ******
loss: 5.561306476593018
***** iteration: 28 ******
loss: 5.563627243041992
***** iteration: 29 ******
loss: 3.5016226768493652
***** iteration: 30 ******
loss: 3.097951650619507
***** iteration: 31 ******
loss: 2.873711347579956
***** iteration: 32 ******
loss: 2.8543126583099365
***** iteration: 33 ******
loss: 2.1133248805999756
***** iteration: 34 ******
loss: 1.589481234550476
***** iteration: 35 ******
loss: 0.9638591408729553
***** iteration: 36 ******
loss: 2.1445257663726807
***** iteration: 37 ******
loss: 3.651045083999634
***** iteration: 38 ******
loss: 4.5557780265808105
***** iteration: 39 ******
loss: 4.924135684967041
***** iteration: 40 ******
loss: 4.814162254333496
***** iteration: 41 ******
loss: 4.277160167694092
***** iteration: 42 ******
loss: 3.358532190322876
***** iteration: 43 ******
loss: 2.113590717315674
***** iteration: 44 ******
loss: 0.8400049805641174
***** iteration: 45 ******
loss: 0.9246618151664734
***** iteration: 46 ******
loss: 1.306520700454712
***** iteration: 47 ******
loss: 1.5587246417999268
***** iteration: 48 ******
loss: 1.6254085302352905
***** iteration: 49 ******
loss: 1.5254789590835571
***** iteration: 50 ******
loss: 1.2759908437728882
***** iteration: 51 ******
loss: 0.9625136852264404
***** iteration: 52 ******
loss: 0.5824938416481018
***** iteration: 53 ******
loss: 1.2621524333953857
***** iteration: 54 ******
loss: 2.1245899200439453
***** iteration: 55 ******
loss: 2.4669837951660156
***** iteration: 56 ******
loss: 2.3431079387664795
***** iteration: 57 ******
loss: 1.800957441329956
***** iteration: 58 ******
loss: 0.8954724669456482
***** iteration: 59 ******
loss: 0.7311838865280151
***** iteration: 60 ******
loss: 0.9133678078651428
***** iteration: 61 ******
loss: 0.9180062413215637
***** iteration: 62 ******
loss: 0.8192623257637024
***** iteration: 63 ******
loss: 0.7020794153213501
***** iteration: 64 ******
loss: 0.4302896559238434
***** iteration: 65 ******
loss: 0.925028920173645
***** iteration: 66 ******
loss: 1.418419361114502
***** iteration: 67 ******
loss: 1.4319816827774048
***** iteration: 68 ******
loss: 1.014673113822937
***** iteration: 69 ******
loss: 0.40259793400764465
***** iteration: 70 ******
loss: 0.5999439358711243
***** iteration: 71 ******
loss: 0.9495458602905273
***** iteration: 72 ******
loss: 0.9193220734596252
***** iteration: 73 ******
loss: 0.9322097897529602
***** iteration: 74 ******
loss: 0.9026468992233276
***** iteration: 75 ******
loss: 0.6946147680282593
***** iteration: 76 ******
loss: 0.41787850856781006
***** iteration: 77 ******
loss: 0.9532373547554016
***** iteration: 78 ******
loss: 1.4766508340835571
***** iteration: 79 ******
loss: 1.8037813901901245
***** iteration: 80 ******
loss: 1.142940640449524
***** iteration: 81 ******
loss: 0.577396035194397
***** iteration: 82 ******
loss: 0.45999017357826233
***** iteration: 83 ******
loss: 0.6149943470954895
***** iteration: 84 ******
loss: 0.6026126742362976
***** iteration: 85 ******
loss: 0.4419379234313965
***** iteration: 86 ******
loss: 0.356222003698349
***** iteration: 87 ******
loss: 0.7311887741088867
***** iteration: 88 ******
loss: 0.5984428524971008
***** iteration: 89 ******
loss: 0.32164862751960754
***** iteration: 90 ******
loss: 0.4426731467247009
***** iteration: 91 ******
loss: 0.5063765048980713
***** iteration: 92 ******
loss: 0.4196794331073761
***** iteration: 93 ******
loss: 0.3482154607772827
***** iteration: 94 ******
loss: 0.50238037109375
***** iteration: 95 ******
loss: 0.5914489030838013
***** iteration: 96 ******
loss: 0.40810611844062805
***** iteration: 97 ******
loss: 0.4598357081413269
***** iteration: 98 ******
loss: 0.5036051273345947
***** iteration: 99 ******
loss: 0.39861753582954407
***** iteration: 100 ******
loss: 0.49257251620292664
***** iteration: 101 ******
loss: 0.5123635530471802
***** iteration: 102 ******
loss: 0.5363664627075195
***** iteration: 103 ******
loss: 0.37886178493499756
***** iteration: 104 ******
loss: 0.450894296169281
***** iteration: 105 ******
loss: 0.510816216468811
***** iteration: 106 ******
loss: 0.4208125174045563
***** iteration: 107 ******
loss: 0.41095519065856934
***** iteration: 108 ******
loss: 0.46469250321388245
***** iteration: 109 ******
loss: 0.5418903231620789
***** iteration: 110 ******
loss: 0.40369078516960144
***** iteration: 111 ******
loss: 0.4639843702316284
***** iteration: 112 ******
loss: 0.49061691761016846
***** iteration: 113 ******
loss: 0.555768609046936
***** iteration: 114 ******
loss: 0.3567781150341034
***** iteration: 115 ******
loss: 0.4796130359172821
***** iteration: 116 ******
loss: 0.6217117309570312
***** iteration: 117 ******
loss: 0.5058931112289429
***** iteration: 118 ******
loss: 0.5509495139122009
***** iteration: 119 ******
loss: 0.5837659239768982
***** iteration: 120 ******
loss: 0.47157740592956543
***** iteration: 121 ******
loss: 0.3488014340400696
***** iteration: 122 ******
loss: 0.8519890308380127
***** iteration: 123 ******
loss: 1.4032139778137207
***** iteration: 124 ******
loss: 0.622376561164856
***** iteration: 125 ******
loss: 0.37409237027168274
***** iteration: 126 ******
loss: 0.4850808084011078
***** iteration: 127 ******
loss: 0.5223924517631531
***** iteration: 128 ******
loss: 0.43965041637420654
***** iteration: 129 ******
loss: 0.5647118091583252
***** iteration: 130 ******
loss: 0.8660994172096252
***** iteration: 131 ******
loss: 0.7992082834243774
***** iteration: 132 ******
loss: 0.36806371808052063
***** iteration: 133 ******
loss: 0.5596446990966797
***** iteration: 134 ******
loss: 0.6745034456253052
***** iteration: 135 ******
loss: 0.6212201714515686
***** iteration: 136 ******
loss: 0.4279230833053589
***** iteration: 137 ******
loss: 0.6235009431838989
***** iteration: 138 ******
loss: 1.1287351846694946
***** iteration: 139 ******
loss: 1.0419739484786987
***** iteration: 140 ******
loss: 0.5769411325454712
***** iteration: 141 ******
loss: 0.3864605128765106
***** iteration: 142 ******
loss: 0.4662806987762451
***** iteration: 143 ******
loss: 0.3985612392425537
***** iteration: 144 ******
loss: 0.3415391743183136
***** iteration: 145 ******
loss: 0.41226038336753845
***** iteration: 146 ******
loss: 0.334457129240036
***** iteration: 147 ******
loss: 0.36287036538124084
***** iteration: 148 ******
loss: 0.36591416597366333
***** iteration: 149 ******
loss: 0.31834205985069275
***** iteration: 150 ******
loss: 0.36563917994499207
***** iteration: 151 ******
loss: 0.4082903265953064
***** iteration: 152 ******
loss: 0.3229871094226837
***** iteration: 153 ******
loss: 0.29965224862098694
***** iteration: 154 ******
loss: 0.543016791343689
***** iteration: 155 ******
loss: 0.4035015106201172
***** iteration: 156 ******
loss: 0.3585241436958313
***** iteration: 157 ******
loss: 0.4327770173549652
***** iteration: 158 ******
loss: 0.4399387538433075
***** iteration: 159 ******
loss: 0.4356873333454132
***** iteration: 160 ******
loss: 0.3849894106388092
***** iteration: 161 ******
loss: 0.4061709940433502
***** iteration: 162 ******
loss: 0.42432188987731934
***** iteration: 163 ******
loss: 0.37982240319252014
***** iteration: 164 ******
loss: 0.5148224830627441
***** iteration: 165 ******
loss: 0.591620922088623
***** iteration: 166 ******
loss: 0.36948031187057495
***** iteration: 167 ******
loss: 0.450620174407959
***** iteration: 168 ******
loss: 0.5611706972122192
***** iteration: 169 ******
loss: 0.4820335805416107
***** iteration: 170 ******
loss: 0.3515855669975281
***** iteration: 171 ******
loss: 0.7056454420089722
***** iteration: 172 ******
loss: 1.033873200416565
***** iteration: 173 ******
loss: 0.9046260118484497
***** iteration: 174 ******
loss: 0.3836047649383545
***** iteration: 175 ******
loss: 0.42954790592193604
***** iteration: 176 ******
loss: 0.5534191727638245
***** iteration: 177 ******
loss: 0.5219162702560425
***** iteration: 178 ******
loss: 0.37180188298225403
***** iteration: 179 ******
loss: 0.36629655957221985
***** iteration: 180 ******
loss: 0.635951817035675
***** iteration: 181 ******
loss: 0.4956391155719757
***** iteration: 182 ******
loss: 0.7428395748138428
***** iteration: 183 ******
loss: 0.37431800365448
***** iteration: 184 ******
loss: 0.38537833094596863
***** iteration: 185 ******
loss: 0.31381353735923767
***** iteration: 186 ******
loss: 0.6101707816123962
***** iteration: 187 ******
loss: 0.8087545037269592
***** iteration: 188 ******
loss: 0.5957217216491699
***** iteration: 189 ******
loss: 0.5229339599609375
***** iteration: 190 ******
loss: 0.570476770401001
***** iteration: 191 ******
loss: 0.4818113446235657
***** iteration: 192 ******
loss: 0.43856722116470337
***** iteration: 193 ******
loss: 0.899064838886261
***** iteration: 194 ******
loss: 0.7411444187164307
***** iteration: 195 ******
loss: 0.41279488801956177
***** iteration: 196 ******
loss: 0.4910160005092621
***** iteration: 197 ******
loss: 0.5236943364143372
***** iteration: 198 ******
loss: 0.43284133076667786
***** iteration: 199 ******
loss: 0.541591465473175
***** iteration: 200 ******
loss: 1.1903493404388428
***** iteration: 201 ******
loss: 0.7644359469413757
***** iteration: 202 ******
loss: 0.43818798661231995
***** iteration: 203 ******
loss: 0.4963749349117279
***** iteration: 204 ******
loss: 0.5773451924324036
***** iteration: 205 ******
loss: 0.50636225938797
***** iteration: 206 ******
loss: 0.43015024065971375
***** iteration: 207 ******
loss: 0.42409852147102356
***** iteration: 208 ******
loss: 0.3981698751449585
***** iteration: 209 ******
loss: 0.34770581126213074
***** iteration: 210 ******
loss: 0.35775285959243774
***** iteration: 211 ******
loss: 0.39177176356315613
***** iteration: 212 ******
loss: 0.3676493167877197
***** iteration: 213 ******
loss: 0.30925270915031433
***** iteration: 214 ******
loss: 0.43586012721061707
***** iteration: 215 ******
loss: 0.30731910467147827
***** iteration: 216 ******
loss: 0.3156873881816864
***** iteration: 217 ******
loss: 0.37592193484306335
***** iteration: 218 ******
loss: 0.38417068123817444
***** iteration: 219 ******
loss: 0.3422069847583771
***** iteration: 220 ******
loss: 0.4701184034347534
***** iteration: 221 ******
loss: 1.047062873840332
***** iteration: 222 ******
loss: 0.35715389251708984
***** iteration: 223 ******
loss: 0.3776383101940155
***** iteration: 224 ******
loss: 0.4205704927444458
***** iteration: 225 ******
loss: 0.4214872419834137
***** iteration: 226 ******
loss: 0.3919934332370758
***** iteration: 227 ******
loss: 0.3915517330169678
***** iteration: 228 ******
loss: 0.35687199234962463
***** iteration: 229 ******
loss: 0.36102718114852905
***** iteration: 230 ******
loss: 0.4244704842567444
***** iteration: 231 ******
loss: 0.3536909222602844
***** iteration: 232 ******
loss: 0.4497584402561188
***** iteration: 233 ******
loss: 0.4660772383213043
***** iteration: 234 ******
loss: 0.33112892508506775
***** iteration: 235 ******
loss: 0.5363216996192932
***** iteration: 236 ******
loss: 0.9961207509040833
***** iteration: 237 ******
loss: 0.6854233145713806
***** iteration: 238 ******
loss: 0.28362423181533813
***** iteration: 239 ******
loss: 0.45049771666526794
***** iteration: 240 ******
loss: 0.8869746327400208
***** iteration: 241 ******
loss: 0.6332531571388245
***** iteration: 242 ******
loss: 0.5484570264816284
***** iteration: 243 ******
loss: 0.36256837844848633
***** iteration: 244 ******
loss: 0.6567581295967102
***** iteration: 245 ******
loss: 1.050473928451538
***** iteration: 246 ******
loss: 0.9796271920204163
***** iteration: 247 ******
loss: 0.4956545829772949
***** iteration: 248 ******
loss: 0.4575254023075104
***** iteration: 249 ******
loss: 0.5705013275146484
***** iteration: 250 ******
loss: 0.5685895681381226
***** iteration: 251 ******
loss: 0.42018166184425354
***** iteration: 252 ******
loss: 0.5305169820785522
***** iteration: 253 ******
loss: 0.8032525777816772
***** iteration: 254 ******
loss: 0.6295158863067627
***** iteration: 255 ******
loss: 0.3608778417110443
***** iteration: 256 ******
loss: 0.4728448688983917
***** iteration: 257 ******
loss: 0.3718492388725281
***** iteration: 258 ******
loss: 0.3851872384548187
***** iteration: 259 ******
loss: 0.36570635437965393
***** iteration: 260 ******
loss: 0.3754368722438812
***** iteration: 261 ******
loss: 0.3653523325920105
***** iteration: 262 ******
loss: 0.3299078345298767
***** iteration: 263 ******
loss: 0.32355713844299316
***** iteration: 264 ******
loss: 0.3084096312522888
***** iteration: 265 ******
loss: 0.28271862864494324
***** iteration: 266 ******
loss: 0.39958885312080383
***** iteration: 267 ******
loss: 0.35778793692588806
***** iteration: 268 ******
loss: 0.4081461727619171
***** iteration: 269 ******
loss: 0.405805379152298
***** iteration: 270 ******
loss: 1.7784919738769531
***** iteration: 271 ******
loss: 0.33859795331954956
***** iteration: 272 ******
loss: 0.2931390702724457
***** iteration: 273 ******
loss: 0.2999558448791504
***** iteration: 274 ******
loss: 0.32508185505867004
***** iteration: 275 ******
loss: 0.305816113948822
***** iteration: 276 ******
loss: 0.28511956334114075
***** iteration: 277 ******
loss: 0.3450585603713989
***** iteration: 278 ******
loss: 0.34553834795951843
***** iteration: 279 ******
loss: 0.2952837347984314
***** iteration: 280 ******
loss: 0.3449043929576874
***** iteration: 281 ******
loss: 0.356516569852829
***** iteration: 282 ******
loss: 0.3278229832649231
***** iteration: 283 ******
loss: 0.4646977186203003
***** iteration: 284 ******
loss: 0.5092319846153259
***** iteration: 285 ******
loss: 0.31319111585617065
***** iteration: 286 ******
loss: 0.43594419956207275
***** iteration: 287 ******
loss: 0.47497668862342834
***** iteration: 288 ******
loss: 0.35394513607025146
***** iteration: 289 ******
loss: 0.6442887187004089
***** iteration: 290 ******
loss: 0.7471927404403687
***** iteration: 291 ******
loss: 0.7415779829025269
***** iteration: 292 ******
loss: 0.34926944971084595
***** iteration: 293 ******
loss: 0.43278250098228455
***** iteration: 294 ******
loss: 0.520897388458252
***** iteration: 295 ******
loss: 0.44309648871421814
***** iteration: 296 ******
loss: 0.6554387211799622
***** iteration: 297 ******
loss: 0.5347518920898438
***** iteration: 298 ******
loss: 0.6208952069282532
***** iteration: 299 ******
loss: 0.5728285908699036
***** iteration: 300 ******
loss: 0.3377542495727539
***** iteration: 301 ******
loss: 0.3543853759765625
***** iteration: 302 ******
loss: 1.4523965120315552
***** iteration: 303 ******
loss: 1.126825213432312
***** iteration: 304 ******
loss: 0.33894777297973633
***** iteration: 305 ******
loss: 0.45458704233169556
***** iteration: 306 ******
loss: 0.44095855951309204
***** iteration: 307 ******
loss: 0.4039517045021057
***** iteration: 308 ******
loss: 0.3776540458202362
***** iteration: 309 ******
loss: 0.4760133624076843
***** iteration: 310 ******
loss: 0.49351564049720764
***** iteration: 311 ******
loss: 0.37480297684669495
***** iteration: 312 ******
loss: 0.4299180805683136
***** iteration: 313 ******
loss: 0.4529675543308258
***** iteration: 314 ******
loss: 0.34371769428253174
***** iteration: 315 ******
loss: 0.9029849171638489
***** iteration: 316 ******
loss: 1.1218338012695312
***** iteration: 317 ******
loss: 1.0454554557800293
***** iteration: 318 ******
loss: 0.6884065270423889
***** iteration: 319 ******
loss: 0.43074288964271545
***** iteration: 320 ******
loss: 0.5077613592147827
***** iteration: 321 ******
loss: 0.440260112285614
***** iteration: 322 ******
loss: 0.3287506699562073
***** iteration: 323 ******
loss: 0.6891294121742249
***** iteration: 324 ******
loss: 0.7544140219688416
***** iteration: 325 ******
loss: 0.30386313796043396
***** iteration: 326 ******
loss: 0.3659764230251312
***** iteration: 327 ******
loss: 0.3781834542751312
***** iteration: 328 ******
loss: 0.30033114552497864
***** iteration: 329 ******
loss: 0.4895156919956207
***** iteration: 330 ******
loss: 0.7197777032852173
***** iteration: 331 ******
loss: 0.33655089139938354
***** iteration: 332 ******
loss: 0.2999536693096161
***** iteration: 333 ******
loss: 0.29710015654563904
***** iteration: 334 ******
loss: 0.4442831575870514
***** iteration: 335 ******
loss: 0.4796186685562134
***** iteration: 336 ******
loss: 0.2980887293815613
***** iteration: 337 ******
loss: 0.3198593258857727
***** iteration: 338 ******
loss: 0.2989732027053833
***** iteration: 339 ******
loss: 0.3560272455215454
***** iteration: 340 ******
loss: 0.3355567753314972
***** iteration: 341 ******
loss: 0.29833802580833435
***** iteration: 342 ******
loss: 0.9623697400093079
***** iteration: 343 ******
loss: 1.538487434387207
***** iteration: 344 ******
loss: 0.3949631154537201
***** iteration: 345 ******
loss: 0.33902809023857117
***** iteration: 346 ******
loss: 0.4000020921230316
***** iteration: 347 ******
loss: 0.4436003863811493
***** iteration: 348 ******
loss: 0.42566490173339844
***** iteration: 349 ******
loss: 0.3948596715927124
***** iteration: 350 ******
loss: 0.32817980647087097
***** iteration: 351 ******
loss: 0.8797069787979126
***** iteration: 352 ******
loss: 0.5786092281341553
***** iteration: 353 ******
loss: 0.49451905488967896
***** iteration: 354 ******
loss: 0.3092215955257416
***** iteration: 355 ******
loss: 0.501027524471283
***** iteration: 356 ******
loss: 0.5392179489135742
***** iteration: 357 ******
loss: 0.4125503897666931
***** iteration: 358 ******
loss: 0.40213802456855774
***** iteration: 359 ******
loss: 0.46260929107666016
***** iteration: 360 ******
loss: 0.3610518276691437
***** iteration: 361 ******
loss: 0.5230845808982849
***** iteration: 362 ******
loss: 0.4963982403278351
***** iteration: 363 ******
loss: 0.4825219213962555
***** iteration: 364 ******
loss: 0.3874538540840149
***** iteration: 365 ******
loss: 0.870373547077179
***** iteration: 366 ******
loss: 1.0673530101776123
***** iteration: 367 ******
loss: 0.8022369742393494
***** iteration: 368 ******
loss: 0.35934439301490784
***** iteration: 369 ******
loss: 0.4603041410446167
***** iteration: 370 ******
loss: 0.5223503112792969
***** iteration: 371 ******
loss: 0.4207737147808075
***** iteration: 372 ******
loss: 0.41804254055023193
***** iteration: 373 ******
loss: 0.5666473507881165
***** iteration: 374 ******
loss: 0.3670562505722046
***** iteration: 375 ******
loss: 0.3717782199382782
***** iteration: 376 ******
loss: 0.4251065254211426
***** iteration: 377 ******
loss: 0.3444920480251312
***** iteration: 378 ******
loss: 0.44548553228378296
***** iteration: 379 ******
loss: 0.589804470539093
***** iteration: 380 ******
loss: 0.3567149043083191
***** iteration: 381 ******
loss: 0.3298790454864502
***** iteration: 382 ******
loss: 0.3804117739200592
***** iteration: 383 ******
loss: 0.2921934723854065
***** iteration: 384 ******
loss: 0.42846962809562683
***** iteration: 385 ******
loss: 0.6128001809120178
***** iteration: 386 ******
loss: 0.3775431513786316
***** iteration: 387 ******
loss: 0.2871881425380707
***** iteration: 388 ******
loss: 0.3132591247558594
***** iteration: 389 ******
loss: 0.2857665419578552
***** iteration: 390 ******
loss: 0.32530248165130615
***** iteration: 391 ******
loss: 0.3141619563102722
***** iteration: 392 ******
loss: 0.333767294883728
***** iteration: 393 ******
loss: 0.3112744987010956
***** iteration: 394 ******
loss: 0.3617731034755707
***** iteration: 395 ******
loss: 0.3615952432155609
***** iteration: 396 ******
loss: 0.31859102845191956
***** iteration: 397 ******
loss: 0.3277375102043152
***** iteration: 398 ******
loss: 0.32410937547683716
***** iteration: 399 ******
loss: 0.26822569966316223
***** iteration: 400 ******
loss: 0.3180435299873352
***** iteration: 401 ******
loss: 0.2887554168701172
***** iteration: 402 ******
loss: 0.293875128030777
***** iteration: 403 ******
loss: 0.3029244542121887
***** iteration: 404 ******
loss: 0.24031682312488556
***** iteration: 405 ******
loss: 0.5435848236083984
***** iteration: 406 ******
loss: 0.6816977262496948
***** iteration: 407 ******
loss: 0.40477606654167175
***** iteration: 408 ******
loss: 0.28150978684425354
***** iteration: 409 ******
loss: 0.32434409856796265
***** iteration: 410 ******
loss: 0.31112051010131836
***** iteration: 411 ******
loss: 0.2596912682056427
***** iteration: 412 ******
loss: 0.2595880329608917
***** iteration: 413 ******
loss: 0.2739333510398865
***** iteration: 414 ******
loss: 0.27842339873313904
***** iteration: 415 ******
loss: 0.26531487703323364
***** iteration: 416 ******
loss: 0.3148839771747589
***** iteration: 417 ******
loss: 0.2902334928512573
***** iteration: 418 ******
loss: 0.2938337028026581
***** iteration: 419 ******
loss: 0.2683815360069275
***** iteration: 420 ******
loss: 0.2727806568145752
***** iteration: 421 ******
loss: 0.44709983468055725
***** iteration: 422 ******
loss: 0.24994933605194092
***** iteration: 423 ******
loss: 0.35590147972106934
***** iteration: 424 ******
loss: 0.3594585061073303
***** iteration: 425 ******
loss: 0.3800397217273712
***** iteration: 426 ******
loss: 0.36566632986068726
***** iteration: 427 ******
loss: 0.3115203380584717
***** iteration: 428 ******
loss: 0.2762989401817322
***** iteration: 429 ******
loss: 0.6214305758476257
***** iteration: 430 ******
loss: 0.5417617559432983
***** iteration: 431 ******
loss: 0.48035022616386414
***** iteration: 432 ******
loss: 0.33032259345054626
***** iteration: 433 ******
loss: 0.3518773019313812
***** iteration: 434 ******
loss: 0.36889412999153137
***** iteration: 435 ******
loss: 0.2851596474647522
***** iteration: 436 ******
loss: 0.3217647969722748
***** iteration: 437 ******
loss: 1.4519026279449463
***** iteration: 438 ******
loss: 1.2205946445465088
***** iteration: 439 ******
loss: 0.3305303454399109
***** iteration: 440 ******
loss: 0.29891538619995117
***** iteration: 441 ******
loss: 0.6843945384025574
***** iteration: 442 ******
loss: 0.7790272235870361
***** iteration: 443 ******
loss: 0.42762869596481323
***** iteration: 444 ******
loss: 0.3533767759799957
***** iteration: 445 ******
loss: 0.558564305305481
***** iteration: 446 ******
loss: 0.47950518131256104
***** iteration: 447 ******
loss: 0.39184698462486267
***** iteration: 448 ******
loss: 0.3984678089618683
***** iteration: 449 ******
loss: 0.5742682218551636
***** iteration: 450 ******
loss: 0.3668690323829651
***** iteration: 451 ******
loss: 0.36165815591812134
***** iteration: 452 ******
loss: 0.4102831184864044
***** iteration: 453 ******
loss: 0.3190591633319855
***** iteration: 454 ******
loss: 0.5778048038482666
***** iteration: 455 ******
loss: 0.7730764746665955
***** iteration: 456 ******
loss: 0.5614166259765625
***** iteration: 457 ******
loss: 0.3151624798774719
***** iteration: 458 ******
loss: 0.3363967537879944
***** iteration: 459 ******
loss: 0.2917726933956146
***** iteration: 460 ******
loss: 0.42904484272003174
***** iteration: 461 ******
loss: 0.4752472937107086
***** iteration: 462 ******
loss: 0.3488054871559143
***** iteration: 463 ******
loss: 0.33009767532348633
***** iteration: 464 ******
loss: 0.4976961612701416
***** iteration: 465 ******
loss: 0.40354615449905396
***** iteration: 466 ******
loss: 0.2872648537158966
***** iteration: 467 ******
loss: 0.31182312965393066
***** iteration: 468 ******
loss: 1.4113398790359497
***** iteration: 469 ******
loss: 0.2976532280445099
***** iteration: 470 ******
loss: 0.26364508271217346
***** iteration: 471 ******
loss: 1.7004393339157104
***** iteration: 472 ******
loss: 1.958409070968628
***** iteration: 473 ******
loss: 0.36122676730155945
***** iteration: 474 ******
loss: 0.3704982101917267
***** iteration: 475 ******
loss: 0.4532112181186676
***** iteration: 476 ******
loss: 0.4394983649253845
***** iteration: 477 ******
loss: 0.5239519476890564
***** iteration: 478 ******
loss: 0.5018940567970276
***** iteration: 479 ******
loss: 0.33668291568756104
***** iteration: 480 ******
loss: 0.398596853017807
***** iteration: 481 ******
loss: 0.41889533400535583
***** iteration: 482 ******
loss: 0.31072986125946045
***** iteration: 483 ******
loss: 0.40848273038864136
***** iteration: 484 ******
loss: 0.31924811005592346
***** iteration: 485 ******
loss: 0.3497336804866791
***** iteration: 486 ******
loss: 0.45358580350875854
***** iteration: 487 ******
loss: 0.3266275227069855
***** iteration: 488 ******
loss: 0.3033175766468048
***** iteration: 489 ******
loss: 0.3309469223022461
***** iteration: 490 ******
loss: 0.2847960889339447
***** iteration: 491 ******
loss: 0.2612840533256531
***** iteration: 492 ******
loss: 0.2717845141887665
***** iteration: 493 ******
loss: 0.2391275018453598
***** iteration: 494 ******
loss: 0.5826984643936157
***** iteration: 495 ******
loss: 0.5382276177406311
***** iteration: 496 ******
loss: 0.355266809463501
***** iteration: 497 ******
loss: 0.29655903577804565
***** iteration: 498 ******
loss: 0.6532168388366699
***** iteration: 499 ******
loss: 0.8537875413894653
***** iteration: 500 ******
loss: 0.6289470195770264
***** iteration: 501 ******
loss: 0.3104490041732788
***** iteration: 502 ******
loss: 0.39486801624298096
***** iteration: 503 ******
loss: 0.36701875925064087
***** iteration: 504 ******
loss: 0.593087375164032
***** iteration: 505 ******
loss: 0.6367049813270569
***** iteration: 506 ******
loss: 0.4259394109249115
***** iteration: 507 ******
loss: 0.3892042934894562
***** iteration: 508 ******
loss: 0.4164407253265381
***** iteration: 509 ******
loss: 0.3269837200641632
***** iteration: 510 ******
loss: 1.3604798316955566
***** iteration: 511 ******
loss: 1.590071678161621
***** iteration: 512 ******
loss: 1.2594645023345947
***** iteration: 513 ******
loss: 1.043337106704712
***** iteration: 514 ******
loss: 0.4625769853591919
***** iteration: 515 ******
loss: 0.6596309542655945
***** iteration: 516 ******
loss: 0.8259468674659729
***** iteration: 517 ******
loss: 0.8165898323059082
***** iteration: 518 ******
loss: 0.6492010354995728
***** iteration: 519 ******
loss: 0.36004576086997986
***** iteration: 520 ******
loss: 1.0499064922332764
***** iteration: 521 ******
loss: 2.23600172996521
***** iteration: 522 ******
loss: 2.055889844894409
***** iteration: 523 ******
loss: 2.027858257293701
***** iteration: 524 ******
loss: 1.5782020092010498
***** iteration: 525 ******
loss: 0.7178590297698975
***** iteration: 526 ******
loss: 0.5421563982963562
***** iteration: 527 ******
loss: 0.7568889260292053
***** iteration: 528 ******
loss: 0.7917727828025818
***** iteration: 529 ******
loss: 0.6642484664916992
***** iteration: 530 ******
loss: 0.3934630751609802
***** iteration: 531 ******
loss: 0.5185737013816833
***** iteration: 532 ******
loss: 1.29993736743927
***** iteration: 533 ******
loss: 1.2871387004852295
***** iteration: 534 ******
loss: 1.1373212337493896
***** iteration: 535 ******
loss: 0.7950287461280823
***** iteration: 536 ******
loss: 0.3676127791404724
***** iteration: 537 ******
loss: 0.42275270819664
***** iteration: 538 ******
loss: 0.36660853028297424
***** iteration: 539 ******
loss: 0.6315943598747253
***** iteration: 540 ******
loss: 0.6818735599517822
***** iteration: 541 ******
loss: 0.5860030055046082
***** iteration: 542 ******
loss: 0.461357057094574
***** iteration: 543 ******
loss: 0.4547937214374542
***** iteration: 544 ******
loss: 0.5041068196296692
***** iteration: 545 ******
loss: 0.42907166481018066
***** iteration: 546 ******
loss: 0.4096796214580536
***** iteration: 547 ******
loss: 0.6209047436714172
***** iteration: 548 ******
loss: 0.5746739506721497
***** iteration: 549 ******
loss: 0.3449983298778534
***** iteration: 550 ******
loss: 0.3724491596221924
***** iteration: 551 ******
loss: 0.3663443624973297
***** iteration: 552 ******
loss: 0.32538843154907227
***** iteration: 553 ******
loss: 0.3206721246242523
***** iteration: 554 ******
loss: 0.32123687863349915
***** iteration: 555 ******
loss: 0.29007142782211304
***** iteration: 556 ******
loss: 1.0752971172332764
***** iteration: 557 ******
loss: 1.2238494157791138
***** iteration: 558 ******
loss: 0.2590910792350769
***** iteration: 559 ******
loss: 0.3511997163295746
***** iteration: 560 ******
loss: 1.608675241470337
***** iteration: 561 ******
loss: 0.45882582664489746
***** iteration: 562 ******
loss: 0.49428099393844604
***** iteration: 563 ******
loss: 0.46919992566108704
***** iteration: 564 ******
loss: 0.3949871063232422
***** iteration: 565 ******
loss: 0.2619277536869049
***** iteration: 566 ******
loss: 4.513887882232666
***** iteration: 567 ******
loss: 7.515161991119385
***** iteration: 568 ******
loss: 8.106342315673828
***** iteration: 569 ******
loss: 6.531306266784668
***** iteration: 570 ******
loss: 3.0135786533355713
***** iteration: 571 ******
loss: 0.411803275346756
***** iteration: 572 ******
loss: 0.41722843050956726
***** iteration: 573 ******
loss: 0.9876863956451416
***** iteration: 574 ******
loss: 1.3214832544326782
***** iteration: 575 ******
loss: 1.1552104949951172
***** iteration: 576 ******
loss: 0.6741374731063843
***** iteration: 577 ******
loss: 0.9033266305923462
***** iteration: 578 ******
loss: 1.0482555627822876
***** iteration: 579 ******
loss: 1.0183874368667603
***** iteration: 580 ******
loss: 0.8312930464744568
***** iteration: 581 ******
loss: 0.5732326507568359
***** iteration: 582 ******
loss: 1.0576155185699463
***** iteration: 583 ******
loss: 1.344988226890564
***** iteration: 584 ******
loss: 1.1381663084030151
***** iteration: 585 ******
loss: 0.5106607675552368
***** iteration: 586 ******
loss: 0.4536415934562683
***** iteration: 587 ******
loss: 0.8549187183380127
***** iteration: 588 ******
loss: 0.9583104252815247
***** iteration: 589 ******
loss: 0.7460038661956787
***** iteration: 590 ******
loss: 0.689494252204895
***** iteration: 591 ******
loss: 0.4839170277118683
***** iteration: 592 ******
loss: 0.7907235026359558
***** iteration: 593 ******
loss: 1.3129856586456299
***** iteration: 594 ******
loss: 1.320751667022705
***** iteration: 595 ******
loss: 0.8644816875457764
***** iteration: 596 ******
loss: 0.36046916246414185
***** iteration: 597 ******
loss: 0.6736904978752136
***** iteration: 598 ******
loss: 0.6995656490325928
***** iteration: 599 ******
loss: 0.5473223328590393
***** iteration: 600 ******
loss: 0.4704383909702301
***** iteration: 601 ******
loss: 0.4914305806159973
***** iteration: 602 ******
loss: 0.68537437915802
***** iteration: 603 ******
loss: 0.4899262487888336
***** iteration: 604 ******
loss: 0.4291023910045624
***** iteration: 605 ******
loss: 0.43101152777671814
***** iteration: 606 ******
loss: 0.4044736921787262
***** iteration: 607 ******
loss: 0.3607758581638336
***** iteration: 608 ******
loss: 0.37636300921440125
***** iteration: 609 ******
loss: 0.3879360854625702
***** iteration: 610 ******
loss: 0.3706628680229187
***** iteration: 611 ******
loss: 0.33677101135253906
***** iteration: 612 ******
loss: 0.44754624366760254
***** iteration: 613 ******
loss: 0.35145998001098633
***** iteration: 614 ******
loss: 0.32644781470298767
***** iteration: 615 ******
loss: 0.34207215905189514
***** iteration: 616 ******
loss: 0.31288257241249084
***** iteration: 617 ******
loss: 0.2617247402667999
***** iteration: 618 ******
loss: 0.5206575989723206
***** iteration: 619 ******
loss: 0.28255024552345276
***** iteration: 620 ******
loss: 0.28705698251724243
***** iteration: 621 ******
loss: 0.28631266951560974
***** iteration: 622 ******
loss: 0.38248175382614136
***** iteration: 623 ******
loss: 0.3682021200656891
***** iteration: 624 ******
loss: 0.2923206686973572
***** iteration: 625 ******
loss: 0.3041510581970215
***** iteration: 626 ******
loss: 0.29093649983406067
***** iteration: 627 ******
loss: 0.383533239364624
***** iteration: 628 ******
loss: 0.2761388123035431
***** iteration: 629 ******
loss: 0.3057478070259094
***** iteration: 630 ******
loss: 0.30490726232528687
***** iteration: 631 ******
loss: 0.2716536223888397
***** iteration: 632 ******
loss: 0.4940539300441742
***** iteration: 633 ******
loss: 0.543139636516571
***** iteration: 634 ******
loss: 0.2597269117832184
***** iteration: 635 ******
loss: 0.3672662675380707
***** iteration: 636 ******
loss: 0.7550672888755798
***** iteration: 637 ******
loss: 0.4165462553501129
***** iteration: 638 ******
loss: 0.3104686737060547
***** iteration: 639 ******
loss: 0.648023247718811
***** iteration: 640 ******
loss: 1.3252087831497192
***** iteration: 641 ******
loss: 1.0779895782470703
***** iteration: 642 ******
loss: 0.4650268852710724
***** iteration: 643 ******
loss: 0.405466228723526
***** iteration: 644 ******
loss: 0.4897615313529968
***** iteration: 645 ******
loss: 0.4179609417915344
***** iteration: 646 ******
loss: 0.6185444593429565
***** iteration: 647 ******
loss: 0.7399693131446838
***** iteration: 648 ******
loss: 0.4205944240093231
***** iteration: 649 ******
loss: 0.42236092686653137
***** iteration: 650 ******
loss: 0.5050040483474731
***** iteration: 651 ******
loss: 0.4617709219455719
***** iteration: 652 ******
loss: 0.3262649178504944
***** iteration: 653 ******
loss: 0.8679639101028442
***** iteration: 654 ******
loss: 1.263106346130371
***** iteration: 655 ******
loss: 0.9867546558380127
***** iteration: 656 ******
loss: 0.5151256322860718
***** iteration: 657 ******
loss: 0.4481622278690338
***** iteration: 658 ******
loss: 0.5698896646499634
***** iteration: 659 ******
loss: 0.5204271674156189
***** iteration: 660 ******
loss: 0.33532965183258057
***** iteration: 661 ******
loss: 0.7427408695220947
***** iteration: 662 ******
loss: 1.3226161003112793
***** iteration: 663 ******
loss: 1.1579653024673462
***** iteration: 664 ******
loss: 0.7774779796600342
***** iteration: 665 ******
loss: 0.3289632797241211
***** iteration: 666 ******
loss: 0.41770556569099426
***** iteration: 667 ******
loss: 0.43709635734558105
***** iteration: 668 ******
loss: 0.3615783751010895
***** iteration: 669 ******
loss: 0.2701854109764099
***** iteration: 670 ******
loss: 0.3004665970802307
***** iteration: 671 ******
loss: 0.5575037002563477
***** iteration: 672 ******
loss: 0.2989109754562378
***** iteration: 673 ******
loss: 0.29540589451789856
***** iteration: 674 ******
loss: 0.3163823187351227
***** iteration: 675 ******
loss: 0.2958690822124481
***** iteration: 676 ******
loss: 0.4624795615673065
***** iteration: 677 ******
loss: 0.4528445601463318
***** iteration: 678 ******
loss: 0.4463270604610443
***** iteration: 679 ******
loss: 0.3795711100101471
***** iteration: 680 ******
loss: 0.7190031409263611
***** iteration: 681 ******
loss: 0.9657310843467712
***** iteration: 682 ******
loss: 0.7233425974845886
***** iteration: 683 ******
loss: 0.3421948254108429
***** iteration: 684 ******
loss: 0.41863784193992615
***** iteration: 685 ******
loss: 0.5633165240287781
***** iteration: 686 ******
loss: 0.4426615238189697
***** iteration: 687 ******
loss: 0.34823861718177795
***** iteration: 688 ******
loss: 0.6010138988494873
***** iteration: 689 ******
loss: 0.8456335663795471
***** iteration: 690 ******
loss: 0.6047863364219666
***** iteration: 691 ******
loss: 0.27573251724243164
***** iteration: 692 ******
loss: 0.35375404357910156
***** iteration: 693 ******
loss: 0.32564935088157654
***** iteration: 694 ******
loss: 0.3324969708919525
***** iteration: 695 ******
loss: 0.3342103362083435
***** iteration: 696 ******
loss: 0.2836247682571411
***** iteration: 697 ******
loss: 0.3282013535499573
***** iteration: 698 ******
loss: 0.30607762932777405
***** iteration: 699 ******
loss: 0.39170899987220764
***** iteration: 700 ******
loss: 0.5556741952896118
***** iteration: 701 ******
loss: 0.2699413299560547
***** iteration: 702 ******
loss: 0.36838576197624207
***** iteration: 703 ******
loss: 0.45638594031333923
***** iteration: 704 ******
loss: 0.43603572249412537
***** iteration: 705 ******
loss: 0.2861269414424896
***** iteration: 706 ******
loss: 0.6893731951713562
***** iteration: 707 ******
loss: 1.0707184076309204
***** iteration: 708 ******
loss: 0.9538813829421997
***** iteration: 709 ******
loss: 0.4134848415851593
***** iteration: 710 ******
loss: 0.3811168670654297
***** iteration: 711 ******
loss: 0.5094080567359924
***** iteration: 712 ******
loss: 0.4814930558204651
***** iteration: 713 ******
loss: 0.3301270604133606
***** iteration: 714 ******
loss: 0.4831611216068268
***** iteration: 715 ******
loss: 0.7802966237068176
***** iteration: 716 ******
loss: 0.6046260595321655
***** iteration: 717 ******
loss: 0.340119332075119
***** iteration: 718 ******
loss: 0.36370745301246643
***** iteration: 719 ******
loss: 1.8707810640335083
***** iteration: 720 ******
loss: 1.9408420324325562
***** iteration: 721 ******
loss: 0.34026986360549927
***** iteration: 722 ******
loss: 0.35214963555336
***** iteration: 723 ******
loss: 0.5615217089653015
***** iteration: 724 ******
loss: 0.6756504774093628
***** iteration: 725 ******
loss: 0.45874592661857605
***** iteration: 726 ******
loss: 0.5084750652313232
***** iteration: 727 ******
loss: 0.6263230443000793
***** iteration: 728 ******
loss: 0.5453627109527588
***** iteration: 729 ******
loss: 0.39039960503578186
***** iteration: 730 ******
loss: 0.4531521797180176
***** iteration: 731 ******
loss: 1.0753779411315918
***** iteration: 732 ******
loss: 1.1143056154251099
***** iteration: 733 ******
loss: 0.7283605337142944
***** iteration: 734 ******
loss: 0.40202295780181885
***** iteration: 735 ******
loss: 0.5334744453430176
***** iteration: 736 ******
loss: 0.6574450135231018
***** iteration: 737 ******
loss: 0.6101303696632385
***** iteration: 738 ******
loss: 0.41764137148857117
***** iteration: 739 ******
loss: 0.45566731691360474
***** iteration: 740 ******
loss: 1.06741201877594
***** iteration: 741 ******
loss: 0.9405744671821594
***** iteration: 742 ******
loss: 0.6509811282157898
***** iteration: 743 ******
loss: 0.37111717462539673
***** iteration: 744 ******
loss: 0.42661792039871216
***** iteration: 745 ******
loss: 0.7469490170478821
***** iteration: 746 ******
loss: 0.5389636754989624
***** iteration: 747 ******
loss: 0.41711562871932983
***** iteration: 748 ******
loss: 0.3878597319126129
***** iteration: 749 ******
loss: 0.4131069779396057
***** iteration: 750 ******
loss: 0.3332067131996155
***** iteration: 751 ******
loss: 0.47567740082740784
***** iteration: 752 ******
loss: 0.6155098676681519
***** iteration: 753 ******
loss: 0.5115146636962891
***** iteration: 754 ******
loss: 0.45763543248176575
***** iteration: 755 ******
loss: 0.31644997000694275
***** iteration: 756 ******
loss: 0.6123005151748657
***** iteration: 757 ******
loss: 1.025713324546814
***** iteration: 758 ******
loss: 1.4339715242385864
***** iteration: 759 ******
loss: 0.5703859925270081
***** iteration: 760 ******
loss: 0.27747416496276855
***** iteration: 761 ******
loss: 0.40884271264076233
***** iteration: 762 ******
loss: 0.5146188735961914
***** iteration: 763 ******
loss: 0.4715169668197632
***** iteration: 764 ******
loss: 0.34251299500465393
***** iteration: 765 ******
loss: 0.5709061622619629
***** iteration: 766 ******
loss: 0.8960381746292114
***** iteration: 767 ******
loss: 0.7712096571922302
***** iteration: 768 ******
loss: 0.3245520293712616
***** iteration: 769 ******
loss: 0.45540931820869446
***** iteration: 770 ******
loss: 0.5906940698623657
***** iteration: 771 ******
loss: 0.5532304048538208
***** iteration: 772 ******
loss: 0.36753901839256287
***** iteration: 773 ******
loss: 0.5192486643791199
***** iteration: 774 ******
loss: 0.9355541467666626
***** iteration: 775 ******
loss: 0.9213789701461792
***** iteration: 776 ******
loss: 0.5196035504341125
***** iteration: 777 ******
loss: 0.36741402745246887
***** iteration: 778 ******
loss: 0.442359983921051
***** iteration: 779 ******
loss: 0.3706591725349426
***** iteration: 780 ******
loss: 0.45173215866088867
***** iteration: 781 ******
loss: 0.4422603249549866
***** iteration: 782 ******
loss: 0.45051732659339905
***** iteration: 783 ******
loss: 0.3260992467403412
***** iteration: 784 ******
loss: 0.400452196598053
***** iteration: 785 ******
loss: 0.40539994835853577
***** iteration: 786 ******
loss: 0.731155276298523
***** iteration: 787 ******
loss: 0.42322900891304016
***** iteration: 788 ******
loss: 0.30308201909065247
***** iteration: 789 ******
loss: 0.32574909925460815
***** iteration: 790 ******
loss: 0.33459311723709106
***** iteration: 791 ******
loss: 0.28916501998901367
***** iteration: 792 ******
loss: 0.3709709048271179
***** iteration: 793 ******
loss: 0.44497668743133545
***** iteration: 794 ******
loss: 0.36113715171813965
***** iteration: 795 ******
loss: 0.36790966987609863
***** iteration: 796 ******
loss: 0.38859325647354126
***** iteration: 797 ******
loss: 0.28565120697021484
***** iteration: 798 ******
loss: 1.1851732730865479
***** iteration: 799 ******
loss: 1.5747216939926147
***** iteration: 800 ******
loss: 1.1491940021514893
***** iteration: 801 ******
loss: 0.9849759936332703
***** iteration: 802 ******
loss: 0.43515846133232117
***** iteration: 803 ******
loss: 0.5104612708091736
***** iteration: 804 ******
loss: 0.6668681502342224
***** iteration: 805 ******
loss: 0.648629903793335
***** iteration: 806 ******
loss: 0.4730907082557678
***** iteration: 807 ******
loss: 0.27428117394447327
***** iteration: 808 ******
loss: 0.9468615651130676
***** iteration: 809 ******
loss: 1.7057130336761475
***** iteration: 810 ******
loss: 0.7712597250938416
***** iteration: 811 ******
loss: 0.45679306983947754
***** iteration: 812 ******
loss: 0.37763622403144836
***** iteration: 813 ******
loss: 0.4655773937702179
***** iteration: 814 ******
loss: 0.4163149893283844
***** iteration: 815 ******
loss: 0.38975369930267334
***** iteration: 816 ******
loss: 0.38654816150665283
***** iteration: 817 ******
loss: 0.27090319991111755
***** iteration: 818 ******
loss: 0.3273974061012268
***** iteration: 819 ******
loss: 0.30519741773605347
***** iteration: 820 ******
loss: 0.2805480360984802
***** iteration: 821 ******
loss: 0.336831271648407
***** iteration: 822 ******
loss: 0.28052961826324463
***** iteration: 823 ******
loss: 0.33104586601257324
***** iteration: 824 ******
loss: 0.49749183654785156
***** iteration: 825 ******
loss: 0.3727932870388031
***** iteration: 826 ******
loss: 0.31447306275367737
***** iteration: 827 ******
loss: 0.5666093826293945
***** iteration: 828 ******
loss: 0.8013246655464172
***** iteration: 829 ******
loss: 0.5627861618995667
***** iteration: 830 ******
loss: 0.3491297960281372
***** iteration: 831 ******
loss: 0.3603997826576233
***** iteration: 832 ******
loss: 0.38663819432258606
***** iteration: 833 ******
loss: 0.30187302827835083
***** iteration: 834 ******
loss: 0.3438768982887268
***** iteration: 835 ******
loss: 0.38670670986175537
***** iteration: 836 ******
loss: 0.3785915672779083
***** iteration: 837 ******
loss: 0.4037657380104065
***** iteration: 838 ******
loss: 0.3387680947780609
***** iteration: 839 ******
loss: 0.3080004155635834
***** iteration: 840 ******
loss: 0.34033671021461487
***** iteration: 841 ******
loss: 0.28134602308273315
***** iteration: 842 ******
loss: 0.40118783712387085
***** iteration: 843 ******
loss: 0.5149326324462891
***** iteration: 844 ******
loss: 0.29100701212882996
***** iteration: 845 ******
loss: 0.3233709931373596
***** iteration: 846 ******
loss: 0.39236074686050415
***** iteration: 847 ******
loss: 0.3527863621711731
***** iteration: 848 ******
loss: 0.24436745047569275
***** iteration: 849 ******
loss: 0.8091632127761841
***** iteration: 850 ******
loss: 1.053511381149292
***** iteration: 851 ******
loss: 1.0388376712799072
***** iteration: 852 ******
loss: 0.6401346921920776
***** iteration: 853 ******
loss: 0.354805588722229
***** iteration: 854 ******
loss: 0.4422861933708191
***** iteration: 855 ******
loss: 0.3851363956928253
***** iteration: 856 ******
loss: 0.2687032222747803
***** iteration: 857 ******
loss: 0.8935793042182922
***** iteration: 858 ******
loss: 1.4790338277816772
***** iteration: 859 ******
loss: 0.3605673909187317
***** iteration: 860 ******
loss: 0.2972312271595001
***** iteration: 861 ******
loss: 0.36109790205955505
***** iteration: 862 ******
loss: 0.32557010650634766
***** iteration: 863 ******
loss: 0.28403809666633606
***** iteration: 864 ******
loss: 0.65945965051651
***** iteration: 865 ******
loss: 0.36318498849868774
***** iteration: 866 ******
loss: 0.2866339087486267
***** iteration: 867 ******
loss: 0.29352229833602905
***** iteration: 868 ******
loss: 0.26565656065940857
***** iteration: 869 ******
loss: 0.49075818061828613
***** iteration: 870 ******
loss: 0.31968167424201965
***** iteration: 871 ******
loss: 0.31237930059432983
***** iteration: 872 ******
loss: 0.33495786786079407
***** iteration: 873 ******
loss: 0.47275692224502563
***** iteration: 874 ******
loss: 0.40031901001930237
***** iteration: 875 ******
loss: 0.3532576858997345
***** iteration: 876 ******
loss: 0.3294922411441803
***** iteration: 877 ******
loss: 0.7731486558914185
***** iteration: 878 ******
loss: 0.7074870467185974
***** iteration: 879 ******
loss: 0.5318689942359924
***** iteration: 880 ******
loss: 0.3675561249256134
***** iteration: 881 ******
loss: 0.5213589668273926
***** iteration: 882 ******
loss: 0.5977453589439392
***** iteration: 883 ******
loss: 0.5071604251861572
***** iteration: 884 ******
loss: 0.3381769359111786
***** iteration: 885 ******
loss: 0.5724648833274841
***** iteration: 886 ******
loss: 1.2114366292953491
***** iteration: 887 ******
loss: 0.48970574140548706
***** iteration: 888 ******
loss: 0.3284434676170349
***** iteration: 889 ******
loss: 0.35688379406929016
***** iteration: 890 ******
loss: 0.42185741662979126
***** iteration: 891 ******
loss: 0.344745934009552
***** iteration: 892 ******
loss: 0.3403162956237793
***** iteration: 893 ******
loss: 0.3985573351383209
***** iteration: 894 ******
loss: 0.30726441740989685
***** iteration: 895 ******
loss: 0.3172683119773865
***** iteration: 896 ******
loss: 0.33526667952537537
***** iteration: 897 ******
loss: 0.3699321150779724
***** iteration: 898 ******
loss: 0.3505970239639282
***** iteration: 899 ******
loss: 0.2988741993904114
***** iteration: 900 ******
loss: 0.39798426628112793
***** iteration: 901 ******
loss: 0.38256001472473145
***** iteration: 902 ******
loss: 0.29807978868484497
***** iteration: 903 ******
loss: 0.3390361964702606
***** iteration: 904 ******
loss: 0.3747684955596924
***** iteration: 905 ******
loss: 0.36629214882850647
***** iteration: 906 ******
loss: 0.3905456066131592
***** iteration: 907 ******
loss: 0.3500462770462036
***** iteration: 908 ******
loss: 0.5962293744087219
***** iteration: 909 ******
loss: 0.6237307786941528
***** iteration: 910 ******
loss: 0.5002396702766418
***** iteration: 911 ******
loss: 0.36086609959602356
***** iteration: 912 ******
loss: 0.38829565048217773
***** iteration: 913 ******
loss: 0.31533512473106384
***** iteration: 914 ******
loss: 0.6998168230056763
***** iteration: 915 ******
loss: 0.8305271863937378
***** iteration: 916 ******
loss: 0.6557627320289612
***** iteration: 917 ******
loss: 0.34491756558418274
***** iteration: 918 ******
loss: 0.5094074010848999
***** iteration: 919 ******
loss: 0.5903385281562805
***** iteration: 920 ******
loss: 0.504294753074646
***** iteration: 921 ******
loss: 0.45337772369384766
***** iteration: 922 ******
loss: 0.3531084954738617
***** iteration: 923 ******
loss: 0.46674951910972595
***** iteration: 924 ******
loss: 0.3285169303417206
***** iteration: 925 ******
loss: 0.42398691177368164
***** iteration: 926 ******
loss: 0.4655705690383911
***** iteration: 927 ******
loss: 0.34734299778938293
***** iteration: 928 ******
loss: 0.27383458614349365
***** iteration: 929 ******
loss: 0.8397137522697449
***** iteration: 930 ******
loss: 0.5198585391044617
***** iteration: 931 ******
loss: 0.28618910908699036
***** iteration: 932 ******
loss: 0.38214996457099915
***** iteration: 933 ******
loss: 0.4490797817707062
***** iteration: 934 ******
loss: 0.36719033122062683
***** iteration: 935 ******
loss: 0.332857221364975
***** iteration: 936 ******
loss: 0.4818198084831238
***** iteration: 937 ******
loss: 0.3811485767364502
***** iteration: 938 ******
loss: 0.4094695448875427
***** iteration: 939 ******
loss: 0.433498352766037
***** iteration: 940 ******
loss: 0.3288048207759857
***** iteration: 941 ******
loss: 0.6298778057098389
***** iteration: 942 ******
loss: 0.849109411239624
***** iteration: 943 ******
loss: 0.7290746569633484
***** iteration: 944 ******
loss: 0.3616974651813507
***** iteration: 945 ******
loss: 0.4955121874809265
***** iteration: 946 ******
loss: 0.588552713394165
***** iteration: 947 ******
loss: 0.7051140069961548
***** iteration: 948 ******
loss: 0.4903331398963928
***** iteration: 949 ******
loss: 0.35344594717025757
***** iteration: 950 ******
loss: 0.6731535792350769
***** iteration: 951 ******
loss: 1.0659470558166504
***** iteration: 952 ******
loss: 0.9573277831077576
***** iteration: 953 ******
loss: 0.44538047909736633
***** iteration: 954 ******
loss: 0.39486730098724365
***** iteration: 955 ******
loss: 0.4831424951553345
***** iteration: 956 ******
loss: 0.8057501912117004
***** iteration: 957 ******
loss: 0.45357945561408997
***** iteration: 958 ******
loss: 0.3729442358016968
***** iteration: 959 ******
loss: 0.4067170023918152
***** iteration: 960 ******
loss: 0.5275169014930725
***** iteration: 961 ******
loss: 0.41520917415618896
***** iteration: 962 ******
loss: 0.42072951793670654
***** iteration: 963 ******
loss: 0.42699384689331055
***** iteration: 964 ******
loss: 0.30792292952537537
***** iteration: 965 ******
loss: 0.6511128544807434
***** iteration: 966 ******
loss: 1.143588900566101
***** iteration: 967 ******
loss: 1.0039328336715698
***** iteration: 968 ******
loss: 0.8659620881080627
***** iteration: 969 ******
loss: 0.40284818410873413
***** iteration: 970 ******
loss: 0.47225096821784973
***** iteration: 971 ******
loss: 0.6102468967437744
***** iteration: 972 ******
loss: 0.5778084993362427
***** iteration: 973 ******
loss: 0.3937875032424927
***** iteration: 974 ******
loss: 0.2987028658390045
***** iteration: 975 ******
loss: 0.5858327150344849
***** iteration: 976 ******
loss: 0.6005107164382935
***** iteration: 977 ******
loss: 0.32524555921554565
***** iteration: 978 ******
loss: 0.3319827616214752
***** iteration: 979 ******
loss: 0.4257672429084778
***** iteration: 980 ******
loss: 0.38834095001220703
***** iteration: 981 ******
loss: 0.27423885464668274
***** iteration: 982 ******
loss: 0.4554574191570282
***** iteration: 983 ******
loss: 0.7113560438156128
***** iteration: 984 ******
loss: 0.5388560891151428
***** iteration: 985 ******
loss: 0.30198562145233154
***** iteration: 986 ******
loss: 0.5450860857963562
***** iteration: 987 ******
loss: 0.6233877539634705
***** iteration: 988 ******
loss: 0.692817747592926
***** iteration: 989 ******
loss: 0.6126115322113037
***** iteration: 990 ******
loss: 0.3988262116909027
***** iteration: 991 ******
loss: 0.5744695067405701
***** iteration: 992 ******
loss: 1.0491242408752441
***** iteration: 993 ******
loss: 1.1661100387573242
***** iteration: 994 ******
loss: 0.7427147030830383
***** iteration: 995 ******
loss: 0.3220627009868622
***** iteration: 996 ******
loss: 0.47238487005233765
***** iteration: 997 ******
loss: 0.6889254450798035
***** iteration: 998 ******
loss: 0.7093827724456787
***** iteration: 999 ******
loss: 0.655641496181488
***** iteration: 1000 ******
loss: 0.4496232271194458
***** iteration: 1001 ******
loss: 0.32354700565338135
***** iteration: 1002 ******
loss: 0.7485589981079102
***** iteration: 1003 ******
loss: 1.007072925567627
***** iteration: 1004 ******
loss: 0.7803615927696228
***** iteration: 1005 ******
loss: 0.3668949604034424
***** iteration: 1006 ******
loss: 0.45676106214523315
***** iteration: 1007 ******
loss: 0.5765209197998047
***** iteration: 1008 ******
loss: 0.5948056578636169
***** iteration: 1009 ******
loss: 0.500388503074646
***** iteration: 1010 ******
loss: 0.34216180443763733
***** iteration: 1011 ******
loss: 0.4068598747253418
***** iteration: 1012 ******
loss: 0.7431241869926453
***** iteration: 1013 ******
loss: 0.7023937106132507
***** iteration: 1014 ******
loss: 0.3505934774875641
***** iteration: 1015 ******
loss: 0.3510889708995819
***** iteration: 1016 ******
loss: 0.4443981349468231
***** iteration: 1017 ******
loss: 0.4600810706615448
***** iteration: 1018 ******
loss: 0.3531288206577301
***** iteration: 1019 ******
loss: 0.3092719316482544
***** iteration: 1020 ******
loss: 0.3445906639099121
***** iteration: 1021 ******
loss: 0.29569560289382935
***** iteration: 1022 ******
loss: 0.2855900228023529
***** iteration: 1023 ******
loss: 0.3397369980812073
***** iteration: 1024 ******
loss: 0.5006449222564697
***** iteration: 1025 ******
loss: 0.31346312165260315
***** iteration: 1026 ******
loss: 0.28130239248275757
***** iteration: 1027 ******
loss: 2.311326742172241
***** iteration: 1028 ******
loss: 2.7829806804656982
***** iteration: 1029 ******
loss: 1.1645381450653076
***** iteration: 1030 ******
loss: 0.5699911117553711
***** iteration: 1031 ******
loss: 0.5684415102005005
***** iteration: 1032 ******
loss: 0.5185768008232117
***** iteration: 1033 ******
loss: 0.6053942441940308
***** iteration: 1034 ******
loss: 0.5369283556938171
***** iteration: 1035 ******
loss: 0.5103623867034912
***** iteration: 1036 ******
loss: 0.47663629055023193
***** iteration: 1037 ******
loss: 0.34163135290145874
***** iteration: 1038 ******
loss: 2.6438748836517334
***** iteration: 1039 ******
loss: 4.423441410064697
***** iteration: 1040 ******
loss: 3.946589469909668
***** iteration: 1041 ******
loss: 1.4323062896728516
***** iteration: 1042 ******
loss: 1.0345768928527832
***** iteration: 1043 ******
loss: 0.6218438148498535
***** iteration: 1044 ******
loss: 0.6917616128921509
***** iteration: 1045 ******
loss: 0.8869710564613342
***** iteration: 1046 ******
loss: 0.9065889716148376
***** iteration: 1047 ******
loss: 0.7721052169799805
***** iteration: 1048 ******
loss: 0.785733699798584
***** iteration: 1049 ******
loss: 1.0271810293197632
***** iteration: 1050 ******
loss: 0.8504807949066162
***** iteration: 1051 ******
loss: 0.6065298318862915
***** iteration: 1052 ******
loss: 0.5792239308357239
***** iteration: 1053 ******
loss: 0.45511800050735474
***** iteration: 1054 ******
loss: 0.29521775245666504
***** iteration: 1055 ******
loss: 2.4075205326080322
***** iteration: 1056 ******
loss: 4.36832857131958
***** iteration: 1057 ******
loss: 4.068607807159424
***** iteration: 1058 ******
loss: 2.4464216232299805
***** iteration: 1059 ******
loss: 0.26017850637435913
***** iteration: 1060 ******
loss: 0.34401434659957886
***** iteration: 1061 ******
loss: 0.3495599627494812
***** iteration: 1062 ******
loss: 0.3762211203575134
***** iteration: 1063 ******
loss: 0.33777177333831787
***** iteration: 1064 ******
loss: 0.30512920022010803
***** iteration: 1065 ******
loss: 0.31934911012649536
***** iteration: 1066 ******
loss: 0.2827650010585785
***** iteration: 1067 ******
loss: 0.3973332643508911
***** iteration: 1068 ******
loss: 0.39062240719795227
***** iteration: 1069 ******
loss: 0.266064316034317
***** iteration: 1070 ******
loss: 0.25603416562080383
***** iteration: 1071 ******
loss: 0.4173835515975952
***** iteration: 1072 ******
loss: 0.37210726737976074
***** iteration: 1073 ******
loss: 0.2709948718547821
***** iteration: 1074 ******
loss: 0.3306891918182373
***** iteration: 1075 ******
loss: 0.31308576464653015
***** iteration: 1076 ******
loss: 0.43807971477508545
***** iteration: 1077 ******
loss: 0.4868107736110687
***** iteration: 1078 ******
loss: 0.34186694025993347
***** iteration: 1079 ******
loss: 0.37734848260879517
***** iteration: 1080 ******
loss: 0.3992477059364319
***** iteration: 1081 ******
loss: 0.3819381594657898
***** iteration: 1082 ******
loss: 0.3662000000476837
***** iteration: 1083 ******
loss: 0.35508695244789124
***** iteration: 1084 ******
loss: 0.533080518245697
***** iteration: 1085 ******
loss: 0.3843168616294861
***** iteration: 1086 ******
loss: 0.3476707935333252
***** iteration: 1087 ******
loss: 0.35291972756385803
***** iteration: 1088 ******
loss: 0.31286442279815674
***** iteration: 1089 ******
loss: 1.1658055782318115
***** iteration: 1090 ******
loss: 1.2721449136734009
***** iteration: 1091 ******
loss: 0.4124836027622223
***** iteration: 1092 ******
loss: 0.3292348086833954
***** iteration: 1093 ******
loss: 0.3372684717178345
***** iteration: 1094 ******
loss: 0.33294516801834106
***** iteration: 1095 ******
loss: 0.26304006576538086
***** iteration: 1096 ******
loss: 0.3166613280773163
***** iteration: 1097 ******
loss: 0.28547337651252747
***** iteration: 1098 ******
loss: 0.3181603252887726
***** iteration: 1099 ******
loss: 0.3243088126182556
***** iteration: 1100 ******
loss: 0.23479478061199188
***** iteration: 1101 ******
loss: 1.7217583656311035
***** iteration: 1102 ******
loss: 0.2685568332672119
***** iteration: 1103 ******
loss: 0.26973605155944824
***** iteration: 1104 ******
loss: 0.32144686579704285
***** iteration: 1105 ******
loss: 0.2920859754085541
***** iteration: 1106 ******
loss: 0.4528779685497284
***** iteration: 1107 ******
loss: 0.5980252623558044
***** iteration: 1108 ******
loss: 0.30004680156707764
***** iteration: 1109 ******
loss: 0.40344592928886414
***** iteration: 1110 ******
loss: 0.45973995327949524
***** iteration: 1111 ******
loss: 0.3546674847602844
***** iteration: 1112 ******
loss: 0.479890376329422
***** iteration: 1113 ******
loss: 0.6842102408409119
***** iteration: 1114 ******
loss: 0.5795292258262634
***** iteration: 1115 ******
loss: 0.31551381945610046
***** iteration: 1116 ******
loss: 0.3560166358947754
***** iteration: 1117 ******
loss: 0.4586661458015442
***** iteration: 1118 ******
loss: 0.29775941371917725
***** iteration: 1119 ******
loss: 0.40693074464797974
***** iteration: 1120 ******
loss: 0.4427310824394226
***** iteration: 1121 ******
loss: 0.26491308212280273
***** iteration: 1122 ******
loss: 1.1467808485031128
***** iteration: 1123 ******
loss: 1.166356086730957
***** iteration: 1124 ******
loss: 0.5870399475097656
***** iteration: 1125 ******
loss: 0.581024706363678
***** iteration: 1126 ******
loss: 0.4174841046333313
***** iteration: 1127 ******
loss: 0.4103853702545166
***** iteration: 1128 ******
loss: 0.751593291759491
***** iteration: 1129 ******
loss: 0.684470534324646
***** iteration: 1130 ******
loss: 0.28318285942077637
***** iteration: 1131 ******
loss: 0.37019577622413635
***** iteration: 1132 ******
loss: 0.48961737751960754
***** iteration: 1133 ******
loss: 0.4797053039073944
***** iteration: 1134 ******
loss: 0.34543317556381226
***** iteration: 1135 ******
loss: 0.4473370909690857
***** iteration: 1136 ******
loss: 0.7413178086280823
***** iteration: 1137 ******
loss: 0.6011571884155273
***** iteration: 1138 ******
loss: 0.2725764811038971
***** iteration: 1139 ******
loss: 0.4331722557544708
***** iteration: 1140 ******
loss: 0.5276311039924622
***** iteration: 1141 ******
loss: 0.4573460519313812
***** iteration: 1142 ******
loss: 0.36084750294685364
***** iteration: 1143 ******
loss: 0.3920707702636719
***** iteration: 1144 ******
loss: 0.26361072063446045
***** iteration: 1145 ******
loss: 1.439247727394104
***** iteration: 1146 ******
loss: 1.7841581106185913
***** iteration: 1147 ******
loss: 0.6346349716186523
***** iteration: 1148 ******
loss: 0.7253977060317993
***** iteration: 1149 ******
loss: 0.7487547397613525
***** iteration: 1150 ******
loss: 0.6150856018066406
***** iteration: 1151 ******
loss: 0.5350343585014343
***** iteration: 1152 ******
loss: 0.7733510136604309
***** iteration: 1153 ******
loss: 0.7002025246620178
***** iteration: 1154 ******
loss: 0.5470672845840454
***** iteration: 1155 ******
loss: 0.529254674911499
***** iteration: 1156 ******
loss: 0.4183098077774048
***** iteration: 1157 ******
loss: 0.39998164772987366
***** iteration: 1158 ******
loss: 2.6203668117523193
***** iteration: 1159 ******
loss: 3.7659122943878174
***** iteration: 1160 ******
loss: 2.778001546859741
***** iteration: 1161 ******
loss: 0.3871109187602997
***** iteration: 1162 ******
loss: 0.4264627993106842
***** iteration: 1163 ******
loss: 0.5113091468811035
***** iteration: 1164 ******
loss: 0.5454577207565308
***** iteration: 1165 ******
loss: 0.5143260359764099
***** iteration: 1166 ******
loss: 0.4335203170776367
***** iteration: 1167 ******
loss: 0.3223547041416168
***** iteration: 1168 ******
loss: 1.6408144235610962
***** iteration: 1169 ******
loss: 3.321287155151367
***** iteration: 1170 ******
loss: 2.837205171585083
***** iteration: 1171 ******
loss: 0.5925670266151428
***** iteration: 1172 ******
loss: 0.31658247113227844
***** iteration: 1173 ******
loss: 0.4522555470466614
***** iteration: 1174 ******
loss: 0.4897204637527466
***** iteration: 1175 ******
loss: 0.848955512046814
***** iteration: 1176 ******
loss: 0.6997373700141907
***** iteration: 1177 ******
loss: 0.6595516800880432
***** iteration: 1178 ******
loss: 0.5188479423522949
***** iteration: 1179 ******
loss: 0.5113915205001831
***** iteration: 1180 ******
loss: 0.4562065005302429
***** iteration: 1181 ******
loss: 0.37436407804489136
***** iteration: 1182 ******
loss: 0.3840673565864563
***** iteration: 1183 ******
loss: 1.1152558326721191
***** iteration: 1184 ******
loss: 0.7761762738227844
***** iteration: 1185 ******
loss: 0.3117578625679016
***** iteration: 1186 ******
loss: 0.5076642632484436
***** iteration: 1187 ******
loss: 0.5497265458106995
***** iteration: 1188 ******
loss: 0.43714699149131775
***** iteration: 1189 ******
loss: 0.43304887413978577
***** iteration: 1190 ******
loss: 0.4196133613586426
***** iteration: 1191 ******
loss: 0.3263208270072937
***** iteration: 1192 ******
loss: 0.48135828971862793
***** iteration: 1193 ******
loss: 0.6767277717590332
***** iteration: 1194 ******
loss: 0.47591325640678406
***** iteration: 1195 ******
loss: 0.3943557143211365
***** iteration: 1196 ******
loss: 0.4587419033050537
***** iteration: 1197 ******
loss: 0.6676990389823914
***** iteration: 1198 ******
loss: 0.46539583802223206
***** iteration: 1199 ******
loss: 0.43952539563179016
***** iteration: 1200 ******
loss: 0.4709048271179199
***** iteration: 1201 ******
loss: 0.35322239995002747
***** iteration: 1202 ******
loss: 0.4025631546974182
***** iteration: 1203 ******
loss: 0.6052420139312744
***** iteration: 1204 ******
loss: 0.6229701638221741
***** iteration: 1205 ******
loss: 0.31959268450737
***** iteration: 1206 ******
loss: 0.3425882160663605
***** iteration: 1207 ******
loss: 0.36128881573677063
***** iteration: 1208 ******
loss: 0.5230588316917419
***** iteration: 1209 ******
loss: 0.5539081692695618
***** iteration: 1210 ******
loss: 0.3674084544181824
***** iteration: 1211 ******
loss: 0.42430180311203003
***** iteration: 1212 ******
loss: 0.5523117184638977
***** iteration: 1213 ******
loss: 0.5984261631965637
***** iteration: 1214 ******
loss: 0.34742510318756104
***** iteration: 1215 ******
loss: 0.37298905849456787
***** iteration: 1216 ******
loss: 0.4821144640445709
***** iteration: 1217 ******
loss: 0.36258718371391296
***** iteration: 1218 ******
loss: 0.43451395630836487
***** iteration: 1219 ******
loss: 0.43508994579315186
***** iteration: 1220 ******
loss: 0.4393802285194397
***** iteration: 1221 ******
loss: 0.327318012714386
***** iteration: 1222 ******
loss: 0.4046262204647064
***** iteration: 1223 ******
loss: 0.37478718161582947
***** iteration: 1224 ******
loss: 0.43486571311950684
***** iteration: 1225 ******
loss: 0.46203315258026123
***** iteration: 1226 ******
loss: 0.3620527684688568
***** iteration: 1227 ******
loss: 0.5422623157501221
***** iteration: 1228 ******
loss: 0.6617098450660706
***** iteration: 1229 ******
loss: 0.49218863248825073
***** iteration: 1230 ******
loss: 0.3713642954826355
***** iteration: 1231 ******
loss: 0.3779577314853668
***** iteration: 1232 ******
loss: 0.404638409614563
***** iteration: 1233 ******
loss: 0.31294578313827515
***** iteration: 1234 ******
loss: 0.5894368290901184
***** iteration: 1235 ******
loss: 0.7701270580291748
***** iteration: 1236 ******
loss: 0.5906647443771362
***** iteration: 1237 ******
loss: 0.2919008731842041
***** iteration: 1238 ******
loss: 0.39102843403816223
***** iteration: 1239 ******
loss: 0.4425865411758423
***** iteration: 1240 ******
loss: 0.35723477602005005
***** iteration: 1241 ******
loss: 0.5726747512817383
***** iteration: 1242 ******
loss: 0.8016369938850403
***** iteration: 1243 ******
loss: 0.5784091949462891
***** iteration: 1244 ******
loss: 0.2734470069408417
***** iteration: 1245 ******
loss: 0.6824700832366943
***** iteration: 1246 ******
loss: 1.1386544704437256
***** iteration: 1247 ******
loss: 0.6351959109306335
***** iteration: 1248 ******
loss: 0.6577087044715881
***** iteration: 1249 ******
loss: 0.5688267350196838
***** iteration: 1250 ******
loss: 0.3985859453678131
***** iteration: 1251 ******
loss: 0.8347952365875244
***** iteration: 1252 ******
loss: 1.1192071437835693
***** iteration: 1253 ******
loss: 0.9877773523330688
***** iteration: 1254 ******
loss: 0.45507633686065674
***** iteration: 1255 ******
loss: 0.3179665803909302
***** iteration: 1256 ******
loss: 0.4379824995994568
***** iteration: 1257 ******
loss: 0.43075206875801086
***** iteration: 1258 ******
loss: 0.3131730556488037
***** iteration: 1259 ******
loss: 0.5671144127845764
***** iteration: 1260 ******
loss: 0.9084258675575256
***** iteration: 1261 ******
loss: 0.7320131063461304
***** iteration: 1262 ******
loss: 0.3257908523082733
***** iteration: 1263 ******
loss: 0.46207356452941895
***** iteration: 1264 ******
loss: 0.591302216053009
***** iteration: 1265 ******
loss: 0.5766778588294983
***** iteration: 1266 ******
loss: 0.4197841286659241
***** iteration: 1267 ******
loss: 0.32887741923332214
***** iteration: 1268 ******
loss: 0.5003337264060974
***** iteration: 1269 ******
loss: 0.40923842787742615
***** iteration: 1270 ******
loss: 0.29263466596603394
***** iteration: 1271 ******
loss: 0.2973646819591522
***** iteration: 1272 ******
loss: 0.30374839901924133
***** iteration: 1273 ******
loss: 0.30697181820869446
***** iteration: 1274 ******
loss: 0.28188952803611755
***** iteration: 1275 ******
loss: 0.260678231716156
***** iteration: 1276 ******
loss: 0.24277611076831818
***** iteration: 1277 ******
loss: 1.4255170822143555
***** iteration: 1278 ******
loss: 0.550050675868988
***** iteration: 1279 ******
loss: 0.25189927220344543
***** iteration: 1280 ******
loss: 0.2453407645225525
***** iteration: 1281 ******
loss: 0.749333918094635
***** iteration: 1282 ******
loss: 0.49966534972190857
***** iteration: 1283 ******
loss: 0.37479305267333984
***** iteration: 1284 ******
loss: 0.37977489829063416
***** iteration: 1285 ******
loss: 0.43183374404907227
***** iteration: 1286 ******
loss: 0.35058507323265076
***** iteration: 1287 ******
loss: 0.5431190729141235
***** iteration: 1288 ******
loss: 0.6493294835090637
***** iteration: 1289 ******
loss: 0.6589329838752747
***** iteration: 1290 ******
loss: 0.26813408732414246
***** iteration: 1291 ******
loss: 0.3719404637813568
***** iteration: 1292 ******
loss: 0.38192954659461975
***** iteration: 1293 ******
loss: 0.48508498072624207
***** iteration: 1294 ******
loss: 0.5280419588088989
***** iteration: 1295 ******
loss: 0.3758001923561096
***** iteration: 1296 ******
loss: 0.36389869451522827
***** iteration: 1297 ******
loss: 0.25126978754997253
***** iteration: 1298 ******
loss: 2.2020623683929443
***** iteration: 1299 ******
loss: 3.8124125003814697
***** iteration: 1300 ******
loss: 2.8734378814697266
***** iteration: 1301 ******
loss: 1.009029746055603
***** iteration: 1302 ******
loss: 0.7124475836753845
***** iteration: 1303 ******
loss: 0.4153095483779907
***** iteration: 1304 ******
loss: 0.5274911522865295
***** iteration: 1305 ******
loss: 0.5062591433525085
***** iteration: 1306 ******
loss: 0.48423004150390625
***** iteration: 1307 ******
loss: 0.47396767139434814
***** iteration: 1308 ******
loss: 0.39226198196411133
***** iteration: 1309 ******
loss: 0.3199767470359802
***** iteration: 1310 ******
loss: 0.8807387948036194
***** iteration: 1311 ******
loss: 1.912583589553833
***** iteration: 1312 ******
loss: 0.9587262272834778
***** iteration: 1313 ******
loss: 0.5205509662628174
***** iteration: 1314 ******
loss: 0.3731110394001007
***** iteration: 1315 ******
loss: 0.4265769124031067
***** iteration: 1316 ******
loss: 0.3730011582374573
***** iteration: 1317 ******
loss: 0.43957045674324036
***** iteration: 1318 ******
loss: 0.4154190123081207
***** iteration: 1319 ******
loss: 0.4056156277656555
***** iteration: 1320 ******
loss: 0.3471294939517975
***** iteration: 1321 ******
loss: 0.37485381960868835
***** iteration: 1322 ******
loss: 0.33200517296791077
***** iteration: 1323 ******
loss: 0.5465036034584045
***** iteration: 1324 ******
loss: 0.723976194858551
***** iteration: 1325 ******
loss: 0.46430620551109314
***** iteration: 1326 ******
loss: 0.3392060101032257
***** iteration: 1327 ******
loss: 0.35897088050842285
***** iteration: 1328 ******
loss: 0.2651676535606384
***** iteration: 1329 ******
loss: 0.4055612087249756
***** iteration: 1330 ******
loss: 0.5045793056488037
***** iteration: 1331 ******
loss: 0.3228241205215454
***** iteration: 1332 ******
loss: 0.3133556842803955
***** iteration: 1333 ******
loss: 0.3520306348800659
***** iteration: 1334 ******
loss: 0.27452895045280457
***** iteration: 1335 ******
loss: 0.3000069856643677
***** iteration: 1336 ******
loss: 0.3501918613910675
***** iteration: 1337 ******
loss: 0.24311134219169617
***** iteration: 1338 ******
loss: 0.5168709754943848
***** iteration: 1339 ******
loss: 0.5177352428436279
***** iteration: 1340 ******
loss: 0.5366310477256775
***** iteration: 1341 ******
loss: 0.48419642448425293
***** iteration: 1342 ******
loss: 0.3336819112300873
***** iteration: 1343 ******
loss: 0.7588226199150085
***** iteration: 1344 ******
loss: 1.0617551803588867
***** iteration: 1345 ******
loss: 0.8942574858665466
***** iteration: 1346 ******
loss: 0.526152491569519
***** iteration: 1347 ******
loss: 0.2875799238681793
***** iteration: 1348 ******
loss: 0.38870474696159363
***** iteration: 1349 ******
loss: 0.3498772084712982
***** iteration: 1350 ******
loss: 0.3381931185722351
***** iteration: 1351 ******
loss: 0.3726332485675812
***** iteration: 1352 ******
loss: 0.248089998960495
***** iteration: 1353 ******
loss: 1.1773576736450195
***** iteration: 1354 ******
loss: 1.2570124864578247
***** iteration: 1355 ******
loss: 0.6668868064880371
***** iteration: 1356 ******
loss: 0.6920056343078613
***** iteration: 1357 ******
loss: 0.5650161504745483
***** iteration: 1358 ******
loss: 0.32624948024749756
***** iteration: 1359 ******
loss: 0.8693719506263733
***** iteration: 1360 ******
loss: 1.4378483295440674
***** iteration: 1361 ******
loss: 1.5146108865737915
***** iteration: 1362 ******
loss: 1.1924960613250732
***** iteration: 1363 ******
loss: 0.42758241295814514
***** iteration: 1364 ******
loss: 0.3940616846084595
***** iteration: 1365 ******
loss: 0.5697577595710754
***** iteration: 1366 ******
loss: 0.5729500651359558
***** iteration: 1367 ******
loss: 0.5038655400276184
***** iteration: 1368 ******
loss: 0.2953128516674042
***** iteration: 1369 ******
loss: 0.5498325228691101
***** iteration: 1370 ******
loss: 0.9133527278900146
***** iteration: 1371 ******
loss: 0.7892085313796997
***** iteration: 1372 ******
loss: 0.333687961101532
***** iteration: 1373 ******
loss: 0.3590518832206726
***** iteration: 1374 ******
loss: 0.4822000563144684
***** iteration: 1375 ******
loss: 0.4506867229938507
***** iteration: 1376 ******
loss: 0.2975994348526001
***** iteration: 1377 ******
loss: 0.3978314697742462
***** iteration: 1378 ******
loss: 0.728158175945282
***** iteration: 1379 ******
loss: 0.6153873205184937
***** iteration: 1380 ******
loss: 0.29137590527534485
***** iteration: 1381 ******
loss: 0.37842920422554016
***** iteration: 1382 ******
loss: 0.5252721309661865
***** iteration: 1383 ******
loss: 0.5311352610588074
***** iteration: 1384 ******
loss: 0.39706650376319885
***** iteration: 1385 ******
loss: 0.2569565773010254
***** iteration: 1386 ******
loss: 0.6483109593391418
***** iteration: 1387 ******
loss: 0.7233363389968872
***** iteration: 1388 ******
loss: 0.6274523735046387
***** iteration: 1389 ******
loss: 0.36312395334243774
***** iteration: 1390 ******
loss: 0.43403664231300354
***** iteration: 1391 ******
loss: 0.5372827649116516
***** iteration: 1392 ******
loss: 0.47799286246299744
***** iteration: 1393 ******
loss: 0.29409897327423096
***** iteration: 1394 ******
loss: 1.426269769668579
***** iteration: 1395 ******
loss: 1.9875138998031616
***** iteration: 1396 ******
loss: 1.575715184211731
***** iteration: 1397 ******
loss: 1.5945008993148804
***** iteration: 1398 ******
loss: 1.1499241590499878
***** iteration: 1399 ******
loss: 0.45676660537719727
***** iteration: 1400 ******
loss: 0.5767669081687927
***** iteration: 1401 ******
loss: 0.7784624099731445
***** iteration: 1402 ******
loss: 0.8028073310852051
***** iteration: 1403 ******
loss: 0.6669199466705322
***** iteration: 1404 ******
loss: 0.39069443941116333
***** iteration: 1405 ******
loss: 1.1593348979949951
***** iteration: 1406 ******
loss: 1.8194451332092285
***** iteration: 1407 ******
loss: 1.6542394161224365
***** iteration: 1408 ******
loss: 1.8491288423538208
***** iteration: 1409 ******
loss: 1.5711092948913574
***** iteration: 1410 ******
loss: 0.8715855479240417
***** iteration: 1411 ******
loss: 0.43844908475875854
***** iteration: 1412 ******
loss: 0.583363950252533
***** iteration: 1413 ******
loss: 0.6319400668144226
***** iteration: 1414 ******
loss: 0.518875002861023
***** iteration: 1415 ******
loss: 0.8132140636444092
***** iteration: 1416 ******
loss: 0.9009820818901062
***** iteration: 1417 ******
loss: 0.3791773319244385
***** iteration: 1418 ******
loss: 0.44173935055732727
***** iteration: 1419 ******
loss: 0.3991687297821045
***** iteration: 1420 ******
loss: 0.4669691026210785
***** iteration: 1421 ******
loss: 0.4925292730331421
***** iteration: 1422 ******
loss: 0.3798397183418274
***** iteration: 1423 ******
loss: 0.3090227544307709
***** iteration: 1424 ******
loss: 0.40855109691619873
***** iteration: 1425 ******
loss: 0.3682318329811096
***** iteration: 1426 ******
loss: 0.2952997386455536
***** iteration: 1427 ******
loss: 0.33472710847854614
***** iteration: 1428 ******
loss: 0.4286387860774994
***** iteration: 1429 ******
loss: 0.3120217025279999
***** iteration: 1430 ******
loss: 0.397854208946228
***** iteration: 1431 ******
loss: 0.4471110999584198
***** iteration: 1432 ******
loss: 0.40738722681999207
***** iteration: 1433 ******
loss: 0.3821403980255127
***** iteration: 1434 ******
loss: 0.36182236671447754
***** iteration: 1435 ******
loss: 0.3965153694152832
***** iteration: 1436 ******
loss: 0.367267370223999
***** iteration: 1437 ******
loss: 0.3733488619327545
***** iteration: 1438 ******
loss: 0.35576698184013367
***** iteration: 1439 ******
loss: 0.33572837710380554
***** iteration: 1440 ******
loss: 0.8265453577041626
***** iteration: 1441 ******
loss: 0.48995763063430786
***** iteration: 1442 ******
loss: 0.3321881592273712
***** iteration: 1443 ******
loss: 0.3534116744995117
***** iteration: 1444 ******
loss: 0.33724841475486755
***** iteration: 1445 ******
loss: 0.26520058512687683
***** iteration: 1446 ******
loss: 2.531627893447876
***** iteration: 1447 ******
loss: 3.9305708408355713
***** iteration: 1448 ******
loss: 3.194603204727173
***** iteration: 1449 ******
loss: 0.6228352189064026
***** iteration: 1450 ******
loss: 1.1047000885009766
***** iteration: 1451 ******
loss: 0.43689027428627014
***** iteration: 1452 ******
loss: 0.4978632628917694
***** iteration: 1453 ******
loss: 0.507997453212738
***** iteration: 1454 ******
loss: 0.46581366658210754
***** iteration: 1455 ******
loss: 0.40175125002861023
***** iteration: 1456 ******
loss: 0.31309396028518677
***** iteration: 1457 ******
loss: 1.2481876611709595
***** iteration: 1458 ******
loss: 2.4608631134033203
***** iteration: 1459 ******
loss: 1.589126467704773
***** iteration: 1460 ******
loss: 0.41832032799720764
***** iteration: 1461 ******
loss: 0.3859604299068451
***** iteration: 1462 ******
loss: 0.6808610558509827
***** iteration: 1463 ******
loss: 0.9302133917808533
***** iteration: 1464 ******
loss: 0.7122076749801636
***** iteration: 1465 ******
loss: 0.5135278701782227
***** iteration: 1466 ******
loss: 0.5534934401512146
***** iteration: 1467 ******
loss: 0.4629337787628174
***** iteration: 1468 ******
loss: 0.6303294897079468
***** iteration: 1469 ******
loss: 0.6732305884361267
***** iteration: 1470 ******
loss: 0.35685715079307556
***** iteration: 1471 ******
loss: 0.5331693887710571
***** iteration: 1472 ******
loss: 0.6458172798156738
***** iteration: 1473 ******
loss: 0.6279035806655884
***** iteration: 1474 ******
loss: 0.5945165157318115
***** iteration: 1475 ******
loss: 0.4087706208229065
***** iteration: 1476 ******
loss: 0.5735441446304321
***** iteration: 1477 ******
loss: 0.9876471757888794
***** iteration: 1478 ******
loss: 0.9099245071411133
***** iteration: 1479 ******
loss: 0.419548898935318
***** iteration: 1480 ******
loss: 0.3297528028488159
***** iteration: 1481 ******
loss: 0.46096882224082947
***** iteration: 1482 ******
loss: 0.426623672246933
***** iteration: 1483 ******
loss: 0.29377511143684387
***** iteration: 1484 ******
loss: 0.5070850849151611
***** iteration: 1485 ******
loss: 0.8140967488288879
***** iteration: 1486 ******
loss: 0.6546729207038879
***** iteration: 1487 ******
loss: 0.2688189744949341
***** iteration: 1488 ******
loss: 0.42671504616737366
***** iteration: 1489 ******
loss: 0.6594352722167969
***** iteration: 1490 ******
loss: 0.6545397043228149
***** iteration: 1491 ******
loss: 0.5842032432556152
***** iteration: 1492 ******
loss: 0.4068698585033417
***** iteration: 1493 ******
loss: 0.39696383476257324
***** iteration: 1494 ******
loss: 0.7114422917366028
***** iteration: 1495 ******
loss: 0.6332415342330933
***** iteration: 1496 ******
loss: 0.36886557936668396
***** iteration: 1497 ******
loss: 0.4789441227912903
***** iteration: 1498 ******
loss: 0.6385542750358582
***** iteration: 1499 ******
loss: 0.6010909676551819
***** iteration: 1500 ******
loss: 0.49306079745292664
***** iteration: 1501 ******
loss: 0.371941477060318
***** iteration: 1502 ******
loss: 0.533755898475647
***** iteration: 1503 ******
loss: 0.5060971975326538
***** iteration: 1504 ******
loss: 0.36648714542388916
***** iteration: 1505 ******
loss: 0.44968563318252563
***** iteration: 1506 ******
loss: 0.49494022130966187
***** iteration: 1507 ******
loss: 0.4128192067146301
***** iteration: 1508 ******
loss: 0.35008731484413147
***** iteration: 1509 ******
loss: 0.3934036195278168
***** iteration: 1510 ******
loss: 0.3246445953845978
***** iteration: 1511 ******
loss: 0.4060184955596924
***** iteration: 1512 ******
loss: 0.42407339811325073
***** iteration: 1513 ******
loss: 0.3505646288394928
***** iteration: 1514 ******
loss: 0.36980053782463074
***** iteration: 1515 ******
loss: 1.0195143222808838
***** iteration: 1516 ******
loss: 0.609566330909729
***** iteration: 1517 ******
loss: 0.2987944781780243
***** iteration: 1518 ******
loss: 0.31692638993263245
***** iteration: 1519 ******
loss: 0.29043543338775635
***** iteration: 1520 ******
loss: 0.25376808643341064
***** iteration: 1521 ******
loss: 0.27100539207458496
***** iteration: 1522 ******
loss: 0.24212047457695007
***** iteration: 1523 ******
loss: 0.29741033911705017
***** iteration: 1524 ******
loss: 0.2939497232437134
***** iteration: 1525 ******
loss: 0.2638765275478363
***** iteration: 1526 ******
loss: 0.37103378772735596
***** iteration: 1527 ******
loss: 0.25086572766304016
***** iteration: 1528 ******
loss: 0.34217900037765503
***** iteration: 1529 ******
loss: 0.35400938987731934
***** iteration: 1530 ******
loss: 0.3336624205112457
***** iteration: 1531 ******
loss: 0.32489463686943054
***** iteration: 1532 ******
loss: 0.24596284329891205
***** iteration: 1533 ******
loss: 0.395219624042511
***** iteration: 1534 ******
loss: 0.4362400770187378
***** iteration: 1535 ******
loss: 0.31841588020324707
***** iteration: 1536 ******
loss: 0.3849807679653168
***** iteration: 1537 ******
loss: 0.4322773218154907
***** iteration: 1538 ******
loss: 0.319914311170578
***** iteration: 1539 ******
loss: 0.39278724789619446
***** iteration: 1540 ******
loss: 0.8650715947151184
***** iteration: 1541 ******
loss: 0.5503014922142029
***** iteration: 1542 ******
loss: 0.29566580057144165
***** iteration: 1543 ******
loss: 0.4383906424045563
***** iteration: 1544 ******
loss: 0.5348119735717773
***** iteration: 1545 ******
loss: 0.46483007073402405
***** iteration: 1546 ******
loss: 0.3072403073310852
***** iteration: 1547 ******
loss: 0.5732558965682983
***** iteration: 1548 ******
loss: 0.6971750259399414
***** iteration: 1549 ******
loss: 0.6438697576522827
***** iteration: 1550 ******
loss: 0.27246031165122986
***** iteration: 1551 ******
loss: 0.2881007492542267
***** iteration: 1552 ******
loss: 0.34085705876350403
***** iteration: 1553 ******
loss: 0.3734127879142761
***** iteration: 1554 ******
loss: 0.3470782935619354
***** iteration: 1555 ******
loss: 0.3919714093208313
***** iteration: 1556 ******
loss: 0.4145585596561432
***** iteration: 1557 ******
loss: 0.33535969257354736
***** iteration: 1558 ******
loss: 0.3349779546260834
***** iteration: 1559 ******
loss: 0.3238278925418854
***** iteration: 1560 ******
loss: 1.282623291015625
***** iteration: 1561 ******
loss: 0.8834668397903442
***** iteration: 1562 ******
loss: 0.3370029628276825
***** iteration: 1563 ******
loss: 0.3860839903354645
***** iteration: 1564 ******
loss: 0.38296762108802795
***** iteration: 1565 ******
loss: 0.5038219094276428
***** iteration: 1566 ******
loss: 0.45560869574546814
***** iteration: 1567 ******
loss: 0.30385783314704895
***** iteration: 1568 ******
loss: 0.4533971846103668
***** iteration: 1569 ******
loss: 0.32590505480766296
***** iteration: 1570 ******
loss: 0.29930517077445984
***** iteration: 1571 ******
loss: 0.4380057156085968
***** iteration: 1572 ******
loss: 0.3808867037296295
***** iteration: 1573 ******
loss: 0.33945173025131226
***** iteration: 1574 ******
loss: 0.3198549747467041
***** iteration: 1575 ******
loss: 0.2251935601234436
***** iteration: 1576 ******
loss: 0.2266559600830078
***** iteration: 1577 ******
loss: 0.9424319863319397
***** iteration: 1578 ******
loss: 0.31559479236602783
***** iteration: 1579 ******
loss: 0.39157986640930176
***** iteration: 1580 ******
loss: 0.3369864821434021
***** iteration: 1581 ******
loss: 0.38767820596694946
***** iteration: 1582 ******
loss: 0.3621748089790344
***** iteration: 1583 ******
loss: 1.1681318283081055
***** iteration: 1584 ******
loss: 1.0391401052474976
***** iteration: 1585 ******
loss: 0.5061177611351013
***** iteration: 1586 ******
loss: 0.39887306094169617
***** iteration: 1587 ******
loss: 0.45102256536483765
***** iteration: 1588 ******
loss: 0.5125119090080261
***** iteration: 1589 ******
loss: 0.4241807758808136
***** iteration: 1590 ******
loss: 0.5104616284370422
***** iteration: 1591 ******
loss: 0.6146949529647827
***** iteration: 1592 ******
loss: 0.3492395579814911
***** iteration: 1593 ******
loss: 0.5836531519889832
***** iteration: 1594 ******
loss: 0.558361828327179
***** iteration: 1595 ******
loss: 0.6576958298683167
***** iteration: 1596 ******
loss: 0.6247366070747375
***** iteration: 1597 ******
loss: 0.4442570209503174
***** iteration: 1598 ******
loss: 0.6844730377197266
***** iteration: 1599 ******
loss: 1.1168789863586426
***** iteration: 1600 ******
loss: 1.0488744974136353
***** iteration: 1601 ******
loss: 0.545335054397583
***** iteration: 1602 ******
loss: 0.35457682609558105
***** iteration: 1603 ******
loss: 0.4205847680568695
***** iteration: 1604 ******
loss: 0.6702830791473389
***** iteration: 1605 ******
loss: 0.3764740228652954
***** iteration: 1606 ******
loss: 0.3149873912334442
***** iteration: 1607 ******
loss: 0.4622713327407837
***** iteration: 1608 ******
loss: 0.5175549387931824
***** iteration: 1609 ******
loss: 0.3391444683074951
***** iteration: 1610 ******
loss: 0.3902921676635742
***** iteration: 1611 ******
loss: 0.5909853577613831
***** iteration: 1612 ******
loss: 0.6115991473197937
***** iteration: 1613 ******
loss: 0.4575381875038147
***** iteration: 1614 ******
loss: 0.38620346784591675
***** iteration: 1615 ******
loss: 0.40790972113609314
***** iteration: 1616 ******
loss: 0.5561501383781433
***** iteration: 1617 ******
loss: 0.44520995020866394
***** iteration: 1618 ******
loss: 0.43533656001091003
***** iteration: 1619 ******
loss: 0.4465658366680145
***** iteration: 1620 ******
loss: 0.3325337767601013
***** iteration: 1621 ******
loss: 1.3216962814331055
***** iteration: 1622 ******
loss: 1.8822262287139893
***** iteration: 1623 ******
loss: 1.2063753604888916
***** iteration: 1624 ******
loss: 1.0730890035629272
***** iteration: 1625 ******
loss: 0.5707902908325195
***** iteration: 1626 ******
loss: 0.5346540808677673
***** iteration: 1627 ******
loss: 0.6872684359550476
***** iteration: 1628 ******
loss: 0.6735212802886963
***** iteration: 1629 ******
loss: 0.5126024484634399
***** iteration: 1630 ******
loss: 0.472472220659256
***** iteration: 1631 ******
loss: 0.6169761419296265
***** iteration: 1632 ******
loss: 0.4825591444969177
***** iteration: 1633 ******
loss: 0.44766050577163696
***** iteration: 1634 ******
loss: 0.4006362855434418
***** iteration: 1635 ******
loss: 0.42520004510879517
***** iteration: 1636 ******
loss: 0.33792588114738464
***** iteration: 1637 ******
loss: 0.4557410180568695
***** iteration: 1638 ******
loss: 0.7234817147254944
***** iteration: 1639 ******
loss: 0.46297508478164673
***** iteration: 1640 ******
loss: 0.3040068447589874
***** iteration: 1641 ******
loss: 0.37433189153671265
***** iteration: 1642 ******
loss: 0.4715980589389801
***** iteration: 1643 ******
loss: 0.35428768396377563
***** iteration: 1644 ******
loss: 0.2666419744491577
***** iteration: 1645 ******
loss: 0.4207669794559479
***** iteration: 1646 ******
loss: 1.6433322429656982
***** iteration: 1647 ******
loss: 0.4978142976760864
***** iteration: 1648 ******
loss: 0.28816989064216614
***** iteration: 1649 ******
loss: 0.3816453218460083
***** iteration: 1650 ******
loss: 0.47633296251296997
***** iteration: 1651 ******
loss: 0.4294701814651489
***** iteration: 1652 ******
loss: 0.2914145588874817
***** iteration: 1653 ******
loss: 0.702786386013031
***** iteration: 1654 ******
loss: 1.0926495790481567
***** iteration: 1655 ******
loss: 1.0189889669418335
***** iteration: 1656 ******
loss: 0.5411763191223145
***** iteration: 1657 ******
loss: 0.3548787534236908
***** iteration: 1658 ******
loss: 0.44467243552207947
***** iteration: 1659 ******
loss: 0.4944336414337158
***** iteration: 1660 ******
loss: 0.35700124502182007
***** iteration: 1661 ******
loss: 0.3164077699184418
***** iteration: 1662 ******
loss: 0.3509804904460907
***** iteration: 1663 ******
loss: 0.2851245701313019
***** iteration: 1664 ******
loss: 0.4083104133605957
***** iteration: 1665 ******
loss: 0.4559735953807831
***** iteration: 1666 ******
loss: 0.4233715832233429
***** iteration: 1667 ******
loss: 0.2861381471157074
***** iteration: 1668 ******
loss: 0.44041579961776733
***** iteration: 1669 ******
loss: 0.6137241721153259
***** iteration: 1670 ******
loss: 0.3815070390701294
***** iteration: 1671 ******
loss: 0.2590846121311188
***** iteration: 1672 ******
loss: 0.7319089770317078
***** iteration: 1673 ******
loss: 0.5396715998649597
***** iteration: 1674 ******
loss: 0.5233074426651001
***** iteration: 1675 ******
loss: 0.37186622619628906
***** iteration: 1676 ******
loss: 0.5832319259643555
***** iteration: 1677 ******
loss: 0.9208826422691345
***** iteration: 1678 ******
loss: 0.9039525985717773
***** iteration: 1679 ******
loss: 0.46033135056495667
***** iteration: 1680 ******
loss: 0.3404904901981354
***** iteration: 1681 ******
loss: 0.40677201747894287
***** iteration: 1682 ******
loss: 0.5825135707855225
***** iteration: 1683 ******
loss: 0.33693045377731323
***** iteration: 1684 ******
loss: 0.3894621729850769
***** iteration: 1685 ******
loss: 0.41112324595451355
***** iteration: 1686 ******
loss: 0.33120712637901306
***** iteration: 1687 ******
loss: 0.4024328887462616
***** iteration: 1688 ******
loss: 0.41326403617858887
***** iteration: 1689 ******
loss: 0.3896198868751526
***** iteration: 1690 ******
loss: 0.3571559488773346
***** iteration: 1691 ******
loss: 0.4547182023525238
***** iteration: 1692 ******
loss: 0.3997233808040619
***** iteration: 1693 ******
loss: 0.45309701561927795
***** iteration: 1694 ******
loss: 0.40552818775177
***** iteration: 1695 ******
loss: 0.4960039258003235
***** iteration: 1696 ******
loss: 0.627957820892334
***** iteration: 1697 ******
loss: 0.43374934792518616
***** iteration: 1698 ******
loss: 0.3829863965511322
***** iteration: 1699 ******
loss: 0.38928911089897156
***** iteration: 1700 ******
loss: 0.44898828864097595
***** iteration: 1701 ******
loss: 0.28623995184898376
***** iteration: 1702 ******
loss: 0.32662129402160645
***** iteration: 1703 ******
loss: 0.33520564436912537
***** iteration: 1704 ******
loss: 0.3415274918079376
***** iteration: 1705 ******
loss: 0.2654109299182892
***** iteration: 1706 ******
loss: 1.6774452924728394
***** iteration: 1707 ******
loss: 1.876042127609253
***** iteration: 1708 ******
loss: 1.0010582208633423
***** iteration: 1709 ******
loss: 0.8793153166770935
***** iteration: 1710 ******
loss: 0.5100367069244385
***** iteration: 1711 ******
loss: 0.5091865062713623
***** iteration: 1712 ******
loss: 0.6556183099746704
***** iteration: 1713 ******
loss: 0.6362996101379395
***** iteration: 1714 ******
loss: 0.49060192704200745
***** iteration: 1715 ******
loss: 0.7352886199951172
***** iteration: 1716 ******
loss: 1.0127066373825073
***** iteration: 1717 ******
loss: 0.8128772377967834
***** iteration: 1718 ******
loss: 0.34431758522987366
***** iteration: 1719 ******
loss: 0.4063452184200287
***** iteration: 1720 ******
loss: 0.7840743660926819
***** iteration: 1721 ******
loss: 0.6414648294448853
***** iteration: 1722 ******
loss: 0.6213797926902771
***** iteration: 1723 ******
loss: 0.45542794466018677
***** iteration: 1724 ******
loss: 0.3225475251674652
***** iteration: 1725 ******
loss: 0.5231429934501648
***** iteration: 1726 ******
loss: 0.4620582163333893
***** iteration: 1727 ******
loss: 0.3336552381515503
***** iteration: 1728 ******
loss: 0.3094302713871002
***** iteration: 1729 ******
loss: 0.4706186056137085
***** iteration: 1730 ******
loss: 0.3283279836177826
***** iteration: 1731 ******
loss: 0.3196861743927002
***** iteration: 1732 ******
loss: 0.32493752241134644
***** iteration: 1733 ******
loss: 0.31623193621635437
***** iteration: 1734 ******
loss: 0.3682915270328522
***** iteration: 1735 ******
loss: 0.368945837020874
***** iteration: 1736 ******
loss: 0.3993091881275177
***** iteration: 1737 ******
loss: 0.3878609240055084
***** iteration: 1738 ******
loss: 0.32948005199432373
***** iteration: 1739 ******
loss: 0.3697376251220703
***** iteration: 1740 ******
loss: 0.345134973526001
***** iteration: 1741 ******
loss: 0.3435809910297394
***** iteration: 1742 ******
loss: 0.3721908926963806
***** iteration: 1743 ******
loss: 0.3011801838874817
***** iteration: 1744 ******
loss: 1.0668292045593262
***** iteration: 1745 ******
loss: 0.8780018091201782
***** iteration: 1746 ******
loss: 0.4452653229236603
***** iteration: 1747 ******
loss: 0.401727557182312
***** iteration: 1748 ******
loss: 0.42294520139694214
***** iteration: 1749 ******
loss: 0.525083601474762
***** iteration: 1750 ******
loss: 0.37045010924339294
***** iteration: 1751 ******
loss: 0.4054580330848694
***** iteration: 1752 ******
loss: 0.4152114987373352
***** iteration: 1753 ******
loss: 0.7192067503929138
***** iteration: 1754 ******
loss: 0.44000041484832764
***** iteration: 1755 ******
loss: 0.2926040589809418
***** iteration: 1756 ******
loss: 0.3101968765258789
***** iteration: 1757 ******
loss: 0.2859185039997101
***** iteration: 1758 ******
loss: 0.23989559710025787
***** iteration: 1759 ******
loss: 0.8779690861701965
***** iteration: 1760 ******
loss: 0.23260916769504547
***** iteration: 1761 ******
loss: 0.3142453134059906
***** iteration: 1762 ******
loss: 0.317732572555542
***** iteration: 1763 ******
loss: 0.27995267510414124
***** iteration: 1764 ******
loss: 0.36633414030075073
***** iteration: 1765 ******
loss: 0.2607339918613434
***** iteration: 1766 ******
loss: 0.5017529726028442
***** iteration: 1767 ******
loss: 0.2837522625923157
***** iteration: 1768 ******
loss: 0.31639364361763
***** iteration: 1769 ******
loss: 0.3181844651699066
***** iteration: 1770 ******
loss: 0.311286062002182
***** iteration: 1771 ******
loss: 0.35937827825546265
***** iteration: 1772 ******
loss: 0.2884773910045624
***** iteration: 1773 ******
loss: 0.27749723196029663
***** iteration: 1774 ******
loss: 0.3390113413333893
***** iteration: 1775 ******
loss: 0.29377999901771545
***** iteration: 1776 ******
loss: 0.2910310924053192
***** iteration: 1777 ******
loss: 0.4148537516593933
***** iteration: 1778 ******
loss: 0.23692667484283447
***** iteration: 1779 ******
loss: 0.3347674012184143
***** iteration: 1780 ******
loss: 0.3439565896987915
***** iteration: 1781 ******
loss: 0.23762604594230652
***** iteration: 1782 ******
loss: 1.409064769744873
***** iteration: 1783 ******
loss: 1.9747507572174072
***** iteration: 1784 ******
loss: 1.0988258123397827
***** iteration: 1785 ******
loss: 0.8576852679252625
***** iteration: 1786 ******
loss: 0.3394724726676941
***** iteration: 1787 ******
loss: 0.5745810270309448
***** iteration: 1788 ******
loss: 0.7418153882026672
***** iteration: 1789 ******
loss: 0.7358380556106567
***** iteration: 1790 ******
loss: 0.5739492177963257
***** iteration: 1791 ******
loss: 0.28019770979881287
***** iteration: 1792 ******
loss: 1.335519552230835
***** iteration: 1793 ******
loss: 2.2413177490234375
***** iteration: 1794 ******
loss: 2.2404232025146484
***** iteration: 1795 ******
loss: 2.416005849838257
***** iteration: 1796 ******
loss: 2.1159677505493164
***** iteration: 1797 ******
loss: 1.4148354530334473
***** iteration: 1798 ******
loss: 0.4462295174598694
***** iteration: 1799 ******
loss: 0.6747667193412781
***** iteration: 1800 ******
loss: 0.9365972280502319
***** iteration: 1801 ******
loss: 1.0147876739501953
***** iteration: 1802 ******
loss: 0.9275067448616028
***** iteration: 1803 ******
loss: 0.6957600712776184
***** iteration: 1804 ******
loss: 0.7999308705329895
***** iteration: 1805 ******
loss: 0.5172547698020935
***** iteration: 1806 ******
loss: 0.8353821635246277
***** iteration: 1807 ******
loss: 1.070500373840332
***** iteration: 1808 ******
loss: 0.8271461725234985
***** iteration: 1809 ******
loss: 0.4432923495769501
***** iteration: 1810 ******
loss: 0.6031812429428101
***** iteration: 1811 ******
loss: 0.7240468859672546
***** iteration: 1812 ******
loss: 0.6756302118301392
***** iteration: 1813 ******
loss: 0.4752090275287628
***** iteration: 1814 ******
loss: 0.25884830951690674
***** iteration: 1815 ******
loss: 0.6261609792709351
***** iteration: 1816 ******
loss: 0.8204866051673889
***** iteration: 1817 ******
loss: 0.6867289543151855
***** iteration: 1818 ******
loss: 0.3165714144706726
***** iteration: 1819 ******
loss: 0.4076356887817383
***** iteration: 1820 ******
loss: 0.5456374287605286
***** iteration: 1821 ******
loss: 0.5399742722511292
***** iteration: 1822 ******
loss: 0.3959473669528961
***** iteration: 1823 ******
loss: 0.34116050601005554
***** iteration: 1824 ******
loss: 0.558627724647522
***** iteration: 1825 ******
loss: 0.5052424073219299
***** iteration: 1826 ******
loss: 0.3202438950538635
***** iteration: 1827 ******
loss: 0.48990964889526367
***** iteration: 1828 ******
loss: 0.5888627171516418
***** iteration: 1829 ******
loss: 0.6118283271789551
***** iteration: 1830 ******
loss: 0.4816626012325287
***** iteration: 1831 ******
loss: 0.283613920211792
***** iteration: 1832 ******
loss: 0.8816465735435486
***** iteration: 1833 ******
loss: 1.4225971698760986
***** iteration: 1834 ******
loss: 1.4526355266571045
***** iteration: 1835 ******
loss: 1.0251328945159912
***** iteration: 1836 ******
loss: 0.32969874143600464
***** iteration: 1837 ******
loss: 0.4044523537158966
***** iteration: 1838 ******
loss: 0.6013023853302002
***** iteration: 1839 ******
loss: 0.6360802054405212
***** iteration: 1840 ******
loss: 0.5318397283554077
***** iteration: 1841 ******
loss: 0.30095812678337097
***** iteration: 1842 ******
loss: 0.5063197016716003
***** iteration: 1843 ******
loss: 0.9820029139518738
***** iteration: 1844 ******
loss: 0.9826744198799133
***** iteration: 1845 ******
loss: 0.5515321493148804
***** iteration: 1846 ******
loss: 0.3098912239074707
***** iteration: 1847 ******
loss: 0.40265291929244995
***** iteration: 1848 ******
loss: 0.44402655959129333
***** iteration: 1849 ******
loss: 0.37117984890937805
***** iteration: 1850 ******
loss: 0.2865820527076721
***** iteration: 1851 ******
loss: 0.4644046425819397
***** iteration: 1852 ******
loss: 0.46052733063697815
***** iteration: 1853 ******
loss: 0.3485161364078522
***** iteration: 1854 ******
loss: 0.38790416717529297
***** iteration: 1855 ******
loss: 0.5744736194610596
***** iteration: 1856 ******
loss: 0.48829734325408936
***** iteration: 1857 ******
loss: 0.38971656560897827
***** iteration: 1858 ******
loss: 0.32722142338752747
***** iteration: 1859 ******
loss: 0.46365565061569214
***** iteration: 1860 ******
loss: 0.5205151438713074
***** iteration: 1861 ******
loss: 0.7976382374763489
***** iteration: 1862 ******
loss: 0.49715685844421387
***** iteration: 1863 ******
loss: 0.2924274206161499
***** iteration: 1864 ******
loss: 0.29101723432540894
***** iteration: 1865 ******
loss: 0.3947605490684509
***** iteration: 1866 ******
loss: 0.47626256942749023
***** iteration: 1867 ******
loss: 0.38699817657470703
***** iteration: 1868 ******
loss: 0.35801801085472107
***** iteration: 1869 ******
loss: 0.37316691875457764
***** iteration: 1870 ******
loss: 0.7127728462219238
***** iteration: 1871 ******
loss: 0.3626723885536194
***** iteration: 1872 ******
loss: 0.4100256860256195
***** iteration: 1873 ******
loss: 0.4271707832813263
***** iteration: 1874 ******
loss: 0.4283521771430969
***** iteration: 1875 ******
loss: 0.3648366928100586
***** iteration: 1876 ******
loss: 0.5990674495697021
***** iteration: 1877 ******
loss: 1.3228524923324585
***** iteration: 1878 ******
loss: 0.37725549936294556
***** iteration: 1879 ******
loss: 0.3560795187950134
***** iteration: 1880 ******
loss: 0.39434322714805603
***** iteration: 1881 ******
loss: 0.3643328547477722
***** iteration: 1882 ******
loss: 0.29524609446525574
***** iteration: 1883 ******
loss: 1.4746387004852295
***** iteration: 1884 ******
loss: 2.4866127967834473
***** iteration: 1885 ******
loss: 1.452561378479004
***** iteration: 1886 ******
loss: 0.4092460870742798
***** iteration: 1887 ******
loss: 0.36501631140708923
***** iteration: 1888 ******
loss: 0.4584358334541321
***** iteration: 1889 ******
loss: 1.3909287452697754
***** iteration: 1890 ******
loss: 0.7609942555427551
***** iteration: 1891 ******
loss: 0.4923253059387207
***** iteration: 1892 ******
loss: 0.5227052569389343
***** iteration: 1893 ******
loss: 0.5991086959838867
***** iteration: 1894 ******
loss: 0.5133821368217468
***** iteration: 1895 ******
loss: 0.353058785200119
***** iteration: 1896 ******
loss: 0.3811168372631073
***** iteration: 1897 ******
loss: 1.212597370147705
***** iteration: 1898 ******
loss: 1.0820773839950562
***** iteration: 1899 ******
loss: 0.3332640528678894
***** iteration: 1900 ******
loss: 0.3635116517543793
***** iteration: 1901 ******
loss: 0.5090426206588745
***** iteration: 1902 ******
loss: 0.4762633144855499
***** iteration: 1903 ******
loss: 0.41603219509124756
***** iteration: 1904 ******
loss: 0.39675360918045044
***** iteration: 1905 ******
loss: 0.320917546749115
***** iteration: 1906 ******
loss: 0.4014091491699219
***** iteration: 1907 ******
loss: 0.41779157519340515
***** iteration: 1908 ******
loss: 0.29501354694366455
***** iteration: 1909 ******
loss: 0.3401816785335541
***** iteration: 1910 ******
loss: 0.3067049980163574
***** iteration: 1911 ******
loss: 0.3409354090690613
***** iteration: 1912 ******
loss: 0.37931913137435913
***** iteration: 1913 ******
loss: 0.27878761291503906
***** iteration: 1914 ******
loss: 0.35827475786209106
***** iteration: 1915 ******
loss: 0.40220627188682556
***** iteration: 1916 ******
loss: 0.3276901841163635
***** iteration: 1917 ******
loss: 0.9257336258888245
***** iteration: 1918 ******
loss: 0.30765193700790405
***** iteration: 1919 ******
loss: 0.3478686213493347
***** iteration: 1920 ******
loss: 0.3808850347995758
***** iteration: 1921 ******
loss: 0.33968132734298706
***** iteration: 1922 ******
loss: 0.342991441488266
***** iteration: 1923 ******
loss: 0.3395496904850006
***** iteration: 1924 ******
loss: 0.4823833107948303
***** iteration: 1925 ******
loss: 0.43420860171318054
***** iteration: 1926 ******
loss: 0.3042929768562317
***** iteration: 1927 ******
loss: 0.34552446007728577
***** iteration: 1928 ******
loss: 0.307569682598114
***** iteration: 1929 ******
loss: 0.4636693596839905
***** iteration: 1930 ******
loss: 0.558549702167511
***** iteration: 1931 ******
loss: 0.3117022216320038
***** iteration: 1932 ******
loss: 0.4117862284183502
***** iteration: 1933 ******
loss: 0.4869108498096466
***** iteration: 1934 ******
loss: 0.3999631702899933
***** iteration: 1935 ******
loss: 0.30510520935058594
***** iteration: 1936 ******
loss: 0.49708300828933716
***** iteration: 1937 ******
loss: 0.4855303466320038
***** iteration: 1938 ******
loss: 0.3087770640850067
***** iteration: 1939 ******
loss: 0.349590003490448
***** iteration: 1940 ******
loss: 0.8369554877281189
***** iteration: 1941 ******
loss: 0.6224464774131775
***** iteration: 1942 ******
loss: 0.31403952836990356
***** iteration: 1943 ******
loss: 0.315807968378067
***** iteration: 1944 ******
loss: 0.3369181454181671
***** iteration: 1945 ******
loss: 0.31094640493392944
***** iteration: 1946 ******
loss: 0.4688890874385834
***** iteration: 1947 ******
loss: 0.23834063112735748
***** iteration: 1948 ******
loss: 1.5202511548995972
***** iteration: 1949 ******
loss: 2.1645381450653076
***** iteration: 1950 ******
loss: 0.8452172875404358
***** iteration: 1951 ******
loss: 0.3514310121536255
***** iteration: 1952 ******
loss: 0.4261242151260376
***** iteration: 1953 ******
loss: 0.4828387200832367
***** iteration: 1954 ******
loss: 0.45900416374206543
***** iteration: 1955 ******
loss: 0.436013787984848
***** iteration: 1956 ******
loss: 0.4110548496246338
***** iteration: 1957 ******
loss: 0.31228214502334595
***** iteration: 1958 ******
loss: 1.5243852138519287
***** iteration: 1959 ******
loss: 2.644744634628296
***** iteration: 1960 ******
loss: 1.7150895595550537
***** iteration: 1961 ******
loss: 0.8635730147361755
***** iteration: 1962 ******
loss: 0.5201398730278015
***** iteration: 1963 ******
loss: 0.49882060289382935
***** iteration: 1964 ******
loss: 0.6348211765289307
***** iteration: 1965 ******
loss: 0.6036945581436157
***** iteration: 1966 ******
loss: 0.457772433757782
***** iteration: 1967 ******
loss: 0.8401190638542175
***** iteration: 1968 ******
loss: 1.0860265493392944
***** iteration: 1969 ******
loss: 0.8502569198608398
***** iteration: 1970 ******
loss: 0.3175119459629059
***** iteration: 1971 ******
loss: 0.5587625503540039
***** iteration: 1972 ******
loss: 0.6738710999488831
***** iteration: 1973 ******
loss: 0.7708820104598999
***** iteration: 1974 ******
loss: 0.7468844652175903
***** iteration: 1975 ******
loss: 0.5690930485725403
***** iteration: 1976 ******
loss: 0.30851811170578003
***** iteration: 1977 ******
loss: 1.0059232711791992
***** iteration: 1978 ******
loss: 1.7245233058929443
***** iteration: 1979 ******
loss: 2.1036949157714844
***** iteration: 1980 ******
loss: 1.763416051864624
***** iteration: 1981 ******
loss: 1.235871434211731
***** iteration: 1982 ******
loss: 0.4419659972190857
***** iteration: 1983 ******
loss: 0.5584775805473328
***** iteration: 1984 ******
loss: 0.790005624294281
***** iteration: 1985 ******
loss: 0.8420884609222412
***** iteration: 1986 ******
loss: 0.7325316071510315
***** iteration: 1987 ******
loss: 0.4804937541484833
***** iteration: 1988 ******
loss: 0.4666295647621155
***** iteration: 1989 ******
loss: 0.9529046416282654
***** iteration: 1990 ******
loss: 1.2036279439926147
***** iteration: 1991 ******
loss: 1.0730794668197632
***** iteration: 1992 ******
loss: 0.5520837306976318
***** iteration: 1993 ******
loss: 0.4126286208629608
***** iteration: 1994 ******
loss: 0.5288166403770447
***** iteration: 1995 ******
loss: 0.49212098121643066
***** iteration: 1996 ******
loss: 0.3304843008518219
***** iteration: 1997 ******
loss: 0.5331926941871643
***** iteration: 1998 ******
loss: 0.847578763961792
***** iteration: 1999 ******
loss: 0.825972855091095
***** iteration: 2000 ******
loss: 0.4910139739513397
***** iteration: 2001 ******
loss: 0.3384148180484772
***** iteration: 2002 ******
loss: 0.39362844824790955
***** iteration: 2003 ******
loss: 0.3363836109638214
***** iteration: 2004 ******
loss: 0.3728562295436859
***** iteration: 2005 ******
loss: 0.3655952513217926
***** iteration: 2006 ******
loss: 0.3666534721851349
***** iteration: 2007 ******
loss: 0.3008531928062439
***** iteration: 2008 ******
loss: 0.3133224844932556
***** iteration: 2009 ******
loss: 0.4599459767341614
***** iteration: 2010 ******
loss: 0.2889691889286041
***** iteration: 2011 ******
loss: 0.31416070461273193
***** iteration: 2012 ******
loss: 0.3372013568878174
***** iteration: 2013 ******
loss: 0.3304043710231781
***** iteration: 2014 ******
loss: 0.3306000530719757
***** iteration: 2015 ******
loss: 0.36333611607551575
***** iteration: 2016 ******
loss: 0.32401955127716064
***** iteration: 2017 ******
loss: 0.3165377676486969
***** iteration: 2018 ******
loss: 0.338084876537323
***** iteration: 2019 ******
loss: 0.2963063418865204
***** iteration: 2020 ******
loss: 0.3446488380432129
***** iteration: 2021 ******
loss: 0.3153361678123474
***** iteration: 2022 ******
loss: 0.37327802181243896
***** iteration: 2023 ******
loss: 0.3133300244808197
***** iteration: 2024 ******
loss: 0.32534268498420715
***** iteration: 2025 ******
loss: 0.3283640742301941
***** iteration: 2026 ******
loss: 0.2955327033996582
***** iteration: 2027 ******
loss: 0.6414094567298889
***** iteration: 2028 ******
loss: 0.5320174694061279
***** iteration: 2029 ******
loss: 0.35459691286087036
***** iteration: 2030 ******
loss: 0.3161889612674713
***** iteration: 2031 ******
loss: 0.5965865850448608
***** iteration: 2032 ******
loss: 0.7218810319900513
***** iteration: 2033 ******
loss: 0.4382499158382416
***** iteration: 2034 ******
loss: 0.31545835733413696
***** iteration: 2035 ******
loss: 0.3447457551956177
***** iteration: 2036 ******
loss: 0.8097410202026367
***** iteration: 2037 ******
loss: 0.5205237865447998
***** iteration: 2038 ******
loss: 0.30215176939964294
***** iteration: 2039 ******
loss: 0.3020843267440796
***** iteration: 2040 ******
loss: 0.3542429208755493
***** iteration: 2041 ******
loss: 0.2909228801727295
***** iteration: 2042 ******
loss: 0.6104981899261475
***** iteration: 2043 ******
loss: 0.7039449214935303
***** iteration: 2044 ******
loss: 0.5843178033828735
***** iteration: 2045 ******
loss: 0.3364062011241913
***** iteration: 2046 ******
loss: 0.4797300696372986
***** iteration: 2047 ******
loss: 0.5778509974479675
***** iteration: 2048 ******
loss: 0.5106239914894104
***** iteration: 2049 ******
loss: 0.30929437279701233
***** iteration: 2050 ******
loss: 0.6526651382446289
***** iteration: 2051 ******
loss: 1.4296151399612427
***** iteration: 2052 ******
loss: 1.3047840595245361
***** iteration: 2053 ******
loss: 0.9742773175239563
***** iteration: 2054 ******
loss: 0.43228304386138916
***** iteration: 2055 ******
loss: 0.4594470262527466
***** iteration: 2056 ******
loss: 0.6204648017883301
***** iteration: 2057 ******
loss: 0.6305340528488159
***** iteration: 2058 ******
loss: 0.5067917108535767
***** iteration: 2059 ******
loss: 0.31142473220825195
***** iteration: 2060 ******
loss: 0.7351922392845154
***** iteration: 2061 ******
loss: 1.844218373298645
***** iteration: 2062 ******
loss: 1.6565768718719482
***** iteration: 2063 ******
loss: 1.1537052392959595
***** iteration: 2064 ******
loss: 0.7016054391860962
***** iteration: 2065 ******
loss: 0.4051175117492676
***** iteration: 2066 ******
loss: 0.5198382139205933
***** iteration: 2067 ******
loss: 0.4962914288043976
***** iteration: 2068 ******
loss: 0.3639344274997711
***** iteration: 2069 ******
loss: 0.5256580710411072
***** iteration: 2070 ******
loss: 1.2446070909500122
***** iteration: 2071 ******
loss: 1.1913292407989502
***** iteration: 2072 ******
loss: 0.5069855451583862
***** iteration: 2073 ******
loss: 0.3378163278102875
***** iteration: 2074 ******
loss: 0.4217588007450104
***** iteration: 2075 ******
loss: 0.3993532061576843
***** iteration: 2076 ******
loss: 0.31919971108436584
***** iteration: 2077 ******
loss: 0.3460772931575775
***** iteration: 2078 ******
loss: 0.35848134756088257
***** iteration: 2079 ******
loss: 0.2793148458003998
***** iteration: 2080 ******
loss: 0.3335762321949005
***** iteration: 2081 ******
loss: 0.28495875000953674
***** iteration: 2082 ******
loss: 0.27306005358695984
***** iteration: 2083 ******
loss: 0.5694332122802734
***** iteration: 2084 ******
loss: 0.27804404497146606
***** iteration: 2085 ******
loss: 0.2776968479156494
***** iteration: 2086 ******
loss: 0.41746968030929565
***** iteration: 2087 ******
loss: 0.27288734912872314
***** iteration: 2088 ******
loss: 0.3270093500614166
***** iteration: 2089 ******
loss: 0.3507177531719208
***** iteration: 2090 ******
loss: 0.34140825271606445
***** iteration: 2091 ******
loss: 0.29013752937316895
***** iteration: 2092 ******
loss: 0.9001127481460571
***** iteration: 2093 ******
loss: 1.2319387197494507
***** iteration: 2094 ******
loss: 0.3592287600040436
***** iteration: 2095 ******
loss: 0.3665381968021393
***** iteration: 2096 ******
loss: 0.4665949046611786
***** iteration: 2097 ******
loss: 0.5210232138633728
***** iteration: 2098 ******
loss: 0.4782353341579437
***** iteration: 2099 ******
loss: 0.47082728147506714
***** iteration: 2100 ******
loss: 0.49045529961586
***** iteration: 2101 ******
loss: 0.40541109442710876
***** iteration: 2102 ******
loss: 0.3730282485485077
***** iteration: 2103 ******
loss: 0.3093668520450592
***** iteration: 2104 ******
loss: 1.0345641374588013
***** iteration: 2105 ******
loss: 1.5608407258987427
***** iteration: 2106 ******
loss: 0.6413753628730774
***** iteration: 2107 ******
loss: 0.4833790063858032
***** iteration: 2108 ******
loss: 0.35590824484825134
***** iteration: 2109 ******
loss: 0.5233319997787476
***** iteration: 2110 ******
loss: 0.592634379863739
***** iteration: 2111 ******
loss: 0.49966463446617126
***** iteration: 2112 ******
loss: 0.39939796924591064
***** iteration: 2113 ******
loss: 0.43917056918144226
***** iteration: 2114 ******
loss: 0.29322338104248047
***** iteration: 2115 ******
loss: 0.713895857334137
***** iteration: 2116 ******
loss: 1.0856038331985474
***** iteration: 2117 ******
loss: 0.66646409034729
***** iteration: 2118 ******
loss: 0.7024503946304321
***** iteration: 2119 ******
loss: 0.6256787180900574
***** iteration: 2120 ******
loss: 0.43242260813713074
***** iteration: 2121 ******
loss: 1.0641541481018066
***** iteration: 2122 ******
loss: 1.602929949760437
***** iteration: 2123 ******
loss: 1.6280797719955444
***** iteration: 2124 ******
loss: 1.191158413887024
***** iteration: 2125 ******
loss: 0.4374189078807831
***** iteration: 2126 ******
loss: 0.5220957398414612
***** iteration: 2127 ******
loss: 0.6741006970405579
***** iteration: 2128 ******
loss: 0.7623468637466431
***** iteration: 2129 ******
loss: 0.7207379937171936
***** iteration: 2130 ******
loss: 0.5604788661003113
***** iteration: 2131 ******
loss: 0.3593355119228363
***** iteration: 2132 ******
loss: 0.5828675627708435
***** iteration: 2133 ******
loss: 1.0850484371185303
***** iteration: 2134 ******
loss: 1.095292091369629
***** iteration: 2135 ******
loss: 0.7631434202194214
***** iteration: 2136 ******
loss: 0.2821497917175293
***** iteration: 2137 ******
loss: 0.4666346311569214
***** iteration: 2138 ******
loss: 0.6442239880561829
***** iteration: 2139 ******
loss: 0.68218594789505
***** iteration: 2140 ******
loss: 0.6068357229232788
***** iteration: 2141 ******
loss: 0.3907320499420166
***** iteration: 2142 ******
loss: 0.4884871542453766
***** iteration: 2143 ******
loss: 0.9505491852760315
***** iteration: 2144 ******
loss: 1.0045698881149292
***** iteration: 2145 ******
loss: 0.586233913898468
***** iteration: 2146 ******
loss: 0.31229260563850403
***** iteration: 2147 ******
loss: 0.42923828959465027
***** iteration: 2148 ******
loss: 0.45313262939453125
***** iteration: 2149 ******
loss: 0.37463927268981934
***** iteration: 2150 ******
loss: 0.3237360715866089
***** iteration: 2151 ******
loss: 0.37828630208969116
***** iteration: 2152 ******
loss: 0.31468841433525085
***** iteration: 2153 ******
loss: 0.32698938250541687
***** iteration: 2154 ******
loss: 0.3397684693336487
***** iteration: 2155 ******
loss: 0.28369325399398804
***** iteration: 2156 ******
loss: 0.400875061750412
***** iteration: 2157 ******
loss: 0.3387596309185028
***** iteration: 2158 ******
loss: 0.26978155970573425
***** iteration: 2159 ******
loss: 0.9007217288017273
***** iteration: 2160 ******
loss: 0.3603540360927582
***** iteration: 2161 ******
loss: 0.29881197214126587
***** iteration: 2162 ******
loss: 0.3771572411060333
***** iteration: 2163 ******
loss: 0.5451474785804749
***** iteration: 2164 ******
loss: 0.368312805891037
***** iteration: 2165 ******
loss: 0.32843172550201416
***** iteration: 2166 ******
loss: 0.35468825697898865
***** iteration: 2167 ******
loss: 0.2917995750904083
***** iteration: 2168 ******
loss: 0.40821075439453125
***** iteration: 2169 ******
loss: 0.41841936111450195
***** iteration: 2170 ******
loss: 0.31337374448776245
***** iteration: 2171 ******
loss: 0.358715295791626
***** iteration: 2172 ******
loss: 0.32512444257736206
***** iteration: 2173 ******
loss: 0.31253674626350403
***** iteration: 2174 ******
loss: 0.3275059163570404
***** iteration: 2175 ******
loss: 0.2679290175437927
***** iteration: 2176 ******
loss: 0.3114383816719055
***** iteration: 2177 ******
loss: 0.2783501148223877
***** iteration: 2178 ******
loss: 0.38046225905418396
***** iteration: 2179 ******
loss: 0.46215516328811646
***** iteration: 2180 ******
loss: 0.26726552844047546
***** iteration: 2181 ******
loss: 0.3430236279964447
***** iteration: 2182 ******
loss: 0.4002624452114105
***** iteration: 2183 ******
loss: 0.3209227919578552
***** iteration: 2184 ******
loss: 0.24876399338245392
***** iteration: 2185 ******
loss: 0.4225810766220093
***** iteration: 2186 ******
loss: 0.2785877287387848
***** iteration: 2187 ******
loss: 0.3203423023223877
***** iteration: 2188 ******
loss: 0.32595548033714294
***** iteration: 2189 ******
loss: 0.24878758192062378
***** iteration: 2190 ******
loss: 1.1906449794769287
***** iteration: 2191 ******
loss: 1.022176742553711
***** iteration: 2192 ******
loss: 0.2545000910758972
***** iteration: 2193 ******
loss: 0.28272005915641785
***** iteration: 2194 ******
loss: 0.23452343046665192
***** iteration: 2195 ******
loss: 0.5462774634361267
***** iteration: 2196 ******
loss: 0.6744105219841003
***** iteration: 2197 ******
loss: 0.4217274785041809
***** iteration: 2198 ******
loss: 1.8728362321853638
***** iteration: 2199 ******
loss: 0.48198986053466797
***** iteration: 2200 ******
loss: 0.509194016456604
***** iteration: 2201 ******
loss: 0.37956395745277405
***** iteration: 2202 ******
loss: 0.4906410872936249
***** iteration: 2203 ******
loss: 0.7091336846351624
***** iteration: 2204 ******
loss: 0.7861146926879883
***** iteration: 2205 ******
loss: 0.4463360905647278
***** iteration: 2206 ******
loss: 0.35027623176574707
***** iteration: 2207 ******
loss: 0.422404408454895
***** iteration: 2208 ******
loss: 0.8533449172973633
***** iteration: 2209 ******
loss: 0.34463804960250854
***** iteration: 2210 ******
loss: 0.34387123584747314
***** iteration: 2211 ******
loss: 0.3692818284034729
***** iteration: 2212 ******
loss: 0.3936026394367218
***** iteration: 2213 ******
loss: 0.3325227200984955
***** iteration: 2214 ******
loss: 0.3736831247806549
***** iteration: 2215 ******
loss: 0.5515710115432739
***** iteration: 2216 ******
loss: 0.36300989985466003
***** iteration: 2217 ******
loss: 0.33498072624206543
***** iteration: 2218 ******
loss: 0.35558414459228516
***** iteration: 2219 ******
loss: 0.27230775356292725
***** iteration: 2220 ******
loss: 1.2145664691925049
***** iteration: 2221 ******
loss: 1.7641589641571045
***** iteration: 2222 ******
loss: 0.7449989318847656
***** iteration: 2223 ******
loss: 0.5216667652130127
***** iteration: 2224 ******
loss: 0.3686807155609131
***** iteration: 2225 ******
loss: 0.4415570795536041
***** iteration: 2226 ******
loss: 0.3928273618221283
***** iteration: 2227 ******
loss: 0.7510788440704346
***** iteration: 2228 ******
loss: 0.8195279836654663
***** iteration: 2229 ******
loss: 0.7159930467605591
***** iteration: 2230 ******
loss: 0.3370648920536041
***** iteration: 2231 ******
loss: 0.41444599628448486
***** iteration: 2232 ******
loss: 1.0316075086593628
***** iteration: 2233 ******
loss: 0.9588344097137451
***** iteration: 2234 ******
loss: 0.7194398641586304
***** iteration: 2235 ******
loss: 0.6825916767120361
***** iteration: 2236 ******
loss: 0.49416738748550415
***** iteration: 2237 ******
loss: 0.4873929023742676
***** iteration: 2238 ******
loss: 0.8654099106788635
***** iteration: 2239 ******
loss: 0.835261344909668
***** iteration: 2240 ******
loss: 0.452859491109848
***** iteration: 2241 ******
loss: 0.3768584132194519
***** iteration: 2242 ******
loss: 0.6432861089706421
***** iteration: 2243 ******
loss: 0.5757464170455933
***** iteration: 2244 ******
loss: 0.5412256717681885
***** iteration: 2245 ******
loss: 0.43917250633239746
***** iteration: 2246 ******
loss: 0.5496366024017334
***** iteration: 2247 ******
loss: 0.8036384582519531
***** iteration: 2248 ******
loss: 0.6715077757835388
***** iteration: 2249 ******
loss: 0.4044387936592102
***** iteration: 2250 ******
loss: 0.47837111353874207
***** iteration: 2251 ******
loss: 0.5840853452682495
***** iteration: 2252 ******
loss: 0.6278760433197021
***** iteration: 2253 ******
loss: 0.5274226665496826
***** iteration: 2254 ******
loss: 0.3848889172077179
***** iteration: 2255 ******
loss: 0.4579629600048065
***** iteration: 2256 ******
loss: 0.7431062459945679
***** iteration: 2257 ******
loss: 0.6338430047035217
***** iteration: 2258 ******
loss: 0.30868425965309143
***** iteration: 2259 ******
loss: 0.43311816453933716
***** iteration: 2260 ******
loss: 0.5678133368492126
***** iteration: 2261 ******
loss: 0.610758900642395
***** iteration: 2262 ******
loss: 0.499406099319458
***** iteration: 2263 ******
loss: 0.27486175298690796
***** iteration: 2264 ******
loss: 0.5808762311935425
***** iteration: 2265 ******
loss: 1.1018646955490112
***** iteration: 2266 ******
loss: 1.1516305208206177
***** iteration: 2267 ******
loss: 0.7525697946548462
***** iteration: 2268 ******
loss: 0.279890775680542
***** iteration: 2269 ******
loss: 0.4632323384284973
***** iteration: 2270 ******
loss: 0.6534011960029602
***** iteration: 2271 ******
loss: 0.7000328302383423
***** iteration: 2272 ******
loss: 0.6023998856544495
***** iteration: 2273 ******
loss: 0.3953488767147064
***** iteration: 2274 ******
loss: 0.30272209644317627
***** iteration: 2275 ******
loss: 0.5152965188026428
***** iteration: 2276 ******
loss: 0.4919937252998352
***** iteration: 2277 ******
loss: 0.3062928020954132
***** iteration: 2278 ******
loss: 0.37148308753967285
***** iteration: 2279 ******
loss: 0.4480312168598175
***** iteration: 2280 ******
loss: 0.378512978553772
***** iteration: 2281 ******
loss: 0.2811688184738159
***** iteration: 2282 ******
loss: 0.3027079999446869
***** iteration: 2283 ******
loss: 0.2980376183986664
***** iteration: 2284 ******
loss: 0.3109142780303955
***** iteration: 2285 ******
loss: 0.3525622487068176
***** iteration: 2286 ******
loss: 0.2560378611087799
***** iteration: 2287 ******
loss: 0.29103192687034607
***** iteration: 2288 ******
loss: 0.38432714343070984
***** iteration: 2289 ******
loss: 0.31263312697410583
***** iteration: 2290 ******
loss: 0.3250703513622284
***** iteration: 2291 ******
loss: 0.32999512553215027
***** iteration: 2292 ******
loss: 0.5244386792182922
***** iteration: 2293 ******
loss: 0.28950822353363037
***** iteration: 2294 ******
loss: 0.3364091217517853
***** iteration: 2295 ******
loss: 0.3818551003932953
***** iteration: 2296 ******
loss: 0.38668984174728394
***** iteration: 2297 ******
loss: 0.34792953729629517
***** iteration: 2298 ******
loss: 0.2946774661540985
***** iteration: 2299 ******
loss: 1.0845248699188232
***** iteration: 2300 ******
loss: 1.3595207929611206
***** iteration: 2301 ******
loss: 0.46589237451553345
***** iteration: 2302 ******
loss: 0.43587103486061096
***** iteration: 2303 ******
loss: 0.2962583601474762
***** iteration: 2304 ******
loss: 0.6753537654876709
***** iteration: 2305 ******
loss: 1.07015860080719
***** iteration: 2306 ******
loss: 0.9753808975219727
***** iteration: 2307 ******
loss: 0.5383949279785156
***** iteration: 2308 ******
loss: 0.28188300132751465
***** iteration: 2309 ******
loss: 0.3572109341621399
***** iteration: 2310 ******
loss: 0.4378601610660553
***** iteration: 2311 ******
loss: 0.27037930488586426
***** iteration: 2312 ******
loss: 0.2845708429813385
***** iteration: 2313 ******
loss: 0.34863218665122986
***** iteration: 2314 ******
loss: 0.24827726185321808
***** iteration: 2315 ******
loss: 0.33312615752220154
***** iteration: 2316 ******
loss: 0.3671208918094635
***** iteration: 2317 ******
loss: 0.29207658767700195
***** iteration: 2318 ******
loss: 0.42957863211631775
***** iteration: 2319 ******
loss: 1.215326189994812
***** iteration: 2320 ******
loss: 0.9068601131439209
***** iteration: 2321 ******
loss: 0.35799190402030945
***** iteration: 2322 ******
loss: 0.32433566451072693
***** iteration: 2323 ******
loss: 0.3723156750202179
***** iteration: 2324 ******
loss: 0.31022894382476807
***** iteration: 2325 ******
loss: 0.4090857207775116
***** iteration: 2326 ******
loss: 0.5067968964576721
***** iteration: 2327 ******
loss: 0.39535394310951233
***** iteration: 2328 ******
loss: 0.2495952844619751
***** iteration: 2329 ******
loss: 0.25606757402420044
***** iteration: 2330 ******
loss: 0.24678124487400055
***** iteration: 2331 ******
loss: 0.23140433430671692
***** iteration: 2332 ******
loss: 0.2969232201576233
***** iteration: 2333 ******
loss: 0.3399263024330139
***** iteration: 2334 ******
loss: 0.2888144850730896
***** iteration: 2335 ******
loss: 0.3309926688671112
***** iteration: 2336 ******
loss: 0.4611940383911133
***** iteration: 2337 ******
loss: 0.31698936223983765
***** iteration: 2338 ******
loss: 0.3385225236415863
***** iteration: 2339 ******
loss: 0.3694385290145874
***** iteration: 2340 ******
loss: 0.289987176656723
***** iteration: 2341 ******
loss: 0.3674907386302948
***** iteration: 2342 ******
loss: 0.402483731508255
***** iteration: 2343 ******
loss: 0.28922444581985474
***** iteration: 2344 ******
loss: 0.5731444358825684
***** iteration: 2345 ******
loss: 0.5353708267211914
***** iteration: 2346 ******
loss: 0.45122337341308594
***** iteration: 2347 ******
loss: 0.3936471939086914
***** iteration: 2348 ******
loss: 0.42229005694389343
***** iteration: 2349 ******
loss: 0.5486563444137573
***** iteration: 2350 ******
loss: 0.3845581114292145
***** iteration: 2351 ******
loss: 0.3833869695663452
***** iteration: 2352 ******
loss: 0.3955698311328888
***** iteration: 2353 ******
loss: 0.33252912759780884
***** iteration: 2354 ******
loss: 0.24089886248111725
***** iteration: 2355 ******
loss: 0.29611021280288696
***** iteration: 2356 ******
loss: 0.3197462856769562
***** iteration: 2357 ******
loss: 0.29664376378059387
***** iteration: 2358 ******
loss: 1.037561058998108
***** iteration: 2359 ******
loss: 0.5342246890068054
***** iteration: 2360 ******
loss: 0.5136130452156067
***** iteration: 2361 ******
loss: 0.2811467945575714
***** iteration: 2362 ******
loss: 0.4112991392612457
***** iteration: 2363 ******
loss: 0.4878501892089844
***** iteration: 2364 ******
loss: 0.40349704027175903
***** iteration: 2365 ******
loss: 0.3932263255119324
***** iteration: 2366 ******
loss: 0.5158039331436157
***** iteration: 2367 ******
loss: 0.5695192217826843
***** iteration: 2368 ******
loss: 0.3398594856262207
***** iteration: 2369 ******
loss: 0.4312213659286499
***** iteration: 2370 ******
loss: 0.4974360167980194
***** iteration: 2371 ******
loss: 0.6727092266082764
***** iteration: 2372 ******
loss: 0.367784321308136
***** iteration: 2373 ******
loss: 0.3366369903087616
***** iteration: 2374 ******
loss: 0.4079524874687195
***** iteration: 2375 ******
loss: 0.31839656829833984
***** iteration: 2376 ******
loss: 0.3794035315513611
***** iteration: 2377 ******
loss: 0.4534607529640198
***** iteration: 2378 ******
loss: 0.4226283133029938
***** iteration: 2379 ******
loss: 0.327333003282547
***** iteration: 2380 ******
loss: 0.6079328060150146
***** iteration: 2381 ******
loss: 0.8388263583183289
***** iteration: 2382 ******
loss: 0.6191693544387817
***** iteration: 2383 ******
loss: 0.33898499608039856
***** iteration: 2384 ******
loss: 0.3651770353317261
***** iteration: 2385 ******
loss: 0.423299103975296
***** iteration: 2386 ******
loss: 0.3312906324863434
***** iteration: 2387 ******
loss: 0.3021101951599121
***** iteration: 2388 ******
loss: 0.4880356788635254
***** iteration: 2389 ******
loss: 0.33692601323127747
***** iteration: 2390 ******
loss: 0.24203582108020782
***** iteration: 2391 ******
loss: 0.8834976553916931
***** iteration: 2392 ******
loss: 0.42878076434135437
***** iteration: 2393 ******
loss: 0.30615997314453125
***** iteration: 2394 ******
loss: 0.28712189197540283
***** iteration: 2395 ******
loss: 0.27793222665786743
***** iteration: 2396 ******
loss: 0.3761226534843445
***** iteration: 2397 ******
loss: 0.37239471077919006
***** iteration: 2398 ******
loss: 0.34557995200157166
***** iteration: 2399 ******
loss: 0.36477649211883545
***** iteration: 2400 ******
loss: 0.3888151943683624
***** iteration: 2401 ******
loss: 0.37435680627822876
***** iteration: 2402 ******
loss: 0.3357963263988495
***** iteration: 2403 ******
loss: 0.32347556948661804
***** iteration: 2404 ******
loss: 0.3271029591560364
***** iteration: 2405 ******
loss: 0.41982582211494446
***** iteration: 2406 ******
loss: 0.2969764769077301
***** iteration: 2407 ******
loss: 0.35559144616127014
***** iteration: 2408 ******
loss: 0.3476355969905853
***** iteration: 2409 ******
loss: 0.36120107769966125
***** iteration: 2410 ******
loss: 0.3620467483997345
***** iteration: 2411 ******
loss: 0.3415830433368683
***** iteration: 2412 ******
loss: 0.3390474319458008
***** iteration: 2413 ******
loss: 0.3305162787437439
***** iteration: 2414 ******
loss: 0.24644646048545837
***** iteration: 2415 ******
loss: 0.25550389289855957
***** iteration: 2416 ******
loss: 0.2526024281978607
***** iteration: 2417 ******
loss: 0.2362571358680725
***** iteration: 2418 ******
loss: 0.2724592387676239
***** iteration: 2419 ******
loss: 0.9752164483070374
***** iteration: 2420 ******
loss: 0.32813820242881775
***** iteration: 2421 ******
loss: 0.3124571740627289
***** iteration: 2422 ******
loss: 0.2759479284286499
***** iteration: 2423 ******
loss: 1.0510209798812866
***** iteration: 2424 ******
loss: 1.7183603048324585
***** iteration: 2425 ******
loss: 0.5078169703483582
***** iteration: 2426 ******
loss: 0.3395911157131195
***** iteration: 2427 ******
loss: 0.4585370123386383
***** iteration: 2428 ******
loss: 0.5099196434020996
***** iteration: 2429 ******
loss: 0.4520857632160187
***** iteration: 2430 ******
loss: 0.5720930099487305
***** iteration: 2431 ******
loss: 0.4867429733276367
***** iteration: 2432 ******
loss: 0.3171575367450714
***** iteration: 2433 ******
loss: 0.3986451327800751
***** iteration: 2434 ******
loss: 0.5407062768936157
***** iteration: 2435 ******
loss: 0.27465686202049255
***** iteration: 2436 ******
loss: 0.3698473870754242
***** iteration: 2437 ******
loss: 0.34066879749298096
***** iteration: 2438 ******
loss: 0.298175185918808
***** iteration: 2439 ******
loss: 0.27115777134895325
***** iteration: 2440 ******
loss: 0.5662083625793457
***** iteration: 2441 ******
loss: 0.4262276589870453
***** iteration: 2442 ******
loss: 0.3500494658946991
***** iteration: 2443 ******
loss: 0.3222324550151825
***** iteration: 2444 ******
loss: 0.34822219610214233
***** iteration: 2445 ******
loss: 0.27511006593704224
***** iteration: 2446 ******
loss: 0.30277907848358154
***** iteration: 2447 ******
loss: 1.1385515928268433
***** iteration: 2448 ******
loss: 0.9413790702819824
***** iteration: 2449 ******
loss: 0.2599255442619324
***** iteration: 2450 ******
loss: 0.276420533657074
***** iteration: 2451 ******
loss: 0.26009470224380493
***** iteration: 2452 ******
loss: 0.5843556523323059
***** iteration: 2453 ******
loss: 0.6425822973251343
***** iteration: 2454 ******
loss: 0.24079830944538116
***** iteration: 2455 ******
loss: 0.29689157009124756
***** iteration: 2456 ******
loss: 0.2652748227119446
***** iteration: 2457 ******
loss: 0.32034388184547424
***** iteration: 2458 ******
loss: 0.34218597412109375
***** iteration: 2459 ******
loss: 0.22477416694164276
***** iteration: 2460 ******
loss: 0.2513063848018646
***** iteration: 2461 ******
loss: 0.23644617199897766
***** iteration: 2462 ******
loss: 0.26604896783828735
***** iteration: 2463 ******
loss: 0.24892419576644897
***** iteration: 2464 ******
loss: 0.4100653827190399
***** iteration: 2465 ******
loss: 0.2893284261226654
***** iteration: 2466 ******
loss: 0.2769166827201843
***** iteration: 2467 ******
loss: 0.2644170820713043
***** iteration: 2468 ******
loss: 0.6777724623680115
***** iteration: 2469 ******
loss: 0.7011561393737793
***** iteration: 2470 ******
loss: 0.2842729687690735
***** iteration: 2471 ******
loss: 0.3006156086921692
***** iteration: 2472 ******
loss: 0.2869340479373932
***** iteration: 2473 ******
loss: 0.32776787877082825
***** iteration: 2474 ******
loss: 0.3080669045448303
***** iteration: 2475 ******
loss: 0.22298596799373627
***** iteration: 2476 ******
loss: 0.5735650062561035
***** iteration: 2477 ******
loss: 0.7416703701019287
***** iteration: 2478 ******
loss: 1.2225064039230347
***** iteration: 2479 ******
loss: 0.30239030718803406
***** iteration: 2480 ******
loss: 0.43050286173820496
***** iteration: 2481 ******
loss: 0.49074533581733704
***** iteration: 2482 ******
loss: 0.40068018436431885
***** iteration: 2483 ******
loss: 0.37471655011177063
***** iteration: 2484 ******
loss: 0.5295211672782898
***** iteration: 2485 ******
loss: 0.6089116930961609
***** iteration: 2486 ******
loss: 0.3830396234989166
***** iteration: 2487 ******
loss: 0.48435041308403015
***** iteration: 2488 ******
loss: 0.5606037974357605
***** iteration: 2489 ******
loss: 0.47487738728523254
***** iteration: 2490 ******
loss: 0.32100310921669006
***** iteration: 2491 ******
loss: 0.5082587003707886
***** iteration: 2492 ******
loss: 0.610444188117981
***** iteration: 2493 ******
loss: 0.3750215768814087
***** iteration: 2494 ******
loss: 0.3791049122810364
***** iteration: 2495 ******
loss: 0.4875105023384094
***** iteration: 2496 ******
loss: 0.46207886934280396
***** iteration: 2497 ******
loss: 0.35248807072639465
***** iteration: 2498 ******
loss: 0.4300815165042877
***** iteration: 2499 ******
loss: 0.6596671342849731
***** iteration: 2500 ******
loss: 0.49507540464401245
***** iteration: 2501 ******
loss: 0.2920936942100525
***** iteration: 2502 ******
loss: 0.603095293045044
***** iteration: 2503 ******
loss: 0.5498035550117493
***** iteration: 2504 ******
loss: 0.6049157381057739
***** iteration: 2505 ******
loss: 0.5599332451820374
***** iteration: 2506 ******
loss: 0.3907005190849304
***** iteration: 2507 ******
loss: 0.7596164345741272
***** iteration: 2508 ******
loss: 1.1889539957046509
***** iteration: 2509 ******
loss: 1.1266065835952759
***** iteration: 2510 ******
loss: 0.629917562007904
***** iteration: 2511 ******
loss: 0.2755099833011627
***** iteration: 2512 ******
loss: 0.458814412355423
***** iteration: 2513 ******
loss: 0.4939059913158417
***** iteration: 2514 ******
loss: 0.5833132266998291
***** iteration: 2515 ******
loss: 0.503244936466217
***** iteration: 2516 ******
loss: 0.39076370000839233
***** iteration: 2517 ******
loss: 0.5795162916183472
***** iteration: 2518 ******
loss: 0.8898611068725586
***** iteration: 2519 ******
loss: 0.7647681832313538
***** iteration: 2520 ******
loss: 0.3978537321090698
***** iteration: 2521 ******
loss: 0.43646886944770813
***** iteration: 2522 ******
loss: 0.588384747505188
***** iteration: 2523 ******
loss: 0.6094942092895508
***** iteration: 2524 ******
loss: 0.557850182056427
***** iteration: 2525 ******
loss: 0.40466612577438354
***** iteration: 2526 ******
loss: 0.5314807295799255
***** iteration: 2527 ******
loss: 0.8656935691833496
***** iteration: 2528 ******
loss: 0.7861562967300415
***** iteration: 2529 ******
loss: 0.4144301414489746
***** iteration: 2530 ******
loss: 0.3950495421886444
***** iteration: 2531 ******
loss: 0.5436651110649109
***** iteration: 2532 ******
loss: 0.5714353322982788
***** iteration: 2533 ******
loss: 0.4466122090816498
***** iteration: 2534 ******
loss: 0.32551416754722595
***** iteration: 2535 ******
loss: 0.7820974588394165
***** iteration: 2536 ******
loss: 0.6312339305877686
***** iteration: 2537 ******
loss: 0.4905749261379242
***** iteration: 2538 ******
loss: 0.38463231921195984
***** iteration: 2539 ******
loss: 0.48699668049812317
***** iteration: 2540 ******
loss: 0.5478456020355225
***** iteration: 2541 ******
loss: 0.45363810658454895
***** iteration: 2542 ******
loss: 0.2737940549850464
***** iteration: 2543 ******
loss: 1.9900439977645874
***** iteration: 2544 ******
loss: 3.170872449874878
***** iteration: 2545 ******
loss: 2.297147035598755
***** iteration: 2546 ******
loss: 1.863104224205017
***** iteration: 2547 ******
loss: 1.7224730253219604
***** iteration: 2548 ******
loss: 1.1412814855575562
***** iteration: 2549 ******
loss: 0.5003392696380615
***** iteration: 2550 ******
loss: 0.7609679698944092
***** iteration: 2551 ******
loss: 1.0005669593811035
***** iteration: 2552 ******
loss: 1.0625993013381958
***** iteration: 2553 ******
loss: 0.9645703434944153
***** iteration: 2554 ******
loss: 0.7226657271385193
***** iteration: 2555 ******
loss: 0.3832227289676666
***** iteration: 2556 ******
loss: 0.7741235494613647
***** iteration: 2557 ******
loss: 1.7134188413619995
***** iteration: 2558 ******
loss: 2.5589261054992676
***** iteration: 2559 ******
loss: 2.081364393234253
***** iteration: 2560 ******
loss: 1.823878288269043
***** iteration: 2561 ******
loss: 1.2439050674438477
***** iteration: 2562 ******
loss: 0.48616883158683777
***** iteration: 2563 ******
loss: 0.6142231225967407
***** iteration: 2564 ******
loss: 0.8647955656051636
***** iteration: 2565 ******
loss: 0.9360728859901428
***** iteration: 2566 ******
loss: 0.8542917370796204
***** iteration: 2567 ******
loss: 0.6275944113731384
***** iteration: 2568 ******
loss: 0.33299633860588074
***** iteration: 2569 ******
loss: 0.8766626715660095
***** iteration: 2570 ******
loss: 2.309027671813965
***** iteration: 2571 ******
loss: 2.2880406379699707
***** iteration: 2572 ******
loss: 2.2100508213043213
***** iteration: 2573 ******
loss: 2.014329671859741
***** iteration: 2574 ******
loss: 1.3829540014266968
***** iteration: 2575 ******
loss: 0.5749131441116333
***** iteration: 2576 ******
loss: 0.6287084221839905
***** iteration: 2577 ******
loss: 0.8813015222549438
***** iteration: 2578 ******
loss: 0.9573971033096313
***** iteration: 2579 ******
loss: 0.870753288269043
***** iteration: 2580 ******
loss: 0.6381445527076721
***** iteration: 2581 ******
loss: 0.33633673191070557
***** iteration: 2582 ******
loss: 1.0901665687561035
***** iteration: 2583 ******
loss: 2.738147020339966
***** iteration: 2584 ******
loss: 2.609194040298462
***** iteration: 2585 ******
loss: 2.4434988498687744
***** iteration: 2586 ******
loss: 2.378293752670288
***** iteration: 2587 ******
loss: 1.9215595722198486
***** iteration: 2588 ******
loss: 1.0387625694274902
***** iteration: 2589 ******
loss: 0.5187702178955078
***** iteration: 2590 ******
loss: 0.763891875743866
***** iteration: 2591 ******
loss: 0.9677477478981018
***** iteration: 2592 ******
loss: 0.9966855645179749
***** iteration: 2593 ******
loss: 0.8681800961494446
***** iteration: 2594 ******
loss: 0.6661394834518433
***** iteration: 2595 ******
loss: 0.5575868487358093
***** iteration: 2596 ******
loss: 0.3595169186592102
***** iteration: 2597 ******
loss: 0.5448500514030457
***** iteration: 2598 ******
loss: 0.5713530778884888
***** iteration: 2599 ******
loss: 0.45604535937309265
***** iteration: 2600 ******
loss: 0.49763959646224976
***** iteration: 2601 ******
loss: 0.557156503200531
***** iteration: 2602 ******
loss: 0.4974674880504608
***** iteration: 2603 ******
loss: 0.3387145400047302
***** iteration: 2604 ******
loss: 0.5333201289176941
***** iteration: 2605 ******
loss: 0.97828608751297
***** iteration: 2606 ******
loss: 0.9069305062294006
***** iteration: 2607 ******
loss: 0.7166897654533386
***** iteration: 2608 ******
loss: 0.43099308013916016
***** iteration: 2609 ******
loss: 0.4958432614803314
***** iteration: 2610 ******
loss: 0.6281773447990417
***** iteration: 2611 ******
loss: 0.6038622856140137
***** iteration: 2612 ******
loss: 0.44280147552490234
***** iteration: 2613 ******
loss: 0.4677642583847046
***** iteration: 2614 ******
loss: 0.5771840214729309
***** iteration: 2615 ******
loss: 0.7048451900482178
***** iteration: 2616 ******
loss: 0.5618808269500732
***** iteration: 2617 ******
loss: 0.3992437720298767
***** iteration: 2618 ******
loss: 0.48372212052345276
***** iteration: 2619 ******
loss: 0.5391864776611328
***** iteration: 2620 ******
loss: 0.6212729215621948
***** iteration: 2621 ******
loss: 0.41326314210891724
***** iteration: 2622 ******
loss: 0.3372591733932495
***** iteration: 2623 ******
loss: 0.41387099027633667
***** iteration: 2624 ******
loss: 0.40279507637023926
***** iteration: 2625 ******
loss: 0.31619682908058167
***** iteration: 2626 ******
loss: 0.31376755237579346
***** iteration: 2627 ******
loss: 0.956108570098877
***** iteration: 2628 ******
loss: 0.3406083285808563
***** iteration: 2629 ******
loss: 0.2848716676235199
***** iteration: 2630 ******
loss: 1.0046117305755615
***** iteration: 2631 ******
loss: 1.230780005455017
***** iteration: 2632 ******
loss: 0.5199227333068848
***** iteration: 2633 ******
loss: 0.39955437183380127
***** iteration: 2634 ******
loss: 0.4183809459209442
***** iteration: 2635 ******
loss: 0.4945836067199707
***** iteration: 2636 ******
loss: 0.43749672174453735
***** iteration: 2637 ******
loss: 0.3988991975784302
***** iteration: 2638 ******
loss: 0.3937133848667145
***** iteration: 2639 ******
loss: 0.2635838985443115
***** iteration: 2640 ******
loss: 1.7271015644073486
***** iteration: 2641 ******
loss: 2.56134033203125
***** iteration: 2642 ******
loss: 1.4281673431396484
***** iteration: 2643 ******
loss: 0.740055501461029
***** iteration: 2644 ******
loss: 0.7615391612052917
***** iteration: 2645 ******
loss: 0.6283015012741089
***** iteration: 2646 ******
loss: 0.4185844659805298
***** iteration: 2647 ******
loss: 0.7966738939285278
***** iteration: 2648 ******
loss: 1.225595474243164
***** iteration: 2649 ******
loss: 1.1768884658813477
***** iteration: 2650 ******
loss: 0.7292386293411255
***** iteration: 2651 ******
loss: 0.4543192684650421
***** iteration: 2652 ******
loss: 0.5136162638664246
***** iteration: 2653 ******
loss: 0.44771042466163635
***** iteration: 2654 ******
loss: 0.5053999423980713
***** iteration: 2655 ******
loss: 0.6446706652641296
***** iteration: 2656 ******
loss: 0.5049890875816345
***** iteration: 2657 ******
loss: 0.4367847144603729
***** iteration: 2658 ******
loss: 0.42433544993400574
***** iteration: 2659 ******
loss: 0.45584890246391296
***** iteration: 2660 ******
loss: 0.37631478905677795
***** iteration: 2661 ******
loss: 0.46004337072372437
***** iteration: 2662 ******
loss: 0.6698995232582092
***** iteration: 2663 ******
loss: 0.4507341980934143
***** iteration: 2664 ******
loss: 0.33968421816825867
***** iteration: 2665 ******
loss: 0.3592239320278168
***** iteration: 2666 ******
loss: 0.4323855936527252
***** iteration: 2667 ******
loss: 0.301129549741745
***** iteration: 2668 ******
loss: 0.3486538827419281
***** iteration: 2669 ******
loss: 0.3704070746898651
***** iteration: 2670 ******
loss: 0.3275463879108429
***** iteration: 2671 ******
loss: 0.2784557342529297
***** iteration: 2672 ******
loss: 0.33063384890556335
***** iteration: 2673 ******
loss: 0.2769452929496765
***** iteration: 2674 ******
loss: 0.28066515922546387
***** iteration: 2675 ******
loss: 0.24560101330280304
***** iteration: 2676 ******
loss: 2.268540143966675
***** iteration: 2677 ******
loss: 1.3471755981445312
***** iteration: 2678 ******
loss: 0.5257571339607239
***** iteration: 2679 ******
loss: 0.2812781035900116
***** iteration: 2680 ******
loss: 0.39389362931251526
***** iteration: 2681 ******
loss: 0.38151082396507263
***** iteration: 2682 ******
loss: 0.41966912150382996
***** iteration: 2683 ******
loss: 0.4224858582019806
***** iteration: 2684 ******
loss: 0.32273831963539124
***** iteration: 2685 ******
loss: 0.4313841164112091
***** iteration: 2686 ******
loss: 1.5007508993148804
***** iteration: 2687 ******
loss: 1.5958133935928345
***** iteration: 2688 ******
loss: 0.3642621636390686
***** iteration: 2689 ******
loss: 0.33444371819496155
***** iteration: 2690 ******
loss: 0.3829224407672882
***** iteration: 2691 ******
loss: 0.5020030736923218
***** iteration: 2692 ******
loss: 0.4845051169395447
***** iteration: 2693 ******
loss: 0.41319361329078674
***** iteration: 2694 ******
loss: 0.3844960033893585
***** iteration: 2695 ******
loss: 0.327236533164978
***** iteration: 2696 ******
loss: 0.3798080086708069
***** iteration: 2697 ******
loss: 0.4425979256629944
***** iteration: 2698 ******
loss: 0.4092627167701721
***** iteration: 2699 ******
loss: 0.2892904579639435
***** iteration: 2700 ******
loss: 0.349737286567688
***** iteration: 2701 ******
loss: 0.3338589370250702
***** iteration: 2702 ******
loss: 0.37173497676849365
***** iteration: 2703 ******
loss: 0.3821657598018646
***** iteration: 2704 ******
loss: 0.3148238956928253
***** iteration: 2705 ******
loss: 0.4343912601470947
***** iteration: 2706 ******
loss: 0.4615035653114319
***** iteration: 2707 ******
loss: 0.41200071573257446
***** iteration: 2708 ******
loss: 0.411140114068985
***** iteration: 2709 ******
loss: 0.38790014386177063
***** iteration: 2710 ******
loss: 0.3156173527240753
***** iteration: 2711 ******
loss: 0.721152126789093
***** iteration: 2712 ******
loss: 0.9874641299247742
***** iteration: 2713 ******
loss: 0.5495639443397522
***** iteration: 2714 ******
loss: 0.5356718301773071
***** iteration: 2715 ******
loss: 0.40248215198516846
***** iteration: 2716 ******
loss: 0.7422487735748291
***** iteration: 2717 ******
loss: 1.122619867324829
***** iteration: 2718 ******
loss: 1.028608798980713
***** iteration: 2719 ******
loss: 0.5565784573554993
***** iteration: 2720 ******
loss: 0.43525099754333496
***** iteration: 2721 ******
loss: 0.5228821039199829
***** iteration: 2722 ******
loss: 0.45180851221084595
***** iteration: 2723 ******
loss: 0.3700990080833435
***** iteration: 2724 ******
loss: 0.3821580111980438
***** iteration: 2725 ******
loss: 0.4616991877555847
***** iteration: 2726 ******
loss: 0.3576048016548157
***** iteration: 2727 ******
loss: 0.36949965357780457
***** iteration: 2728 ******
loss: 0.4056861102581024
***** iteration: 2729 ******
loss: 0.3086560070514679
***** iteration: 2730 ******
loss: 0.41940611600875854
***** iteration: 2731 ******
loss: 0.790614664554596
***** iteration: 2732 ******
loss: 0.8068376779556274
***** iteration: 2733 ******
loss: 0.4793786406517029
***** iteration: 2734 ******
loss: 0.3216359615325928
***** iteration: 2735 ******
loss: 0.3773791491985321
***** iteration: 2736 ******
loss: 0.327497273683548
***** iteration: 2737 ******
loss: 0.378661572933197
***** iteration: 2738 ******
loss: 0.4579075872898102
***** iteration: 2739 ******
loss: 0.3825627267360687
***** iteration: 2740 ******
loss: 0.35944825410842896
***** iteration: 2741 ******
loss: 0.373029500246048
***** iteration: 2742 ******
loss: 0.3298443853855133
***** iteration: 2743 ******
loss: 0.3385651707649231
***** iteration: 2744 ******
loss: 0.3248715102672577
***** iteration: 2745 ******
loss: 0.3312094807624817
***** iteration: 2746 ******
loss: 0.361133337020874
***** iteration: 2747 ******
loss: 0.2881988286972046
***** iteration: 2748 ******
loss: 0.4117497503757477
***** iteration: 2749 ******
loss: 0.45894232392311096
***** iteration: 2750 ******
loss: 0.284621924161911
***** iteration: 2751 ******
loss: 0.38660919666290283
***** iteration: 2752 ******
loss: 0.4612247943878174
***** iteration: 2753 ******
loss: 0.4032723307609558
***** iteration: 2754 ******
loss: 0.2570361793041229
***** iteration: 2755 ******
loss: 0.5642667412757874
***** iteration: 2756 ******
loss: 0.8822171688079834
***** iteration: 2757 ******
loss: 0.7322311401367188
***** iteration: 2758 ******
loss: 0.6696469187736511
***** iteration: 2759 ******
loss: 0.2640317976474762
***** iteration: 2760 ******
loss: 0.2878231704235077
***** iteration: 2761 ******
loss: 0.2594980299472809
***** iteration: 2762 ******
loss: 0.8185387253761292
***** iteration: 2763 ******
loss: 0.6043081879615784
***** iteration: 2764 ******
loss: 0.3877493143081665
***** iteration: 2765 ******
loss: 0.357349157333374
***** iteration: 2766 ******
loss: 0.37378329038619995
***** iteration: 2767 ******
loss: 0.31710782647132874
***** iteration: 2768 ******
loss: 0.5983308553695679
***** iteration: 2769 ******
loss: 1.038245439529419
***** iteration: 2770 ******
loss: 0.28492051362991333
***** iteration: 2771 ******
loss: 0.342258483171463
***** iteration: 2772 ******
loss: 0.37894678115844727
***** iteration: 2773 ******
loss: 0.38269615173339844
***** iteration: 2774 ******
loss: 0.33472079038619995
***** iteration: 2775 ******
loss: 0.2622395157814026
***** iteration: 2776 ******
loss: 1.640163779258728
***** iteration: 2777 ******
loss: 2.636625289916992
***** iteration: 2778 ******
loss: 1.644803762435913
***** iteration: 2779 ******
loss: 0.3041582405567169
***** iteration: 2780 ******
loss: 0.8431704640388489
***** iteration: 2781 ******
loss: 0.4712270200252533
***** iteration: 2782 ******
loss: 0.4907139539718628
***** iteration: 2783 ******
loss: 0.4418349266052246
***** iteration: 2784 ******
loss: 0.45447516441345215
***** iteration: 2785 ******
loss: 0.4298449158668518
***** iteration: 2786 ******
loss: 0.36734843254089355
***** iteration: 2787 ******
loss: 0.3836551010608673
***** iteration: 2788 ******
loss: 1.2514729499816895
***** iteration: 2789 ******
loss: 1.6059913635253906
***** iteration: 2790 ******
loss: 0.49568799138069153
***** iteration: 2791 ******
loss: 0.36115363240242004
***** iteration: 2792 ******
loss: 0.41405317187309265
***** iteration: 2793 ******
loss: 0.43211525678634644
***** iteration: 2794 ******
loss: 0.4168068766593933
***** iteration: 2795 ******
loss: 0.3639521598815918
***** iteration: 2796 ******
loss: 0.2882912755012512
***** iteration: 2797 ******
loss: 0.7318177223205566
***** iteration: 2798 ******
loss: 0.6831660866737366
***** iteration: 2799 ******
loss: 0.394674152135849
***** iteration: 2800 ******
loss: 0.33814099431037903
***** iteration: 2801 ******
loss: 0.5254523158073425
***** iteration: 2802 ******
loss: 0.695540726184845
***** iteration: 2803 ******
loss: 0.46864157915115356
***** iteration: 2804 ******
loss: 0.3377350866794586
***** iteration: 2805 ******
loss: 0.3565729856491089
***** iteration: 2806 ******
loss: 0.2690090835094452
***** iteration: 2807 ******
loss: 0.34037959575653076
***** iteration: 2808 ******
loss: 0.373683363199234
***** iteration: 2809 ******
loss: 0.5770086646080017
***** iteration: 2810 ******
loss: 0.29431769251823425
***** iteration: 2811 ******
loss: 0.26898613572120667
***** iteration: 2812 ******
loss: 0.4247802495956421
***** iteration: 2813 ******
loss: 0.49798116087913513
***** iteration: 2814 ******
loss: 0.32659363746643066
***** iteration: 2815 ******
loss: 0.38470911979675293
***** iteration: 2816 ******
loss: 0.4389716684818268
***** iteration: 2817 ******
loss: 0.34642457962036133
***** iteration: 2818 ******
loss: 0.45461130142211914
***** iteration: 2819 ******
loss: 0.6201026439666748
***** iteration: 2820 ******
loss: 0.5227838754653931
***** iteration: 2821 ******
loss: 0.3337918817996979
***** iteration: 2822 ******
loss: 0.42719972133636475
***** iteration: 2823 ******
loss: 0.6075073480606079
***** iteration: 2824 ******
loss: 0.45283037424087524
***** iteration: 2825 ******
loss: 0.35621345043182373
***** iteration: 2826 ******
loss: 0.540465772151947
***** iteration: 2827 ******
loss: 0.7846443057060242
***** iteration: 2828 ******
loss: 0.6141635775566101
***** iteration: 2829 ******
loss: 0.3391211926937103
***** iteration: 2830 ******
loss: 0.4434043765068054
***** iteration: 2831 ******
loss: 1.0254064798355103
***** iteration: 2832 ******
loss: 1.0268422365188599
***** iteration: 2833 ******
loss: 0.5913736820220947
***** iteration: 2834 ******
loss: 0.5159768462181091
***** iteration: 2835 ******
loss: 0.33040469884872437
***** iteration: 2836 ******
loss: 0.780130922794342
***** iteration: 2837 ******
loss: 1.2421327829360962
***** iteration: 2838 ******
loss: 1.2132328748703003
***** iteration: 2839 ******
loss: 0.7757608890533447
***** iteration: 2840 ******
loss: 0.2875969111919403
***** iteration: 2841 ******
loss: 0.42766451835632324
***** iteration: 2842 ******
loss: 0.6173928380012512
***** iteration: 2843 ******
loss: 0.6179784536361694
***** iteration: 2844 ******
loss: 0.5096569657325745
***** iteration: 2845 ******
loss: 0.32173267006874084
***** iteration: 2846 ******
loss: 0.6347998976707458
***** iteration: 2847 ******
loss: 1.442157506942749
***** iteration: 2848 ******
loss: 1.3124624490737915
***** iteration: 2849 ******
loss: 0.8823680877685547
***** iteration: 2850 ******
loss: 0.4571463465690613
***** iteration: 2851 ******
loss: 0.4732411503791809
***** iteration: 2852 ******
loss: 0.6252814531326294
***** iteration: 2853 ******
loss: 0.6142050623893738
***** iteration: 2854 ******
loss: 0.4567849636077881
***** iteration: 2855 ******
loss: 0.3588123023509979
***** iteration: 2856 ******
loss: 0.5633596777915955
***** iteration: 2857 ******
loss: 0.6264522671699524
***** iteration: 2858 ******
loss: 0.3456536531448364
***** iteration: 2859 ******
loss: 0.3633219301700592
***** iteration: 2860 ******
loss: 0.41294851899147034
***** iteration: 2861 ******
loss: 0.33368247747421265
***** iteration: 2862 ******
loss: 0.3588484525680542
***** iteration: 2863 ******
loss: 0.45337483286857605
***** iteration: 2864 ******
loss: 0.3488912582397461
***** iteration: 2865 ******
loss: 0.29751265048980713
***** iteration: 2866 ******
loss: 0.31610262393951416
***** iteration: 2867 ******
loss: 0.2661186754703522
***** iteration: 2868 ******
loss: 0.34928521513938904
***** iteration: 2869 ******
loss: 0.37178170680999756
***** iteration: 2870 ******
loss: 0.34442803263664246
***** iteration: 2871 ******
loss: 0.3681618273258209
***** iteration: 2872 ******
loss: 0.3311809301376343
***** iteration: 2873 ******
loss: 0.3136082887649536
***** iteration: 2874 ******
loss: 0.7058879733085632
***** iteration: 2875 ******
loss: 0.5819404721260071
***** iteration: 2876 ******
loss: 0.3125409781932831
***** iteration: 2877 ******
loss: 0.3885236084461212
***** iteration: 2878 ******
loss: 0.40046367049217224
***** iteration: 2879 ******
loss: 0.3189069330692291
***** iteration: 2880 ******
loss: 0.7187618613243103
***** iteration: 2881 ******
loss: 1.131142258644104
***** iteration: 2882 ******
loss: 0.64056396484375
***** iteration: 2883 ******
loss: 0.41815003752708435
***** iteration: 2884 ******
loss: 0.42265188694000244
***** iteration: 2885 ******
loss: 0.5100264549255371
***** iteration: 2886 ******
loss: 0.4486197233200073
***** iteration: 2887 ******
loss: 0.35361793637275696
***** iteration: 2888 ******
loss: 0.33247822523117065
***** iteration: 2889 ******
loss: 0.6378807425498962
***** iteration: 2890 ******
loss: 0.6137335300445557
***** iteration: 2891 ******
loss: 0.35868039727211
***** iteration: 2892 ******
loss: 0.3310345411300659
***** iteration: 2893 ******
loss: 0.41557690501213074
***** iteration: 2894 ******
loss: 0.4861026108264923
***** iteration: 2895 ******
loss: 0.3162567913532257
***** iteration: 2896 ******
loss: 0.3411703109741211
***** iteration: 2897 ******
loss: 0.5074937343597412
***** iteration: 2898 ******
loss: 0.42324742674827576
***** iteration: 2899 ******
loss: 0.34214434027671814
***** iteration: 2900 ******
loss: 0.3230072557926178
***** iteration: 2901 ******
loss: 0.4408826231956482
***** iteration: 2902 ******
loss: 0.36937999725341797
***** iteration: 2903 ******
loss: 0.33360111713409424
***** iteration: 2904 ******
loss: 0.32046762108802795
***** iteration: 2905 ******
loss: 0.3522402048110962
***** iteration: 2906 ******
loss: 0.3688131868839264
***** iteration: 2907 ******
loss: 0.3393324315547943
***** iteration: 2908 ******
loss: 0.357809841632843
***** iteration: 2909 ******
loss: 0.3646112382411957
***** iteration: 2910 ******
loss: 0.32770219445228577
***** iteration: 2911 ******
loss: 0.2964966595172882
***** iteration: 2912 ******
loss: 0.3144593834877014
***** iteration: 2913 ******
loss: 0.31001758575439453
***** iteration: 2914 ******
loss: 0.3420356214046478
***** iteration: 2915 ******
loss: 0.3185911178588867
***** iteration: 2916 ******
loss: 0.2994571924209595
***** iteration: 2917 ******
loss: 0.31447523832321167
***** iteration: 2918 ******
loss: 0.2602646052837372
***** iteration: 2919 ******
loss: 0.6981806755065918
***** iteration: 2920 ******
loss: 0.7638388276100159
***** iteration: 2921 ******
loss: 0.5108368992805481
***** iteration: 2922 ******
loss: 0.29909655451774597
***** iteration: 2923 ******
loss: 0.4577353596687317
***** iteration: 2924 ******
loss: 0.544891357421875
***** iteration: 2925 ******
loss: 0.8003376126289368
***** iteration: 2926 ******
loss: 0.39386415481567383
***** iteration: 2927 ******
loss: 0.24335411190986633
***** iteration: 2928 ******
loss: 0.49130359292030334
***** iteration: 2929 ******
loss: 0.563360869884491
***** iteration: 2930 ******
loss: 0.3456197679042816
***** iteration: 2931 ******
loss: 0.4162714183330536
***** iteration: 2932 ******
loss: 0.48088786005973816
***** iteration: 2933 ******
loss: 0.49859726428985596
***** iteration: 2934 ******
loss: 0.3283572494983673
***** iteration: 2935 ******
loss: 0.41014596819877625
***** iteration: 2936 ******
loss: 0.566234827041626
***** iteration: 2937 ******
loss: 0.4095044434070587
***** iteration: 2938 ******
loss: 0.38125258684158325
***** iteration: 2939 ******
loss: 0.406412273645401
***** iteration: 2940 ******
loss: 0.3290785253047943
***** iteration: 2941 ******
loss: 0.30532824993133545
***** iteration: 2942 ******
loss: 0.298835426568985
***** iteration: 2943 ******
loss: 0.3258934020996094
***** iteration: 2944 ******
loss: 0.377521276473999
***** iteration: 2945 ******
loss: 0.33201223611831665
***** iteration: 2946 ******
loss: 0.3782276511192322
***** iteration: 2947 ******
loss: 0.4193495213985443
***** iteration: 2948 ******
loss: 0.3281491696834564
***** iteration: 2949 ******
loss: 0.2947657108306885
***** iteration: 2950 ******
loss: 0.2883462905883789
***** iteration: 2951 ******
loss: 0.26942017674446106
***** iteration: 2952 ******
loss: 0.43825212121009827
***** iteration: 2953 ******
loss: 0.23307523131370544
***** iteration: 2954 ******
loss: 0.2814405560493469
***** iteration: 2955 ******
loss: 0.29212257266044617
***** iteration: 2956 ******
loss: 0.7423078417778015
***** iteration: 2957 ******
loss: 0.35399994254112244
***** iteration: 2958 ******
loss: 0.31478017568588257
***** iteration: 2959 ******
loss: 0.26614055037498474
***** iteration: 2960 ******
loss: 0.29400524497032166
***** iteration: 2961 ******
loss: 0.264727383852005
***** iteration: 2962 ******
loss: 0.3042048513889313
***** iteration: 2963 ******
loss: 0.3262646198272705
***** iteration: 2964 ******
loss: 0.2891494035720825
***** iteration: 2965 ******
loss: 0.8368807435035706
***** iteration: 2966 ******
loss: 0.8462722301483154
***** iteration: 2967 ******
loss: 0.6955625414848328
***** iteration: 2968 ******
loss: 0.4691500663757324
***** iteration: 2969 ******
loss: 0.4457029104232788
***** iteration: 2970 ******
loss: 0.5333858132362366
***** iteration: 2971 ******
loss: 0.4691751003265381
***** iteration: 2972 ******
loss: 0.38368913531303406
***** iteration: 2973 ******
loss: 0.37512269616127014
***** iteration: 2974 ******
loss: 0.26197367906570435
***** iteration: 2975 ******
loss: 2.4419023990631104
***** iteration: 2976 ******
loss: 3.4495441913604736
***** iteration: 2977 ******
loss: 2.464663505554199
***** iteration: 2978 ******
loss: 0.3029453158378601
***** iteration: 2979 ******
loss: 0.36840954422950745
***** iteration: 2980 ******
loss: 0.5261319875717163
***** iteration: 2981 ******
loss: 0.5881481170654297
***** iteration: 2982 ******
loss: 0.5567923784255981
***** iteration: 2983 ******
loss: 0.5651328563690186
***** iteration: 2984 ******
loss: 0.5406043529510498
***** iteration: 2985 ******
loss: 0.6452707648277283
***** iteration: 2986 ******
loss: 0.44188931584358215
***** iteration: 2987 ******
loss: 0.36175021529197693
***** iteration: 2988 ******
loss: 0.25858074426651
***** iteration: 2989 ******
loss: 3.3587887287139893
***** iteration: 2990 ******
loss: 5.433785915374756
***** iteration: 2991 ******
loss: 5.431375503540039
***** iteration: 2992 ******
loss: 3.5352962017059326
***** iteration: 2993 ******
loss: 0.43221503496170044
***** iteration: 2994 ******
loss: 0.390431672334671
***** iteration: 2995 ******
loss: 0.4795207977294922
***** iteration: 2996 ******
loss: 0.5511459708213806
***** iteration: 2997 ******
loss: 0.5337619185447693
***** iteration: 2998 ******
loss: 0.5785369277000427
***** iteration: 2999 ******
loss: 0.5282217860221863
======== testing rumen/g__Prevotella_test_wrapper =========
x shape is: (127, 16384)
features shape is: (127, 16384)
subtypes shape is: (127,)
x_test.shape is: (127, 5) y_test.len is: 127
out_test is:
tensor([[  1.5793,  -5.1515, -10.6974],
        [  2.4754,  -5.7427, -10.9591],
        [  2.5145,  -5.7462, -10.9808],
        [  1.1196,  -4.8666, -10.5455],
        [  2.0650,  -5.4825, -10.8051],
        [  2.2329,  -5.5551, -10.9180],
        [  0.7242,  -4.5474, -10.5269],
        [  1.3274,  -4.9657, -10.6624],
        [  1.7929,  -5.2915, -10.7658],
        [  0.3343,  -4.2437, -10.5391],
        [  1.6075,  -5.1925, -10.6707],
        [  1.6912,  -5.1941, -10.7978],
        [  1.6809,  -5.1619, -10.8144],
        [  1.6021,  -5.1461, -10.7309],
        [  1.6419,  -5.1168, -10.8506],
        [  1.7950,  -5.2961, -10.7662],
        [  1.1265,  -4.7950, -10.6827],
        [  2.3514,  -5.7037, -10.8235],
        [  1.4911,  -5.1130, -10.6473],
        [  1.8309,  -5.2879, -10.8116],
        [  2.5564,  -5.7333, -11.0608],
        [  2.1492,  -5.4760, -10.9268],
        [  1.9484,  -5.3768, -10.8599],
        [  2.5213,  -5.7595, -10.9719],
        [  2.1077,  -5.4987, -10.8534],
        [  2.0953,  -5.4784, -10.8665],
        [  2.4707,  -5.7201, -10.9774],
        [  2.0011,  -5.4602, -10.7710],
        [  2.1198,  -5.4720, -10.9140],
        [  2.3773,  -5.6743, -10.9195],
        [  0.3749,  -4.2781, -10.4821],
        [  2.4748,  -5.7058, -10.9998],
        [  0.5126,  -4.3918, -10.5024],
        [  1.7287,  -5.2404, -10.7662],
        [  2.7227,  -5.8890, -11.0297],
        [  2.2082,  -5.4634, -11.0226],
        [  0.5046,  -4.3824, -10.5075],
        [  2.0844,  -5.4771, -10.8575],
        [  0.9839,  -4.7599, -10.5331],
        [  1.5376,  -5.1273, -10.6708],
        [  1.7791,  -5.2470, -10.8131],
        [  2.0152,  -5.4468, -10.8082],
        [  1.6884,  -5.2326, -10.7172],
        [  2.0097,  -5.4045, -10.8638],
        [  1.9621,  -5.3733, -10.8639],
        [  0.3573,  -4.3351, -10.3886],
        [  0.8021,  -4.6210, -10.5053],
        [  2.3549,  -5.6496, -10.9370],
        [  2.2794,  -5.4983, -11.0758],
        [  1.8972,  -5.3684, -10.7993],
        [  0.9084,  -4.7125, -10.5083],
        [  2.1937,  -5.5089, -10.9463],
        [  1.9995,  -5.3763, -10.9111],
        [  1.9272,  -5.3782, -10.8087],
        [  2.2147,  -5.5243, -10.9606],
        [  1.8777,  -5.3131, -10.8285],
        [  1.5872,  -5.1560, -10.6773],
        [  2.3472,  -5.6040, -10.9963],
        [  2.5338,  -5.7313, -11.0414],
        [  1.8340,  -5.2689, -10.8397],
        [  1.9674,  -5.3919, -10.8309],
        [  2.2682,  -5.5778, -10.9190],
        [  2.0263,  -5.4151, -10.8708],
        [  2.2561,  -5.6175, -10.8682],
        [  1.0895,  -4.7734, -10.6646],
        [  1.1480,  -4.7965, -10.6938],
        [  0.6469,  -4.4248, -10.6146],
        [  2.3794,  -5.6853, -10.9011],
        [  1.1562,  -4.8848, -10.5553],
        [  2.1359,  -5.5219, -10.8657],
        [  0.8578,  -4.6164, -10.6029],
        [  2.4652,  -5.7931, -10.8369],
        [  2.5054,  -5.7648, -10.9367],
        [  2.3109,  -5.6451, -10.8939],
        [  2.4865,  -5.7169, -11.0089],
        [  2.1126,  -5.4261, -10.9602],
        [  0.9646,  -4.5801, -10.7862],
        [  1.7018,  -5.2189, -10.7521],
        [  0.7051,  -4.5713, -10.4551],
        [  3.1810,  -5.8612, -11.6451],
        [  1.3687,  -4.9662, -10.7347],
        [  2.4216,  -5.6773, -10.9864],
        [  2.1017,  -5.4930, -10.8458],
        [  2.1643,  -5.5120, -10.9136],
        [  1.5976,  -4.9917, -10.9585],
        [  0.3408,  -4.2728, -10.4561],
        [  2.1229,  -5.4809, -10.8848],
        [  2.0909,  -5.4613, -10.8945],
        [  1.9520,  -5.3964, -10.8028],
        [  1.0967,  -4.7418, -10.7058],
        [  2.0620,  -5.5400, -10.7122],
        [  0.9376,  -4.6874, -10.5858],
        [  0.9733,  -4.7253, -10.5686],
        [  0.9005,  -4.6452, -10.6050],
        [  1.7260,  -5.2314, -10.7663],
        [  2.0338,  -5.4335, -10.8497],
        [  0.2720,  -4.1129, -10.8646],
        [  0.3887,  -4.2355, -10.5591],
        [  2.3381,  -5.5975, -10.9938],
        [  0.7897,  -4.5852, -10.5509],
        [  1.4676,  -5.0742, -10.6619],
        [  2.2610,  -5.5811, -10.9053],
        [  1.6997,  -5.2247, -10.7399],
        [  1.5496,  -5.1673, -10.6350],
        [  2.0609,  -5.4614, -10.8473],
        [  2.2309,  -5.6253, -10.8184],
        [  2.0806,  -5.4621, -10.8628],
        [  1.8468,  -5.3142, -10.8018],
        [  2.9241,  -6.0292, -11.0518],
        [  2.0291,  -5.4406, -10.8326],
        [  2.1998,  -5.5671, -10.8585],
        [  1.7466,  -5.2333, -10.7780],
        [  0.6224,  -4.5097, -10.4411],
        [  1.8455,  -5.3176, -10.8006],
        [  1.8546,  -5.2605, -10.8993],
        [  1.6900,  -5.1777, -10.7941],
        [  2.2610,  -5.5710, -10.9389],
        [  1.4167,  -5.0225, -10.7031],
        [  1.9577,  -5.3668, -10.8709],
        [  1.1079,  -4.7994, -10.6574],
        [  1.8055,  -5.2757, -10.8024],
        [  1.3320,  -4.9330, -10.7036],
        [  0.3678,  -4.3209, -10.4846],
        [  1.9813,  -5.4057, -10.8248],
        [  1.2693,  -4.9183, -10.6549],
        [  2.2419,  -5.5883, -10.8712],
        [  2.1656,  -5.5082, -10.8981]], grad_fn=<AddmmBackward>)
Number of rejected data: 0.00% (0/127)
Accuracy of non-rejected data: 0.00 % (0/127)
Test empirical 0-1-c risk: 1.000000
======== testing rumen/g__RC9_test_wrapper =========
x shape is: (52, 16384)
features shape is: (52, 16384)
subtypes shape is: (52,)
x_test.shape is: (52, 5) y_test.len is: 52
out_test is:
tensor([[  1.5940,  -4.9935, -10.9554],
        [  0.4680,  -4.2546, -10.6386],
        [  0.4760,  -4.3362, -10.5623],
        [  1.0916,  -4.6410, -10.8505],
        [  0.5621,  -4.4584, -10.4692],
        [  1.3764,  -4.8728, -10.8514],
        [  0.5170,  -4.2999, -10.7616],
        [  1.5236,  -5.1063, -10.6820],
        [  1.0539,  -4.6941, -10.6802],
        [  0.7193,  -4.6198, -10.3897],
        [  0.5229,  -4.3297, -10.5644],
        [  1.5943,  -5.2056, -10.6323],
        [  0.9008,  -4.6568, -10.5486],
        [  0.5336,  -4.3495, -10.5873],
        [  2.5749,  -5.8095, -10.9543],
        [  0.5184,  -4.2424, -11.2536],
        [  0.5986,  -4.5140, -10.3877],
        [  1.8679,  -5.3755, -10.7206],
        [  1.3687,  -5.0445, -10.5951],
        [  0.9711,  -4.6575, -10.6428],
        [  0.4774,  -4.2848, -10.6149],
        [  0.6832,  -4.5382, -10.4734],
        [  0.6605,  -4.4068, -10.6333],
        [  1.6722,  -5.2418, -10.6754],
        [  0.4869,  -4.4147, -10.5051],
        [  0.5388,  -4.3210, -11.0238],
        [  0.5308,  -4.2943, -10.6592],
        [  1.7412,  -5.1255, -10.9253],
        [  0.5242,  -4.3637, -10.8693],
        [  1.1073,  -4.8054, -10.5934],
        [  0.6878,  -4.4350, -10.6089],
        [  0.4444,  -4.2441, -10.6515],
        [  2.0291,  -5.3553, -10.9649],
        [  2.2379,  -5.5339, -10.9437],
        [  1.5920,  -5.0440, -10.8751],
        [  0.6152,  -4.4101, -10.5656],
        [  2.1446,  -5.4685, -10.9306],
        [  2.3307,  -5.5794, -10.9816],
        [  0.3973,  -4.2212, -10.6371],
        [  0.6189,  -4.4035, -10.5682],
        [  0.3652,  -4.2916, -10.5699],
        [  1.5444,  -5.0063, -10.8734],
        [  2.3671,  -5.6614, -10.9121],
        [  0.5030,  -4.5142, -10.2881],
        [  1.1245,  -4.8263, -10.5934],
        [  0.5063,  -4.3323, -10.5707],
        [  0.9006,  -4.7376, -10.4482],
        [  2.9612,  -6.0895, -11.0367],
        [  0.5450,  -4.3801, -10.5260],
        [  0.4104,  -4.2159, -10.6780],
        [  1.9956,  -5.3991, -10.8470],
        [  0.5286,  -4.2622, -11.4005]], grad_fn=<AddmmBackward>)
Number of rejected data: 0.00% (0/52)
Accuracy of non-rejected data: 0.00 % (0/52)
Test empirical 0-1-c risk: 1.000000
======== testing rumen/g__C941_test_wrapper =========
x shape is: (35, 16384)
features shape is: (35, 16384)
subtypes shape is: (35,)
x_test.shape is: (35, 5) y_test.len is: 35
out_test is:
tensor([[  1.3630,  -4.9960, -10.6591],
        [  1.5012,  -5.0270, -10.7942],
        [  0.3562,  -4.1941, -10.8257],
        [  1.8035,  -5.2858, -10.7828],
        [  1.6625,  -5.2093, -10.7234],
        [  0.2428,  -4.1373, -10.6075],
        [  0.3164,  -4.2634, -10.5238],
        [  1.6033,  -5.1912, -10.6811],
        [  0.4033,  -4.3085, -10.4906],
        [  2.0318,  -5.4520, -10.8016],
        [  1.5741,  -5.0982, -10.7606],
        [  0.4404,  -4.1631, -10.8231],
        [  0.3516,  -3.8663, -11.0779],
        [  0.8964,  -4.6924, -10.5102],
        [  0.3109,  -4.2342, -10.5073],
        [  0.3037,  -4.2534, -10.4444],
        [  0.5094,  -4.4213, -10.4258],
        [  1.2776,  -4.9271, -10.6599],
        [  0.8422,  -4.6313, -10.5414],
        [  1.0837,  -4.8125, -10.5750],
        [  0.8661,  -4.6164, -10.5917],
        [  0.5798,  -4.4232, -10.5234],
        [  0.7847,  -4.5990, -10.5317],
        [  1.3489,  -4.9407, -10.7231],
        [  0.6978,  -4.4708, -10.6119],
        [  1.2022,  -4.8717, -10.6372],
        [  1.1085,  -4.7950, -10.6409],
        [  0.8236,  -4.6104, -10.5493],
        [  0.2610,  -4.1016, -10.7210],
        [  0.9824,  -4.7350, -10.5658],
        [  1.4875,  -4.9516, -10.8581],
        [  0.4757,  -4.3822, -10.4511],
        [  0.2905,  -4.0387, -10.9671],
        [  1.3713,  -4.9960, -10.6871],
        [  1.2900,  -4.9764, -10.5931]], grad_fn=<AddmmBackward>)
Number of rejected data: 0.00% (0/35)
Accuracy of non-rejected data: 0.00 % (0/35)
Test empirical 0-1-c risk: 1.000000
======== testing rumen/f__Lachnospiraceae_test_wrapper =========
x shape is: (131, 16384)
features shape is: (131, 16384)
subtypes shape is: (131,)
x_test.shape is: (131, 5) y_test.len is: 131
out_test is:
tensor([[  0.3095,  -4.2727, -10.4861],
        [  2.4649,  -5.6532, -11.0652],
        [  2.8458,  -5.8190, -11.2632],
        [  0.8988,  -4.5783, -10.6801],
        [  2.0893,  -5.3420, -11.0423],
        [  0.6517,  -4.4672, -10.5512],
        [  2.6635,  -5.6135, -11.3219],
        [  0.4741,  -4.2160, -10.9053],
        [  2.7330,  -5.7300, -11.2758],
        [  2.5514,  -5.6633, -11.1637],
        [  2.3781,  -5.5068, -11.1830],
        [  0.7993,  -4.3434, -10.8625],
        [  3.0224,  -5.8277, -11.5286],
        [  1.3567,  -5.0697, -10.5598],
        [  1.7324,  -5.2970, -10.6603],
        [  1.2849,  -4.8674, -10.7447],
        [  3.2760,  -6.1323, -11.3649],
        [  1.2525,  -4.8450, -10.7424],
        [  0.6622,  -4.2775, -10.8183],
        [  1.9112,  -5.2855, -10.9287],
        [  0.6923,  -4.3659, -10.7386],
        [  2.5625,  -5.6760, -11.1678],
        [  2.3308,  -5.4982, -11.0938],
        [  2.7416,  -5.5985, -11.4712],
        [  0.8416,  -4.5077, -10.6861],
        [  1.4038,  -4.8746, -10.8764],
        [  2.6542,  -5.6411, -11.3191],
        [  1.8686,  -5.2604, -10.8962],
        [  2.0586,  -5.2914, -11.0833],
        [  0.8942,  -4.5858, -10.6570],
        [  1.9603,  -5.2624, -11.0068],
        [  0.6133,  -4.6224, -10.3049],
        [  3.3376,  -6.0961, -11.4846],
        [  1.5646,  -4.9039, -10.9876],
        [  2.1297,  -5.4896, -10.8790],
        [  0.6139,  -4.6275, -10.3320],
        [  1.1702,  -5.0272, -10.3668],
        [  2.2136,  -5.5143, -10.9422],
        [  1.3267,  -4.8269, -10.8693],
        [  1.9129,  -5.1868, -11.0253],
        [  2.4331,  -5.5997, -11.0961],
        [  3.2043,  -6.1489, -11.2414],
        [  1.4127,  -4.9332, -10.7811],
        [  2.2664,  -5.4924, -11.0226],
        [  2.9157,  -5.7968, -11.3849],
        [  3.1173,  -5.9825, -11.3403],
        [  0.6158,  -4.4895, -10.7973],
        [  1.9490,  -5.3767, -10.8397],
        [  0.4756,  -4.1645, -10.8366],
        [  1.6217,  -5.0828, -10.8305],
        [  0.3775,  -4.2420, -10.5678],
        [  1.1685,  -4.6810, -10.8332],
        [  2.7360,  -5.8571, -11.1009],
        [  1.5914,  -5.1426, -10.7152],
        [  2.9120,  -5.7019, -11.5409],
        [  2.3687,  -5.5754, -11.0419],
        [  3.1253,  -6.0551, -11.2967],
        [  2.8510,  -6.0124, -11.0122],
        [  0.4041,  -4.2122, -10.7904],
        [  0.3963,  -4.2896, -10.6310],
        [  2.1548,  -5.4027, -11.0100],
        [  0.4825,  -4.3300, -10.5204],
        [  0.9823,  -4.4458, -10.9348],
        [  2.5642,  -5.7193, -11.0778],
        [  2.3759,  -5.5908, -11.0244],
        [  2.9447,  -5.8311, -11.3863],
        [  1.4328,  -4.9125, -10.8615],
        [  2.6406,  -5.6386, -11.3130],
        [  1.6730,  -5.1030, -10.8872],
        [  0.7964,  -4.5706, -10.5744],
        [  2.3254,  -5.4821, -11.1316],
        [  0.8425,  -4.5755, -10.6504],
        [  2.5699,  -5.6573, -11.1777],
        [  1.1824,  -4.7355, -10.7897],
        [  1.5308,  -5.0937, -10.7122],
        [  1.3042,  -4.8837, -10.7565],
        [  0.6224,  -4.3825, -10.6107],
        [  2.0132,  -5.3161, -10.9550],
        [  1.3469,  -4.9420, -10.6777],
        [  1.9146,  -5.2266, -10.9779],
        [  0.7757,  -4.4457, -10.6868],
        [  0.9604,  -4.5689, -10.7440],
        [  1.5113,  -4.9172, -10.9473],
        [  1.8671,  -5.2049, -10.9518],
        [  3.0655,  -5.9712, -11.3369],
        [  0.8003,  -4.5271, -10.6402],
        [  2.3571,  -5.6152, -10.9635],
        [  1.2235,  -4.7608, -10.7739],
        [  2.2534,  -5.5053, -11.0386],
        [  1.6852,  -5.0535, -10.9505],
        [  2.1010,  -5.3727, -10.9839],
        [  0.5344,  -4.3123, -10.6040],
        [  1.9425,  -5.2728, -10.9186],
        [  3.0181,  -5.8945, -11.3959],
        [  2.0478,  -5.3355, -10.9767],
        [  1.4231,  -4.9740, -10.7486],
        [  0.6158,  -4.6997, -10.2841],
        [  2.4619,  -5.5688, -11.2083],
        [  0.9473,  -4.6353, -10.6506],
        [  2.9897,  -5.9634, -11.2515],
        [  2.7707,  -5.8117, -11.2150],
        [  1.5896,  -5.0540, -10.8548],
        [  0.4836,  -4.2844, -10.7664],
        [  2.9752,  -5.8745, -11.3602],
        [  1.8061,  -5.1887, -10.8735],
        [  0.4276,  -4.2886, -10.6582],
        [  0.7216,  -4.8237, -10.2068],
        [  2.5294,  -5.6187, -11.2106],
        [  0.9577,  -4.6421, -10.6719],
        [  0.5938,  -4.2423, -10.7561],
        [  1.6781,  -5.0130, -10.9741],
        [  2.6247,  -5.8361, -11.0187],
        [  1.8552,  -5.3025, -10.7827],
        [  1.2249,  -4.7280, -10.8757],
        [  1.9467,  -5.3833, -10.8636],
        [  0.7638,  -4.5167, -10.5705],
        [  2.0103,  -5.2962, -10.9947],
        [  2.1425,  -5.3256, -11.1239],
        [  3.0562,  -6.0128, -11.2509],
        [  3.0122,  -5.9460, -11.3179],
        [  1.0144,  -4.6983, -10.6375],
        [  1.3594,  -4.9216, -10.7526],
        [  2.1420,  -5.4338, -10.9637],
        [  1.5334,  -4.9079, -10.9894],
        [  1.7776,  -5.0980, -10.9920],
        [  0.3947,  -4.2363, -10.6574],
        [  2.7253,  -5.8075, -11.1567],
        [  0.7398,  -4.4594, -10.6874],
        [  1.9329,  -5.2044, -11.0085],
        [  1.5085,  -4.9620, -10.8525],
        [  2.2096,  -5.4077, -11.1271]], grad_fn=<AddmmBackward>)
Number of rejected data: 0.00% (0/131)
Accuracy of non-rejected data: 100.00 % (131/131)
Test empirical 0-1-c risk: 0.000000
======== testing rumen/g__Ruminococcus_E_test_wrapper =========
x shape is: (58, 16384)
features shape is: (58, 16384)
subtypes shape is: (58,)
x_test.shape is: (58, 5) y_test.len is: 58
out_test is:
tensor([[ -0.2401,  -3.8771, -10.4466],
        [ -0.2338,  -3.8711, -10.5556],
        [ -0.4236,  -3.6140, -10.8111],
        [  0.0448,  -4.0508, -10.4821],
        [ -0.2266,  -3.6119, -11.2849],
        [  1.0881,  -4.8274, -10.6313],
        [ -0.1731,  -3.8802, -10.4964],
        [  0.8029,  -4.5283, -10.7441],
        [  1.5676,  -5.1052, -10.8122],
        [ -0.2248,  -3.6856, -11.0414],
        [  0.0805,  -4.1213, -10.4310],
        [  0.0679,  -4.1088, -10.4590],
        [ -0.2882,  -3.5053, -11.3837],
        [  0.5235,  -4.3536, -10.6626],
        [  1.6303,  -5.1584, -10.8199],
        [ -0.1211,  -3.8482, -10.7246],
        [ -0.2229,  -3.7209, -10.9398],
        [ -0.0975,  -3.9112, -10.5299],
        [ -0.2156,  -3.7616, -10.8343],
        [ -0.3552,  -3.7938, -10.4395],
        [ -0.2013,  -3.8247, -10.7425],
        [  0.8294,  -4.6253, -10.5630],
        [  0.9488,  -4.6915, -10.6821],
        [  0.6955,  -4.5855, -10.4717],
        [  0.1636,  -4.2117, -10.3900],
        [  0.0408,  -3.9950, -10.6545],
        [ -0.1859,  -3.9149, -10.4889],
        [ -0.2997,  -3.5409, -11.2659],
        [ -0.2527,  -3.7269, -10.7504],
        [ -0.1682,  -3.9240, -10.4906],
        [  0.6863,  -4.4346, -10.7295],
        [ -0.2173,  -3.7071, -10.9278],
        [ -0.1951,  -3.9201, -10.4469],
        [  1.6978,  -5.1991, -10.8542],
        [ -0.1525,  -3.8695, -10.6788],
        [  1.4451,  -5.0348, -10.7682],
        [  0.1563,  -4.1524, -10.5037],
        [ -0.3097,  -3.5747, -11.0443],
        [ -0.1562,  -3.9395, -10.4085],
        [ -0.1057,  -3.9092, -10.7437],
        [ -0.2293,  -3.9118, -10.3608],
        [  0.6655,  -4.5317, -10.5563],
        [ -0.2305,  -3.7137, -10.9995],
        [  0.8683,  -4.6243, -10.6830],
        [  1.9891,  -5.3648, -10.9404],
        [ -0.2152,  -3.7308, -10.8656],
        [ -0.2312,  -3.6579, -10.9290],
        [ -0.1482,  -4.0152, -10.3040],
        [  0.0170,  -3.9976, -10.6267],
        [ -0.2854,  -3.5568, -11.1757],
        [ -0.2644,  -3.6315, -11.1384],
        [ -0.1491,  -3.9856, -10.3515],
        [ -0.1763,  -3.8894, -10.4580],
        [  0.4829,  -4.3123, -10.6657],
        [ -0.0136,  -3.9157, -10.9229],
        [ -0.2802,  -3.5802, -11.1831],
        [ -0.2987,  -3.5685, -11.2437],
        [  1.2633,  -4.8814, -10.7927]], grad_fn=<AddmmBackward>)
Number of rejected data: 60.34% (35/58)
Accuracy of non-rejected data: 100.00 % (23/23)
Test empirical 0-1-c risk: 0.150862
