# -*- coding: utf-8 -*-
"""simple_notebook_rejection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vplDAuD_PwIY9uM2gNSLKCZ3ELlxfiBi

# Written by Nontawat Charoenphakdee and Yivan Zhang.
"""

import numpy as np

import matplotlib.pyplot as plt
from matplotlib import colors
import math

np.random.seed(2020)

"""# Losses"""

sigmoid = lambda z: torch.sigmoid(-z)
ramp = lambda z: torch.clamp((1 - z) / 2, min=0, max=1)
logistic = lambda z: torch.log(1 + torch.exp(-z))
exponential = lambda z: torch.exp(-z)
hinge = lambda z: torch.clamp(1 - z, min=0)
squared = lambda z: (1 - z).pow(2)
unhinged = lambda z: 1 - z
savage = lambda z: torch.sigmoid(-2 * z).pow(2)
tangent = lambda z: (2 * torch.atan(z) - 1).pow(2)
almost = lambda z: torch.sigmoid(-2 * z).pow(1.5)

# classification-calibrated losses
losses = {
    # proper composite losses -> applicable to both confidence-based and cost-senstive approach
    "logistic": lambda z: torch.log(1 + torch.exp(-z)),
    "exp": lambda z: torch.exp(-z),
    "square": lambda z: (1 - z).pow(2),
    "savage": lambda z: torch.sigmoid(-2 * z).pow(2),
    "tangent": lambda z: (2 * torch.atan(z) - 1).pow(2),
    # non-proper composite losses -> not applicable to the confidence-based approach but applicable to the cost-sensitive approach  
    "sigmoid": lambda z: torch.sigmoid(-z),
    "hinge": lambda z: torch.clamp(1 - z, min=0),
    "unhinge": lambda z: 1 - z,
    "ramp": lambda z: torch.clamp((1 - z) / 2, min=0, max=1),
}

# link function is only available for proper composite losses
links = {
    "logistic": lambda p: math.log(p / (1 - p)),
    "exp": lambda p: 0.5 * math.log(p / (1 - p)),
    "square": lambda p: 2 * p - 1,
    "savage": lambda p: math.log(p / (1 - p)),
    "tangent": lambda p: math.tan(p - 0.5),
}

"""# Arguments"""

# loss for the one-vs-all (ova) approach
loss_name_ova = 'logistic'
# loss for the cost-sensitive approach
loss_name_cost = 'hinge'
rej_cost = 0.25

"""# Prepare data"""

# generate multivariate Gaussian sample by the Cholesky decomposition
# https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Drawing_values_from_the_distribution
dim_features = 2
num_classes = 3

mean = np.array([[-2, -2],
                 [1, 2],
                 [2, -2]])  # [num_classes, dim_features]
std = np.array([[1, 1],
                [1, 1],
                [1, 1]])  # [num_classes, dim_features]
corr = np.array([0, -.2, .4])  # [num_classes]

# calculate the covariance matrix
cov = np.zeros((num_classes, dim_features, dim_features))
for i in range(num_classes):
    cov[i] = np.diag(std[i]) @ np.array([[1, corr[i]], [corr[i], 1]]) @ np.diag(std[i])

num_clean_train = 800

y = np.random.choice(num_classes, size=num_clean_train)
x = mean[y] + (np.linalg.cholesky(cov)[y] @ np.random.randn(num_clean_train, dim_features, 1)).squeeze()
print(x.shape)
print(y.shape)

num_clean_test = 800
y_test = np.random.choice(num_classes, size=num_clean_test)
x_test = mean[y_test] + (np.linalg.cholesky(cov)[y_test] @ np.random.randn(num_clean_test, dim_features, 1)).squeeze()
print(y_test.shape)
print(x_test.shape)

# add an extra noisy cluster
mean_cluster = np.array([-4, 6])  # [dim_features]
num_extra_train = 200
num_extra_test = 200

x_cluster_train = mean_cluster + np.random.randn(num_extra_train, dim_features)
y_cluster_train = np.random.choice([0, 1], size=num_extra_train)
x = np.append(x, x_cluster_train, axis=0)
y = np.append(y, y_cluster_train, axis=0)

x_cluster_test = mean_cluster + np.random.randn(num_extra_test, dim_features)
y_cluster_test = np.random.choice([0, 1], size=num_extra_test)
x_test = np.append(x_test, x_cluster_test, axis=0)
y_test = np.append(y_test, y_cluster_test, axis=0)

print(x.shape)
print(y.shape)
print(y)

print(x_test.shape)
print(y_test.shape)
print(y_test)

# canvas
margin = 2
x_min = x.min(0) - margin 
x_max = x.max(0) + margin

print(x_min)
print(x_max)

# meshgrid for evaluation
n_mesh = 500
x_mesh = np.stack(np.meshgrid(np.linspace(x_min[0], x_max[0], n_mesh),
                              np.linspace(x_min[1], x_max[1], n_mesh)), -1)

x_plot = n_mesh * (x - x_min) / (x_max - x_min)
print(x.shape)
print(x_mesh)
print(x_mesh.shape)

# colors
color_list = ['xkcd:red', 'xkcd:green', 'xkcd:blue']
rgb_list = np.array([list(colors.to_rgb(c)) for c in color_list])
cmap = colors.ListedColormap(rgb_list)

def scatter(ax, x):
    # global: y, cmap
    ax.scatter(*x.T, 
               s=150,
               c=y, cmap=cmap,
               edgecolors='k', linewidth=1)

fig, ax = plt.subplots(figsize=(10, 10))
scatter(ax, x)

"""# Prepare model"""

import torch
from torch import nn, optim
import torch.nn.functional as F

from tqdm import trange

"""### Convert data to tensor"""

x_tensor = torch.tensor(x).float()
y_tensor = torch.tensor(y).long()
x_mesh_tensor = torch.tensor(x_mesh).float()
x_tensor_test = torch.tensor(x_test).float()
y_tensor_test = torch.tensor(y_test).long()

"""### Model"""

def get_mlp():
    model = nn.Sequential(
        nn.Linear(dim_features, 64),
        nn.ReLU(inplace=True),
        nn.Linear(64, num_classes),
    )
    return model

def train(model, optimizer, loss_func, x, y, iterations):
    for iteration in trange(iterations):
        # forward
        out = model(x)  # [batch_size, num_classes]
        loss = loss_func(out, y)
        # backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

def evaluate_mesh(model):
    # global: x_mesh_tensor, n_mesh, dim_features, num_classes
    with torch.no_grad():
        x_mesh_flatten = x_mesh_tensor.reshape(n_mesh * n_mesh, dim_features)
        out_mesh_flatten = model(x_mesh_flatten)
        out_mesh = out_mesh_flatten.reshape(n_mesh, n_mesh, num_classes)
    return out_mesh

def test(model):
    # global: x_mesh_tensor, n_mesh, dim_features, num_classes
    with torch.no_grad():
        out_test = model_ova(x_tensor_test)
        out_mesh = out_mesh_flatten.reshape(n_mesh, n_mesh, num_classes)
    return out_mesh

def contour(ax, data, boundary, color):
    # fill
    ax.contourf(data, levels=[boundary, 1e10],
                colors=color, alpha=0.2,
                zorder=1)
    # contour
    ax.contour(data, levels=[boundary],
               colors=color, linewidths=5)

"""---

# Cost-sensitive approach: https://arxiv.org/abs/2010.11748
"""

def cost_sensitive_loss(loss_func, rej_cost):
    def loss(out, y):
        batch_size, num_classes = out.shape
        mask = torch.eye(num_classes, num_classes).long()
        
        out_p = out.masked_select(mask[y] == 1)  # [batch_size]
        out_n = out.masked_select(mask[y] == 0)  # [batch_size * (num_classes - 1)]
        
        l_p = loss_func(out_p)  # [batch_size]
        l_n = loss_func(-out_n)  # [batch_size * (num_classes - 1)]
        
        # sum -> weighted average -> batch average
        return (rej_cost * l_p.sum() + (1 - rej_cost) * l_n.sum()) / batch_size
        
    return loss

"""## Train """

# setup
model_cost = get_mlp()
optimizer = optim.AdamW(model_cost.parameters())
loss_func = cost_sensitive_loss(losses[loss_name_cost], rej_cost=rej_cost)

# training
train(model_cost, optimizer, loss_func, x_tensor, y_tensor, 1000)

"""## Visualize"""

# evaluation
out_mesh = evaluate_mesh(model_cost)

# plot
fig, ax = plt.subplots(figsize=(10, 10))
ax.set_title(f'Cost-sensitive:{loss_name_cost}', fontsize=20)
ax.set_xticks([])
ax.set_yticks([])

for i in range(num_classes):
    # contour at 0 (sign)
    contour(ax, data=out_mesh[..., i], boundary=0., color=color_list[i])
scatter(ax, x_plot)

"""## Test"""

def cs_reject(t):
    top1, top2 = t.topk(2, dim=1)[0].T
    return (top1 < 0) | (top2 > 0)

out_test = model_cost(x_tensor_test)
rejected = cs_reject(out_test)
result = torch.zeros_like(y_tensor_test)
result[rejected] = -1
result[~rejected & (y_tensor_test == out_test.argmax(1))] = 1

num_data = len(result)
num_rejected = (result == -1).sum().item()
num_wrong = (result == 0).sum().item()
num_correct = (result == 1).sum().item()
num_selected = num_wrong + num_correct
zero_one_c = (num_wrong + rej_cost * num_rejected) / num_data

print(f"Number of rejected data: {num_rejected / num_data * 100:.2f}% ({num_rejected}/{num_data})")
print(f"Accuracy of non-rejected data: {num_correct / num_selected * 100:.2f} % ({num_correct}/{num_selected})")
print(f"Test empirical 0-1-c risk: {zero_one_c:.6f}")

"""---

# One-vs-all loss (confidence-based approach): https://arxiv.org/abs/1901.10655
"""

def one_vs_all_loss(loss_func):
    def loss(out, y):
        batch_size, num_classes = out.shape
        mask = torch.eye(num_classes, num_classes).long()
        
        out_p = out.masked_select(mask[y] == 1)  # [batch_size]
        out_n = out.masked_select(mask[y] == 0)  # [batch_size * (num_classes - 1)]
        
        l_p = loss_func(out_p)  # [batch_size]
        l_n = loss_func(-out_n)  # [batch_size * (num_classes - 1)]
        
        # sum -> batch average
        return (l_p.sum() + l_n.sum()) / batch_size
        
    return loss

"""## Train"""

# setup
model_ova = get_mlp()
optimizer = optim.AdamW(model_ova.parameters())
loss_func = one_vs_all_loss(losses[loss_name_ova])

# training
train(model_ova, optimizer, loss_func, x_tensor, y_tensor, 1000)

"""## Visualize"""

# evaluation
out_mesh = evaluate_mesh(model_ova)
p_y_x = torch.sigmoid(out_mesh)

# plot
fig, ax = plt.subplots(figsize=(10, 10))
ax.set_title(f'One-vs-all:{loss_name_ova}', fontsize=20)
ax.set_xticks([])
ax.set_yticks([])

for i in range(num_classes):
    # contour at 1 - c
    contour(ax, data=p_y_x[..., i], boundary=1 - rej_cost, color=color_list[i])
scatter(ax, x_plot)

"""## Test"""

def conf_reject(threshold: float):
    def reject(t):
        return t.max(dim=1)[0] < threshold

    return reject

out_test = model_ova(x_tensor_test)
rejected = conf_reject(-links[loss_name_ova](rej_cost))(out_test)
result = torch.zeros_like(y_tensor_test)
result[rejected] = -1
result[~rejected & (y_tensor_test == out_test.argmax(1))] = 1

num_data = len(result)
num_rejected = (result == -1).sum().item()
num_wrong = (result == 0).sum().item()
num_correct = (result == 1).sum().item()
num_selected = num_wrong + num_correct
zero_one_c = (num_wrong + rej_cost * num_rejected) / num_data

print(f"Number of rejected data: {num_rejected / num_data * 100:.2f}% ({num_rejected}/{num_data})")
print(f"Accuracy of non-rejected data: {num_correct / num_selected * 100:.2f} % ({num_correct}/{num_selected})")
print(f"Test empirical 0-1-c risk: {zero_one_c:.6f}")

"""---

# Softmax cross-entropy loss (confidence-based approach): https://arxiv.org/abs/1901.10655

## Train
"""

# setup
model_ce = get_mlp()
optimizer = optim.AdamW(model_ce.parameters())
loss_func = F.cross_entropy

# training
train(model_ce, optimizer, loss_func, x_tensor, y_tensor, 1000)

"""## Visualize"""

# evaluation
out_mesh = evaluate_mesh(model_ce)
p_y_x = torch.softmax(out_mesh, dim=2)

# plot
fig, ax = plt.subplots(figsize=(10, 10))
ax.set_title('Softmax cross-entropy', fontsize=20)
ax.set_xticks([])
ax.set_yticks([])

for i in range(num_classes):
    # contour at 1 - c
    contour(ax, data=p_y_x[..., i], boundary=1 - rej_cost, color=color_list[i])
scatter(ax, x_plot)

"""## Test"""

out_test = model_ova(x_tensor_test)
# use conf_reject in the test part of one-vs-all
rejected = conf_reject(1-rej_cost)(out_test)
result = torch.zeros_like(y_tensor_test)
result[rejected] = -1
result[~rejected & (y_tensor_test == out_test.argmax(1))] = 1

num_data = len(result)
num_rejected = (result == -1).sum().item()
num_wrong = (result == 0).sum().item()
num_correct = (result == 1).sum().item()
num_selected = num_wrong + num_correct
zero_one_c = (num_wrong + rej_cost * num_rejected) / num_data

print(f"Number of rejected data: {num_rejected / num_data * 100:.2f}% ({num_rejected}/{num_data})")
print(f"Accuracy of non-rejected data: {num_correct / num_selected * 100:.2f} % ({num_correct}/{num_selected})")
print(f"Test empirical 0-1-c risk: {zero_one_c:.6f}")

"""---"""