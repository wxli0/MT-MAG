x shape is: (2577, 16384)
features shape is: (2577, 16384)
subtypes shape is: (2577,)
x shape is: (2577, 16384)
y shape is: (2577,)
y_dict is: {'c__Clostridia': 0, 'c__Clostridia_eval': 0, 'c__Clostridia_test': 0, 'c__Clostridia_A': 1, 'c__Clostridia_A_eval': 1, 'c__Clostridia_A_test': 1, 'c__Thermoanaerobacteria': 2, 'c__Thermoanaerobacteria_eval': 2, 'c__Thermoanaerobacteria_test': 2}
x.shape is: (2577, 5) y.len is: 2577
***** iteration: 0 ******
loss: 17.9273624420166
***** iteration: 1 ******
loss: 10.530588150024414
***** iteration: 2 ******
loss: 9.244966506958008
***** iteration: 3 ******
loss: 7.776532173156738
***** iteration: 4 ******
loss: 6.220754623413086
***** iteration: 5 ******
loss: 4.613906383514404
***** iteration: 6 ******
loss: 2.9735841751098633
***** iteration: 7 ******
loss: 5.54573917388916
***** iteration: 8 ******
loss: 5.134258270263672
***** iteration: 9 ******
loss: 5.030752182006836
***** iteration: 10 ******
loss: 5.95961332321167
***** iteration: 11 ******
loss: 7.44191837310791
***** iteration: 12 ******
loss: 7.931877136230469
***** iteration: 13 ******
loss: 7.633224010467529
***** iteration: 14 ******
loss: 6.696722030639648
***** iteration: 15 ******
loss: 5.236436367034912
***** iteration: 16 ******
loss: 3.343982696533203
***** iteration: 17 ******
loss: 1.3304518461227417
***** iteration: 18 ******
loss: 1.5606459379196167
***** iteration: 19 ******
loss: 2.081347703933716
***** iteration: 20 ******
loss: 2.343038558959961
***** iteration: 21 ******
loss: 2.379676342010498
***** iteration: 22 ******
loss: 2.219890594482422
***** iteration: 23 ******
loss: 1.8880112171173096
***** iteration: 24 ******
loss: 1.4048435688018799
***** iteration: 25 ******
loss: 0.8153023719787598
***** iteration: 26 ******
loss: 1.393301248550415
***** iteration: 27 ******
loss: 2.5627756118774414
***** iteration: 28 ******
loss: 3.0264275074005127
***** iteration: 29 ******
loss: 3.5482637882232666
***** iteration: 30 ******
loss: 3.5924456119537354
***** iteration: 31 ******
loss: 3.112962245941162
***** iteration: 32 ******
loss: 2.1807165145874023
***** iteration: 33 ******
loss: 1.0352745056152344
***** iteration: 34 ******
loss: 0.8670008778572083
***** iteration: 35 ******
loss: 1.1512614488601685
***** iteration: 36 ******
loss: 1.257960319519043
***** iteration: 37 ******
loss: 1.1785708665847778
***** iteration: 38 ******
loss: 0.9328233599662781
***** iteration: 39 ******
loss: 0.7463269233703613
***** iteration: 40 ******
loss: 0.6894950270652771
***** iteration: 41 ******
loss: 0.797863245010376
***** iteration: 42 ******
loss: 1.0636588335037231
***** iteration: 43 ******
loss: 0.9581893086433411
***** iteration: 44 ******
loss: 0.6811134815216064
***** iteration: 45 ******
loss: 0.6348428130149841
***** iteration: 46 ******
loss: 0.6852694749832153
***** iteration: 47 ******
loss: 0.6203739643096924
***** iteration: 48 ******
loss: 0.7062565684318542
***** iteration: 49 ******
loss: 0.5814722180366516
***** iteration: 50 ******
loss: 0.5069888830184937
***** iteration: 51 ******
loss: 0.5725147128105164
***** iteration: 52 ******
loss: 0.5177576541900635
***** iteration: 53 ******
loss: 0.48943692445755005
***** iteration: 54 ******
loss: 0.6580989360809326
***** iteration: 55 ******
loss: 0.6810945272445679
***** iteration: 56 ******
loss: 0.4651949405670166
***** iteration: 57 ******
loss: 0.5563960671424866
***** iteration: 58 ******
loss: 0.6225659251213074
***** iteration: 59 ******
loss: 0.5840581655502319
***** iteration: 60 ******
loss: 0.5516020655632019
***** iteration: 61 ******
loss: 0.4846479296684265
***** iteration: 62 ******
loss: 0.667230486869812
***** iteration: 63 ******
loss: 0.7801699638366699
***** iteration: 64 ******
loss: 0.5039103031158447
***** iteration: 65 ******
loss: 0.5028572082519531
***** iteration: 66 ******
loss: 0.5140854120254517
***** iteration: 67 ******
loss: 0.5382178425788879
***** iteration: 68 ******
loss: 0.4784916043281555
***** iteration: 69 ******
loss: 0.41250893473625183
***** iteration: 70 ******
loss: 0.49155956506729126
***** iteration: 71 ******
loss: 0.40664803981781006
***** iteration: 72 ******
loss: 0.44099608063697815
***** iteration: 73 ******
loss: 0.45551562309265137
***** iteration: 74 ******
loss: 0.43839266896247864
***** iteration: 75 ******
loss: 0.375990092754364
***** iteration: 76 ******
loss: 0.5077168345451355
***** iteration: 77 ******
loss: 0.5350251197814941
***** iteration: 78 ******
loss: 0.45926618576049805
***** iteration: 79 ******
loss: 0.42288121581077576
***** iteration: 80 ******
loss: 0.49812859296798706
***** iteration: 81 ******
loss: 0.4408102035522461
***** iteration: 82 ******
loss: 0.4367855191230774
***** iteration: 83 ******
loss: 0.4499453008174896
***** iteration: 84 ******
loss: 0.40912702679634094
***** iteration: 85 ******
loss: 0.48472946882247925
***** iteration: 86 ******
loss: 0.5323514938354492
***** iteration: 87 ******
loss: 0.37515661120414734
***** iteration: 88 ******
loss: 0.4714968800544739
***** iteration: 89 ******
loss: 0.537420392036438
***** iteration: 90 ******
loss: 0.45051562786102295
***** iteration: 91 ******
loss: 0.33926841616630554
***** iteration: 92 ******
loss: 1.624947190284729
***** iteration: 93 ******
loss: 0.4371139407157898
***** iteration: 94 ******
loss: 0.3538705110549927
***** iteration: 95 ******
loss: 0.44069093465805054
***** iteration: 96 ******
loss: 0.46144434809684753
***** iteration: 97 ******
loss: 0.5148314237594604
***** iteration: 98 ******
loss: 0.484426349401474
***** iteration: 99 ******
loss: 0.4006955027580261
***** iteration: 100 ******
loss: 0.47195953130722046
***** iteration: 101 ******
loss: 0.48506394028663635
***** iteration: 102 ******
loss: 0.37325426936149597
***** iteration: 103 ******
loss: 0.40349599719047546
***** iteration: 104 ******
loss: 0.37404757738113403
***** iteration: 105 ******
loss: 0.4890234172344208
***** iteration: 106 ******
loss: 0.5611774325370789
***** iteration: 107 ******
loss: 0.3478613495826721
***** iteration: 108 ******
loss: 0.4620860517024994
***** iteration: 109 ******
loss: 0.5129966735839844
***** iteration: 110 ******
loss: 0.4685313403606415
***** iteration: 111 ******
loss: 1.9432941675186157
***** iteration: 112 ******
loss: 0.6807839274406433
***** iteration: 113 ******
loss: 0.4746764898300171
***** iteration: 114 ******
loss: 0.36775338649749756
***** iteration: 115 ******
loss: 0.45365577936172485
***** iteration: 116 ******
loss: 0.37711474299430847
***** iteration: 117 ******
loss: 0.4529699683189392
***** iteration: 118 ******
loss: 0.5238136053085327
***** iteration: 119 ******
loss: 0.410148948431015
***** iteration: 120 ******
loss: 0.43082183599472046
***** iteration: 121 ******
loss: 0.5716291069984436
***** iteration: 122 ******
loss: 0.49925497174263
***** iteration: 123 ******
loss: 0.35753944516181946
***** iteration: 124 ******
loss: 0.46262043714523315
***** iteration: 125 ******
loss: 0.40171685814857483
***** iteration: 126 ******
loss: 0.36641183495521545
***** iteration: 127 ******
loss: 0.36955228447914124
***** iteration: 128 ******
loss: 0.3319987654685974
***** iteration: 129 ******
loss: 0.31689804792404175
***** iteration: 130 ******
loss: 0.3764187693595886
***** iteration: 131 ******
loss: 2.0167746543884277
***** iteration: 132 ******
loss: 0.3559969663619995
***** iteration: 133 ******
loss: 0.3197008967399597
***** iteration: 134 ******
loss: 0.32347601652145386
***** iteration: 135 ******
loss: 0.36468005180358887
***** iteration: 136 ******
loss: 0.35731232166290283
***** iteration: 137 ******
loss: 0.34664857387542725
***** iteration: 138 ******
loss: 0.34240278601646423
***** iteration: 139 ******
loss: 0.35436832904815674
***** iteration: 140 ******
loss: 0.34539300203323364
***** iteration: 141 ******
loss: 0.3526657819747925
***** iteration: 142 ******
loss: 0.34197476506233215
***** iteration: 143 ******
loss: 0.3361757695674896
***** iteration: 144 ******
loss: 0.3259822428226471
***** iteration: 145 ******
loss: 0.3402060568332672
***** iteration: 146 ******
loss: 0.32522231340408325
***** iteration: 147 ******
loss: 0.578078031539917
***** iteration: 148 ******
loss: 0.43495452404022217
***** iteration: 149 ******
loss: 0.41737645864486694
***** iteration: 150 ******
loss: 0.31931397318840027
***** iteration: 151 ******
loss: 0.8285091519355774
***** iteration: 152 ******
loss: 1.180664300918579
***** iteration: 153 ******
loss: 0.9891420602798462
***** iteration: 154 ******
loss: 0.4744974970817566
***** iteration: 155 ******
loss: 0.5855629444122314
***** iteration: 156 ******
loss: 0.7244274020195007
***** iteration: 157 ******
loss: 0.6767652034759521
***** iteration: 158 ******
loss: 0.7229118347167969
***** iteration: 159 ******
loss: 0.42902258038520813
***** iteration: 160 ******
loss: 0.46385595202445984
***** iteration: 161 ******
loss: 0.5688338279724121
***** iteration: 162 ******
loss: 0.4873315989971161
***** iteration: 163 ******
loss: 0.5152554512023926
***** iteration: 164 ******
loss: 0.45217061042785645
***** iteration: 165 ******
loss: 0.32477280497550964
***** iteration: 166 ******
loss: 0.35174107551574707
***** iteration: 167 ******
loss: 0.32685786485671997
***** iteration: 168 ******
loss: 0.3088842034339905
***** iteration: 169 ******
loss: 0.3765241801738739
***** iteration: 170 ******
loss: 0.734994649887085
***** iteration: 171 ******
loss: 0.4242354929447174
***** iteration: 172 ******
loss: 0.3417564034461975
***** iteration: 173 ******
loss: 0.32069864869117737
***** iteration: 174 ******
loss: 0.8638773560523987
***** iteration: 175 ******
loss: 0.8559012413024902
***** iteration: 176 ******
loss: 0.4332129955291748
***** iteration: 177 ******
loss: 0.3628094494342804
***** iteration: 178 ******
loss: 0.3916270136833191
***** iteration: 179 ******
loss: 0.32237306237220764
***** iteration: 180 ******
loss: 0.6145036816596985
***** iteration: 181 ******
loss: 0.7615025043487549
***** iteration: 182 ******
loss: 0.5329582095146179
***** iteration: 183 ******
loss: 0.3383423388004303
***** iteration: 184 ******
loss: 0.3638087511062622
***** iteration: 185 ******
loss: 0.30163925886154175
***** iteration: 186 ******
loss: 0.4984249770641327
***** iteration: 187 ******
loss: 0.5258679986000061
***** iteration: 188 ******
loss: 0.293655127286911
***** iteration: 189 ******
loss: 0.44971081614494324
***** iteration: 190 ******
loss: 0.676351010799408
***** iteration: 191 ******
loss: 0.5944516062736511
***** iteration: 192 ******
loss: 0.5124138593673706
***** iteration: 193 ******
loss: 0.33352503180503845
***** iteration: 194 ******
loss: 0.917018711566925
***** iteration: 195 ******
loss: 1.4440195560455322
***** iteration: 196 ******
loss: 1.4170408248901367
***** iteration: 197 ******
loss: 0.8460267782211304
***** iteration: 198 ******
loss: 0.340609610080719
***** iteration: 199 ******
loss: 0.43651628494262695
***** iteration: 200 ******
loss: 0.4223792850971222
***** iteration: 201 ******
loss: 0.33753663301467896
***** iteration: 202 ******
loss: 0.589564859867096
***** iteration: 203 ******
loss: 0.6014049053192139
***** iteration: 204 ******
loss: 0.34367382526397705
***** iteration: 205 ******
loss: 0.4609981179237366
***** iteration: 206 ******
loss: 0.5310741662979126
***** iteration: 207 ******
loss: 0.43357524275779724
***** iteration: 208 ******
loss: 0.33496811985969543
***** iteration: 209 ******
loss: 0.5649299621582031
***** iteration: 210 ******
loss: 0.5480601191520691
***** iteration: 211 ******
loss: 0.36618128418922424
***** iteration: 212 ******
loss: 0.3597724139690399
***** iteration: 213 ******
loss: 0.4258163869380951
***** iteration: 214 ******
loss: 0.35142385959625244
***** iteration: 215 ******
loss: 0.3317830264568329
***** iteration: 216 ******
loss: 0.3480374813079834
***** iteration: 217 ******
loss: 1.346503496170044
***** iteration: 218 ******
loss: 0.32347720861434937
***** iteration: 219 ******
loss: 0.3113775849342346
***** iteration: 220 ******
loss: 0.38447919487953186
***** iteration: 221 ******
loss: 0.31620335578918457
***** iteration: 222 ******
loss: 0.33860504627227783
***** iteration: 223 ******
loss: 0.31182801723480225
***** iteration: 224 ******
loss: 0.6655923128128052
***** iteration: 225 ******
loss: 0.5986234545707703
***** iteration: 226 ******
loss: 0.319762647151947
***** iteration: 227 ******
loss: 0.39220130443573
***** iteration: 228 ******
loss: 0.3767222464084625
***** iteration: 229 ******
loss: 0.3808888792991638
***** iteration: 230 ******
loss: 0.35839489102363586
***** iteration: 231 ******
loss: 0.38286712765693665
***** iteration: 232 ******
loss: 0.39310649037361145
***** iteration: 233 ******
loss: 0.34256601333618164
***** iteration: 234 ******
loss: 0.5362899899482727
***** iteration: 235 ******
loss: 0.6376633644104004
***** iteration: 236 ******
loss: 0.33998849987983704
***** iteration: 237 ******
loss: 0.40951988101005554
***** iteration: 238 ******
loss: 0.49537980556488037
***** iteration: 239 ******
loss: 0.4317839741706848
***** iteration: 240 ******
loss: 0.2717272639274597
***** iteration: 241 ******
loss: 1.010541558265686
***** iteration: 242 ******
loss: 2.38952898979187
***** iteration: 243 ******
loss: 1.5491458177566528
***** iteration: 244 ******
loss: 1.2846006155014038
***** iteration: 245 ******
loss: 0.6112939715385437
***** iteration: 246 ******
loss: 0.470057874917984
***** iteration: 247 ******
loss: 0.6519485712051392
***** iteration: 248 ******
loss: 0.6431688666343689
***** iteration: 249 ******
loss: 0.461751252412796
***** iteration: 250 ******
loss: 0.3539363741874695
***** iteration: 251 ******
loss: 0.6867399215698242
***** iteration: 252 ******
loss: 0.529137134552002
***** iteration: 253 ******
loss: 0.3194252848625183
***** iteration: 254 ******
loss: 0.3366720974445343
***** iteration: 255 ******
loss: 0.3113833963871002
***** iteration: 256 ******
loss: 0.3353171944618225
***** iteration: 257 ******
loss: 0.3077011704444885
***** iteration: 258 ******
loss: 0.6041265726089478
***** iteration: 259 ******
loss: 0.6081860065460205
***** iteration: 260 ******
loss: 0.35507291555404663
***** iteration: 261 ******
loss: 0.33750954270362854
***** iteration: 262 ******
loss: 0.3456190228462219
***** iteration: 263 ******
loss: 0.7746244072914124
***** iteration: 264 ******
loss: 0.5041349530220032
***** iteration: 265 ******
loss: 0.33380258083343506
***** iteration: 266 ******
loss: 0.41210055351257324
***** iteration: 267 ******
loss: 0.48397213220596313
***** iteration: 268 ******
loss: 0.4515400230884552
***** iteration: 269 ******
loss: 0.326646089553833
***** iteration: 270 ******
loss: 0.8125401735305786
***** iteration: 271 ******
loss: 1.2417702674865723
***** iteration: 272 ******
loss: 0.860518753528595
***** iteration: 273 ******
loss: 0.4229649305343628
***** iteration: 274 ******
loss: 0.5375786423683167
***** iteration: 275 ******
loss: 0.6762359738349915
***** iteration: 276 ******
loss: 0.6253554224967957
***** iteration: 277 ******
loss: 0.4159409701824188
***** iteration: 278 ******
loss: 0.6571910381317139
***** iteration: 279 ******
loss: 1.2589812278747559
***** iteration: 280 ******
loss: 1.1612073183059692
***** iteration: 281 ******
loss: 0.6888787150382996
***** iteration: 282 ******
loss: 0.3803264796733856
***** iteration: 283 ******
loss: 0.45756644010543823
***** iteration: 284 ******
loss: 0.38802453875541687
***** iteration: 285 ******
loss: 0.3846673369407654
***** iteration: 286 ******
loss: 0.4806598424911499
***** iteration: 287 ******
loss: 0.33312445878982544
***** iteration: 288 ******
loss: 0.40546005964279175
***** iteration: 289 ******
loss: 0.44572195410728455
***** iteration: 290 ******
loss: 0.3525899052619934
***** iteration: 291 ******
loss: 0.4042980968952179
***** iteration: 292 ******
loss: 0.5503835082054138
***** iteration: 293 ******
loss: 0.3409947156906128
***** iteration: 294 ******
loss: 0.37786993384361267
***** iteration: 295 ******
loss: 0.4524574875831604
***** iteration: 296 ******
loss: 0.36945846676826477
***** iteration: 297 ******
loss: 0.2874758541584015
***** iteration: 298 ******
loss: 0.43937164545059204
***** iteration: 299 ******
loss: 0.4343610405921936
***** iteration: 300 ******
loss: 0.29040130972862244
***** iteration: 301 ******
loss: 0.41952255368232727
***** iteration: 302 ******
loss: 0.4780464768409729
***** iteration: 303 ******
loss: 0.4199671745300293
***** iteration: 304 ******
loss: 0.6291225552558899
***** iteration: 305 ******
loss: 0.686120867729187
***** iteration: 306 ******
loss: 0.3525528609752655
***** iteration: 307 ******
loss: 0.5215291976928711
***** iteration: 308 ******
loss: 0.6029413938522339
***** iteration: 309 ******
loss: 0.6530742645263672
***** iteration: 310 ******
loss: 0.5247788429260254
***** iteration: 311 ******
loss: 0.31771430373191833
***** iteration: 312 ******
loss: 1.430810809135437
***** iteration: 313 ******
loss: 2.289557695388794
***** iteration: 314 ******
loss: 1.8377246856689453
***** iteration: 315 ******
loss: 1.7239494323730469
***** iteration: 316 ******
loss: 1.1164438724517822
***** iteration: 317 ******
loss: 0.4044067859649658
***** iteration: 318 ******
loss: 0.579330325126648
***** iteration: 319 ******
loss: 0.6429333686828613
***** iteration: 320 ******
loss: 0.6372020840644836
***** iteration: 321 ******
loss: 0.5065130591392517
***** iteration: 322 ******
loss: 0.30573830008506775
***** iteration: 323 ******
loss: 0.6998631358146667
***** iteration: 324 ******
loss: 1.5683395862579346
***** iteration: 325 ******
loss: 1.2372550964355469
***** iteration: 326 ******
loss: 0.8647022247314453
***** iteration: 327 ******
loss: 0.34443992376327515
***** iteration: 328 ******
loss: 0.47394174337387085
***** iteration: 329 ******
loss: 0.9302640557289124
***** iteration: 330 ******
loss: 0.6558690667152405
***** iteration: 331 ******
loss: 0.6108601093292236
***** iteration: 332 ******
loss: 0.46475544571876526
***** iteration: 333 ******
loss: 0.757544219493866
***** iteration: 334 ******
loss: 1.2271602153778076
***** iteration: 335 ******
loss: 1.1317895650863647
***** iteration: 336 ******
loss: 0.5707191228866577
***** iteration: 337 ******
loss: 0.5307311415672302
***** iteration: 338 ******
loss: 0.6340525150299072
***** iteration: 339 ******
loss: 0.5577524900436401
***** iteration: 340 ******
loss: 0.48623842000961304
***** iteration: 341 ******
loss: 0.40579313039779663
***** iteration: 342 ******
loss: 0.5842044353485107
***** iteration: 343 ******
loss: 0.5087377429008484
***** iteration: 344 ******
loss: 0.4609413743019104
***** iteration: 345 ******
loss: 0.4846912920475006
***** iteration: 346 ******
loss: 0.40485677123069763
***** iteration: 347 ******
loss: 0.4181971549987793
***** iteration: 348 ******
loss: 1.431665062904358
***** iteration: 349 ******
loss: 1.6704075336456299
***** iteration: 350 ******
loss: 0.3622216582298279
***** iteration: 351 ******
loss: 0.3765738606452942
***** iteration: 352 ******
loss: 0.41948431730270386
***** iteration: 353 ******
loss: 0.41717010736465454
***** iteration: 354 ******
loss: 0.3505943715572357
***** iteration: 355 ******
loss: 0.5448406934738159
***** iteration: 356 ******
loss: 1.9895976781845093
***** iteration: 357 ******
loss: 2.414602518081665
***** iteration: 358 ******
loss: 0.7505246996879578
***** iteration: 359 ******
loss: 0.45602038502693176
***** iteration: 360 ******
loss: 0.43520253896713257
***** iteration: 361 ******
loss: 0.8712445497512817
***** iteration: 362 ******
loss: 1.1253408193588257
***** iteration: 363 ******
loss: 0.8337089419364929
***** iteration: 364 ******
loss: 0.5143740773200989
***** iteration: 365 ******
loss: 0.5408966541290283
***** iteration: 366 ******
loss: 0.4382510781288147
***** iteration: 367 ******
loss: 0.49582719802856445
***** iteration: 368 ******
loss: 0.6848322749137878
***** iteration: 369 ******
loss: 0.9366284012794495
***** iteration: 370 ******
loss: 0.41969507932662964
***** iteration: 371 ******
loss: 0.4066169559955597
***** iteration: 372 ******
loss: 0.4214085042476654
***** iteration: 373 ******
loss: 0.5834249258041382
***** iteration: 374 ******
loss: 0.49391046166419983
***** iteration: 375 ******
loss: 0.40435561537742615
***** iteration: 376 ******
loss: 0.3704487979412079
***** iteration: 377 ******
loss: 0.3220725357532501
***** iteration: 378 ******
loss: 0.4088853895664215
***** iteration: 379 ******
loss: 0.2843204140663147
***** iteration: 380 ******
loss: 0.39284878969192505
***** iteration: 381 ******
loss: 0.4284159243106842
***** iteration: 382 ******
loss: 0.3117426037788391
***** iteration: 383 ******
loss: 0.6081851124763489
***** iteration: 384 ******
loss: 0.9443055987358093
***** iteration: 385 ******
loss: 0.8718695640563965
***** iteration: 386 ******
loss: 0.3561508059501648
***** iteration: 387 ******
loss: 0.44115743041038513
***** iteration: 388 ******
loss: 0.5786965489387512
***** iteration: 389 ******
loss: 0.5405102968215942
***** iteration: 390 ******
loss: 0.3561989367008209
***** iteration: 391 ******
loss: 0.6802657246589661
***** iteration: 392 ******
loss: 1.1365933418273926
***** iteration: 393 ******
loss: 1.030748963356018
***** iteration: 394 ******
loss: 0.4540034234523773
***** iteration: 395 ******
loss: 0.4195708632469177
***** iteration: 396 ******
loss: 0.5538053512573242
***** iteration: 397 ******
loss: 0.5186471939086914
***** iteration: 398 ******
loss: 0.35159730911254883
***** iteration: 399 ******
loss: 0.6383431553840637
***** iteration: 400 ******
loss: 1.0495012998580933
***** iteration: 401 ******
loss: 0.907976508140564
***** iteration: 402 ******
loss: 0.37243449687957764
***** iteration: 403 ******
loss: 0.43584340810775757
***** iteration: 404 ******
loss: 0.5841025114059448
***** iteration: 405 ******
loss: 0.5630502104759216
***** iteration: 406 ******
loss: 0.39789989590644836
***** iteration: 407 ******
loss: 0.3499643802642822
***** iteration: 408 ******
loss: 0.5535539984703064
***** iteration: 409 ******
loss: 0.4068485200405121
***** iteration: 410 ******
loss: 0.2991451323032379
***** iteration: 411 ******
loss: 0.366289883852005
***** iteration: 412 ******
loss: 0.337117075920105
***** iteration: 413 ******
loss: 0.4612022936344147
***** iteration: 414 ******
loss: 0.6208109855651855
***** iteration: 415 ******
loss: 0.5682154893875122
***** iteration: 416 ******
loss: 0.3246644139289856
***** iteration: 417 ******
loss: 0.42579886317253113
***** iteration: 418 ******
loss: 0.5360510349273682
***** iteration: 419 ******
loss: 0.49964648485183716
***** iteration: 420 ******
loss: 0.3669840395450592
***** iteration: 421 ******
loss: 0.5245977640151978
***** iteration: 422 ******
loss: 0.9015358090400696
***** iteration: 423 ******
loss: 0.7308449745178223
***** iteration: 424 ******
loss: 0.36172640323638916
***** iteration: 425 ******
loss: 0.46975985169410706
***** iteration: 426 ******
loss: 0.6351321339607239
***** iteration: 427 ******
loss: 0.6354674696922302
***** iteration: 428 ******
loss: 0.5199170112609863
***** iteration: 429 ******
loss: 0.34486955404281616
***** iteration: 430 ******
loss: 1.1136900186538696
***** iteration: 431 ******
loss: 1.6376640796661377
***** iteration: 432 ******
loss: 1.3860018253326416
***** iteration: 433 ******
loss: 1.1798267364501953
***** iteration: 434 ******
loss: 0.5337129831314087
***** iteration: 435 ******
loss: 0.5259615778923035
***** iteration: 436 ******
loss: 0.6950978636741638
***** iteration: 437 ******
loss: 0.6792401075363159
***** iteration: 438 ******
loss: 0.49410519003868103
***** iteration: 439 ******
loss: 0.30139395594596863
***** iteration: 440 ******
loss: 1.8623242378234863
***** iteration: 441 ******
loss: 2.6357030868530273
***** iteration: 442 ******
loss: 1.1649560928344727
***** iteration: 443 ******
loss: 0.7170467376708984
***** iteration: 444 ******
loss: 0.7329896688461304
***** iteration: 445 ******
loss: 0.5307652950286865
***** iteration: 446 ******
loss: 0.5965022444725037
***** iteration: 447 ******
loss: 0.5686773657798767
***** iteration: 448 ******
loss: 0.5543254613876343
***** iteration: 449 ******
loss: 0.44555577635765076
***** iteration: 450 ******
loss: 0.3744284510612488
***** iteration: 451 ******
loss: 2.529653549194336
***** iteration: 452 ******
loss: 3.888390302658081
***** iteration: 453 ******
loss: 2.9082818031311035
***** iteration: 454 ******
loss: 0.43987664580345154
***** iteration: 455 ******
loss: 0.43202975392341614
***** iteration: 456 ******
loss: 0.5012521743774414
***** iteration: 457 ******
loss: 0.5048671960830688
***** iteration: 458 ******
loss: 0.44982412457466125
***** iteration: 459 ******
loss: 0.3463001251220703
***** iteration: 460 ******
loss: 1.519270896911621
***** iteration: 461 ******
loss: 3.5722553730010986
***** iteration: 462 ******
loss: 3.276075839996338
***** iteration: 463 ******
loss: 0.9155733585357666
***** iteration: 464 ******
loss: 0.3135497570037842
***** iteration: 465 ******
loss: 0.45534154772758484
***** iteration: 466 ******
loss: 0.4924854636192322
***** iteration: 467 ******
loss: 0.47575727105140686
***** iteration: 468 ******
loss: 1.0119131803512573
***** iteration: 469 ******
loss: 0.4652850031852722
***** iteration: 470 ******
loss: 0.4601134657859802
***** iteration: 471 ******
loss: 0.35318514704704285
***** iteration: 472 ******
loss: 0.6295366287231445
***** iteration: 473 ******
loss: 1.5988517999649048
***** iteration: 474 ******
loss: 1.664210319519043
***** iteration: 475 ******
loss: 0.5447137355804443
***** iteration: 476 ******
loss: 0.3983176648616791
***** iteration: 477 ******
loss: 0.46790266036987305
***** iteration: 478 ******
loss: 0.5099376440048218
***** iteration: 479 ******
loss: 0.5133512020111084
***** iteration: 480 ******
loss: 0.5683664083480835
***** iteration: 481 ******
loss: 0.5519313812255859
***** iteration: 482 ******
loss: 0.44620436429977417
***** iteration: 483 ******
loss: 0.37365201115608215
***** iteration: 484 ******
loss: 0.4259496033191681
***** iteration: 485 ******
loss: 0.644776463508606
***** iteration: 486 ******
loss: 0.4349997043609619
***** iteration: 487 ******
loss: 0.33801212906837463
***** iteration: 488 ******
loss: 0.37392139434814453
***** iteration: 489 ******
loss: 0.35479414463043213
***** iteration: 490 ******
loss: 0.3094235062599182
***** iteration: 491 ******
loss: 0.4469299614429474
***** iteration: 492 ******
loss: 0.4745304584503174
***** iteration: 493 ******
loss: 0.26674699783325195
***** iteration: 494 ******
loss: 1.5619878768920898
***** iteration: 495 ******
loss: 0.4388032555580139
***** iteration: 496 ******
loss: 0.5569985508918762
***** iteration: 497 ******
loss: 0.5031981468200684
***** iteration: 498 ******
loss: 0.5415180921554565
***** iteration: 499 ******
loss: 0.4628497064113617
***** iteration: 500 ******
loss: 0.4191769063472748
***** iteration: 501 ******
loss: 0.35149312019348145
***** iteration: 502 ******
loss: 0.6479237675666809
***** iteration: 503 ******
loss: 0.7337235808372498
***** iteration: 504 ******
loss: 0.4811622202396393
***** iteration: 505 ******
loss: 0.42103636264801025
***** iteration: 506 ******
loss: 0.4850604832172394
***** iteration: 507 ******
loss: 0.6832216382026672
***** iteration: 508 ******
loss: 0.43891045451164246
***** iteration: 509 ******
loss: 0.5209523439407349
***** iteration: 510 ******
loss: 0.5724241137504578
***** iteration: 511 ******
loss: 0.44437941908836365
***** iteration: 512 ******
loss: 0.3223106861114502
***** iteration: 513 ******
loss: 0.4559079706668854
***** iteration: 514 ******
loss: 0.472691148519516
***** iteration: 515 ******
loss: 0.2928195595741272
***** iteration: 516 ******
loss: 0.30378639698028564
***** iteration: 517 ******
loss: 0.47432374954223633
***** iteration: 518 ******
loss: 0.5313313007354736
***** iteration: 519 ******
loss: 0.3108551502227783
***** iteration: 520 ******
loss: 0.2727683186531067
***** iteration: 521 ******
loss: 0.637533962726593
***** iteration: 522 ******
loss: 0.7398902773857117
***** iteration: 523 ******
loss: 0.3871600329875946
***** iteration: 524 ******
loss: 0.3588029742240906
***** iteration: 525 ******
loss: 0.4062557816505432
***** iteration: 526 ******
loss: 0.33892694115638733
***** iteration: 527 ******
loss: 0.5091935992240906
***** iteration: 528 ******
loss: 0.789523184299469
***** iteration: 529 ******
loss: 0.5477914214134216
***** iteration: 530 ******
loss: 0.30444273352622986
***** iteration: 531 ******
loss: 0.31909051537513733
***** iteration: 532 ******
loss: 0.4316794276237488
***** iteration: 533 ******
loss: 0.32586824893951416
***** iteration: 534 ******
loss: 0.3513055145740509
***** iteration: 535 ******
loss: 0.3778027892112732
***** iteration: 536 ******
loss: 0.2984622120857239
***** iteration: 537 ******
loss: 0.5105136036872864
***** iteration: 538 ******
loss: 0.6616886258125305
***** iteration: 539 ******
loss: 0.38786932826042175
***** iteration: 540 ******
loss: 0.3339545428752899
***** iteration: 541 ******
loss: 0.4065226912498474
***** iteration: 542 ******
loss: 0.3385566473007202
***** iteration: 543 ******
loss: 0.3663117587566376
***** iteration: 544 ******
loss: 0.4716923236846924
***** iteration: 545 ******
loss: 0.28712737560272217
***** iteration: 546 ******
loss: 0.36462458968162537
***** iteration: 547 ******
loss: 0.3967092037200928
***** iteration: 548 ******
loss: 0.29798758029937744
***** iteration: 549 ******
loss: 0.5987732410430908
***** iteration: 550 ******
loss: 0.9559099078178406
***** iteration: 551 ******
loss: 0.6355937123298645
***** iteration: 552 ******
loss: 0.2653662860393524
***** iteration: 553 ******
loss: 0.3730967044830322
***** iteration: 554 ******
loss: 0.462725967168808
***** iteration: 555 ******
loss: 0.3776437044143677
***** iteration: 556 ******
loss: 0.27856484055519104
***** iteration: 557 ******
loss: 0.6409696340560913
***** iteration: 558 ******
loss: 0.8405815958976746
***** iteration: 559 ******
loss: 0.4750208258628845
***** iteration: 560 ******
loss: 0.3426336348056793
***** iteration: 561 ******
loss: 0.40908920764923096
***** iteration: 562 ******
loss: 0.32649993896484375
***** iteration: 563 ******
loss: 0.5497372150421143
***** iteration: 564 ******
loss: 0.7367526292800903
***** iteration: 565 ******
loss: 0.47856080532073975
***** iteration: 566 ******
loss: 0.35252588987350464
***** iteration: 567 ******
loss: 0.3910113573074341
***** iteration: 568 ******
loss: 0.3198242783546448
***** iteration: 569 ******
loss: 0.4367566406726837
***** iteration: 570 ******
loss: 0.47012007236480713
***** iteration: 571 ******
loss: 0.31430506706237793
***** iteration: 572 ******
loss: 0.4370277225971222
***** iteration: 573 ******
loss: 0.38681837916374207
***** iteration: 574 ******
loss: 0.35005584359169006
***** iteration: 575 ******
loss: 0.4137634038925171
***** iteration: 576 ******
loss: 0.44150832295417786
***** iteration: 577 ******
loss: 0.3673152029514313
***** iteration: 578 ******
loss: 0.3293175995349884
***** iteration: 579 ******
loss: 0.27946439385414124
***** iteration: 580 ******
loss: 0.9374532699584961
***** iteration: 581 ******
loss: 0.4352734684944153
***** iteration: 582 ******
loss: 0.5331122875213623
***** iteration: 583 ******
loss: 0.38134920597076416
***** iteration: 584 ******
loss: 0.42345699667930603
***** iteration: 585 ******
loss: 0.42273634672164917
***** iteration: 586 ******
loss: 0.4201517105102539
***** iteration: 587 ******
loss: 0.3264601528644562
***** iteration: 588 ******
loss: 1.0479886531829834
***** iteration: 589 ******
loss: 2.1011054515838623
***** iteration: 590 ******
loss: 1.1559065580368042
***** iteration: 591 ******
loss: 0.8892569541931152
***** iteration: 592 ******
loss: 0.43884655833244324
***** iteration: 593 ******
loss: 0.6799454689025879
***** iteration: 594 ******
loss: 0.8604955673217773
***** iteration: 595 ******
loss: 0.8481327891349792
***** iteration: 596 ******
loss: 0.6621254682540894
***** iteration: 597 ******
loss: 0.4759364724159241
***** iteration: 598 ******
loss: 0.6946247220039368
***** iteration: 599 ******
loss: 0.5026640892028809
***** iteration: 600 ******
loss: 0.33301979303359985
***** iteration: 601 ******
loss: 0.6729893684387207
***** iteration: 602 ******
loss: 0.5870835781097412
***** iteration: 603 ******
loss: 0.33966517448425293
***** iteration: 604 ******
loss: 0.3950579762458801
***** iteration: 605 ******
loss: 0.41416430473327637
***** iteration: 606 ******
loss: 0.38309019804000854
***** iteration: 607 ******
loss: 0.3543161153793335
***** iteration: 608 ******
loss: 0.31318122148513794
***** iteration: 609 ******
loss: 0.4748697578907013
***** iteration: 610 ******
loss: 0.3501243591308594
***** iteration: 611 ******
loss: 0.30991867184638977
***** iteration: 612 ******
loss: 0.3469201326370239
***** iteration: 613 ******
loss: 0.32741639018058777
***** iteration: 614 ******
loss: 0.27075010538101196
***** iteration: 615 ******
loss: 0.6279982924461365
***** iteration: 616 ******
loss: 1.06876540184021
***** iteration: 617 ******
loss: 0.3756231367588043
***** iteration: 618 ******
loss: 0.3108670115470886
***** iteration: 619 ******
loss: 0.3406873345375061
***** iteration: 620 ******
loss: 0.4050784111022949
***** iteration: 621 ******
loss: 0.29590904712677
***** iteration: 622 ******
loss: 0.3966716527938843
***** iteration: 623 ******
loss: 0.47374555468559265
***** iteration: 624 ******
loss: 0.3708656132221222
***** iteration: 625 ******
loss: 0.3058779835700989
***** iteration: 626 ******
loss: 0.35381507873535156
***** iteration: 627 ******
loss: 0.28966689109802246
***** iteration: 628 ******
loss: 0.24839982390403748
***** iteration: 629 ******
loss: 0.5278676152229309
***** iteration: 630 ******
loss: 0.5728426575660706
***** iteration: 631 ******
loss: 0.24493364989757538
***** iteration: 632 ******
loss: 0.44114989042282104
***** iteration: 633 ******
loss: 0.5435214638710022
***** iteration: 634 ******
loss: 0.4622891843318939
***** iteration: 635 ******
loss: 0.2648504972457886
***** iteration: 636 ******
loss: 0.77354896068573
***** iteration: 637 ******
loss: 1.1669062376022339
***** iteration: 638 ******
loss: 1.0067001581192017
***** iteration: 639 ******
loss: 0.3918633759021759
***** iteration: 640 ******
loss: 0.4027547836303711
***** iteration: 641 ******
loss: 0.5619539618492126
***** iteration: 642 ******
loss: 0.5507026314735413
***** iteration: 643 ******
loss: 0.39263051748275757
***** iteration: 644 ******
loss: 0.29545482993125916
***** iteration: 645 ******
loss: 0.6634058952331543
***** iteration: 646 ******
loss: 0.6217203736305237
***** iteration: 647 ******
loss: 0.28169968724250793
***** iteration: 648 ******
loss: 0.48286810517311096
***** iteration: 649 ******
loss: 0.6173089146614075
***** iteration: 650 ******
loss: 0.5650272965431213
***** iteration: 651 ******
loss: 0.4441223442554474
***** iteration: 652 ******
loss: 0.33291131258010864
***** iteration: 653 ******
loss: 0.4254854917526245
***** iteration: 654 ******
loss: 0.3201647698879242
***** iteration: 655 ******
loss: 0.6257539987564087
***** iteration: 656 ******
loss: 0.5667408108711243
***** iteration: 657 ******
loss: 0.6231143474578857
***** iteration: 658 ******
loss: 0.54836505651474
***** iteration: 659 ******
loss: 0.40597981214523315
***** iteration: 660 ******
loss: 0.5940794348716736
***** iteration: 661 ******
loss: 0.5225338935852051
***** iteration: 662 ******
loss: 0.32567673921585083
***** iteration: 663 ******
loss: 0.3469146192073822
***** iteration: 664 ******
loss: 0.3335637152194977
***** iteration: 665 ******
loss: 0.27228933572769165
***** iteration: 666 ******
loss: 0.2783273756504059
***** iteration: 667 ******
loss: 0.28824031352996826
***** iteration: 668 ******
loss: 0.27557289600372314
***** iteration: 669 ******
loss: 0.2685157358646393
***** iteration: 670 ******
loss: 0.8197906017303467
***** iteration: 671 ******
loss: 0.2482425570487976
***** iteration: 672 ******
loss: 0.32319197058677673
***** iteration: 673 ******
loss: 0.27862656116485596
***** iteration: 674 ******
loss: 0.3281438648700714
***** iteration: 675 ******
loss: 0.3386901021003723
***** iteration: 676 ******
loss: 0.29308846592903137
***** iteration: 677 ******
loss: 0.47478756308555603
***** iteration: 678 ******
loss: 0.3938840627670288
***** iteration: 679 ******
loss: 0.4486084580421448
***** iteration: 680 ******
loss: 0.4844147861003876
***** iteration: 681 ******
loss: 0.3648553788661957
***** iteration: 682 ******
loss: 0.6625911593437195
***** iteration: 683 ******
loss: 0.9563717842102051
***** iteration: 684 ******
loss: 0.7798288464546204
***** iteration: 685 ******
loss: 0.337557852268219
***** iteration: 686 ******
loss: 0.48231175541877747
***** iteration: 687 ******
loss: 0.6011934280395508
***** iteration: 688 ******
loss: 0.5394694805145264
***** iteration: 689 ******
loss: 0.41807428002357483
***** iteration: 690 ******
loss: 0.3432960510253906
***** iteration: 691 ******
loss: 0.43279215693473816
***** iteration: 692 ******
loss: 0.3411060571670532
***** iteration: 693 ******
loss: 0.30742430686950684
***** iteration: 694 ******
loss: 0.5981531739234924
***** iteration: 695 ******
loss: 0.6810840964317322
***** iteration: 696 ******
loss: 0.49540936946868896
***** iteration: 697 ******
loss: 0.3235284984111786
***** iteration: 698 ******
loss: 0.4566960632801056
***** iteration: 699 ******
loss: 0.4897240996360779
***** iteration: 700 ******
loss: 0.34917017817497253
***** iteration: 701 ******
loss: 0.7349238395690918
***** iteration: 702 ******
loss: 1.0047391653060913
***** iteration: 703 ******
loss: 1.141085147857666
***** iteration: 704 ******
loss: 1.0021694898605347
***** iteration: 705 ******
loss: 0.4444277882575989
***** iteration: 706 ******
loss: 0.5566526055335999
***** iteration: 707 ******
loss: 0.7236793637275696
***** iteration: 708 ******
loss: 0.7002474069595337
***** iteration: 709 ******
loss: 0.5057600736618042
***** iteration: 710 ******
loss: 0.3197875916957855
***** iteration: 711 ******
loss: 1.1551345586776733
***** iteration: 712 ******
loss: 1.264998435974121
***** iteration: 713 ******
loss: 0.7085474729537964
***** iteration: 714 ******
loss: 0.40366512537002563
***** iteration: 715 ******
loss: 0.5018678307533264
***** iteration: 716 ******
loss: 0.6178978681564331
***** iteration: 717 ******
loss: 0.5522964000701904
***** iteration: 718 ******
loss: 0.36567771434783936
***** iteration: 719 ******
loss: 0.6998352408409119
***** iteration: 720 ******
loss: 1.5138628482818604
***** iteration: 721 ******
loss: 1.3933606147766113
***** iteration: 722 ******
loss: 0.9724394679069519
***** iteration: 723 ******
loss: 0.4218481779098511
***** iteration: 724 ******
loss: 0.5185765624046326
***** iteration: 725 ******
loss: 0.6901901960372925
***** iteration: 726 ******
loss: 0.673453688621521
***** iteration: 727 ******
loss: 0.48744356632232666
***** iteration: 728 ******
loss: 0.2971194386482239
***** iteration: 729 ******
loss: 0.846308708190918
***** iteration: 730 ******
loss: 1.1908408403396606
***** iteration: 731 ******
loss: 0.6709811091423035
***** iteration: 732 ******
loss: 0.3676421344280243
***** iteration: 733 ******
loss: 0.42126157879829407
***** iteration: 734 ******
loss: 0.5206543803215027
***** iteration: 735 ******
loss: 0.4547690749168396
***** iteration: 736 ******
loss: 0.33020880818367004
***** iteration: 737 ******
loss: 0.4757125973701477
***** iteration: 738 ******
loss: 0.8858315944671631
***** iteration: 739 ******
loss: 0.44114187359809875
***** iteration: 740 ******
loss: 0.30749011039733887
***** iteration: 741 ******
loss: 0.33623459935188293
***** iteration: 742 ******
loss: 0.287060409784317
***** iteration: 743 ******
loss: 0.3979710042476654
***** iteration: 744 ******
loss: 0.4665413796901703
***** iteration: 745 ******
loss: 0.27344679832458496
***** iteration: 746 ******
loss: 0.4114382565021515
***** iteration: 747 ******
loss: 0.4562426507472992
***** iteration: 748 ******
loss: 0.33352574706077576
***** iteration: 749 ******
loss: 0.45821821689605713
***** iteration: 750 ******
loss: 0.7310723662376404
***** iteration: 751 ******
loss: 0.484607994556427
***** iteration: 752 ******
loss: 0.2850048542022705
***** iteration: 753 ******
loss: 0.32410338521003723
***** iteration: 754 ******
loss: 0.2536804974079132
***** iteration: 755 ******
loss: 0.7107877731323242
***** iteration: 756 ******
loss: 0.8620080351829529
***** iteration: 757 ******
loss: 0.41658565402030945
***** iteration: 758 ******
loss: 0.2884688079357147
***** iteration: 759 ******
loss: 0.30861398577690125
***** iteration: 760 ******
loss: 0.34000712633132935
***** iteration: 761 ******
loss: 0.2580108940601349
***** iteration: 762 ******
loss: 0.4315919280052185
***** iteration: 763 ******
loss: 0.4558521509170532
***** iteration: 764 ******
loss: 0.4302116334438324
***** iteration: 765 ******
loss: 0.30595651268959045
***** iteration: 766 ******
loss: 0.8363716006278992
***** iteration: 767 ******
loss: 1.1925058364868164
***** iteration: 768 ******
loss: 0.9987325072288513
***** iteration: 769 ******
loss: 0.4020956754684448
***** iteration: 770 ******
loss: 0.42275920510292053
***** iteration: 771 ******
loss: 0.5885902643203735
***** iteration: 772 ******
loss: 0.6017382144927979
***** iteration: 773 ******
loss: 0.4465848207473755
***** iteration: 774 ******
loss: 0.2867646813392639
***** iteration: 775 ******
loss: 0.494113564491272
***** iteration: 776 ******
loss: 0.3627801239490509
***** iteration: 777 ******
loss: 0.3128799498081207
***** iteration: 778 ******
loss: 0.34502264857292175
***** iteration: 779 ******
loss: 0.2832220196723938
***** iteration: 780 ******
loss: 0.48688769340515137
***** iteration: 781 ******
loss: 0.5829154849052429
***** iteration: 782 ******
loss: 0.2904333174228668
***** iteration: 783 ******
loss: 0.3819761276245117
***** iteration: 784 ******
loss: 0.46110931038856506
***** iteration: 785 ******
loss: 0.3836098313331604
***** iteration: 786 ******
loss: 0.2457192987203598
***** iteration: 787 ******
loss: 1.0001792907714844
***** iteration: 788 ******
loss: 1.8560678958892822
***** iteration: 789 ******
loss: 1.2907757759094238
***** iteration: 790 ******
loss: 1.0437266826629639
***** iteration: 791 ******
loss: 0.3902948200702667
***** iteration: 792 ******
loss: 0.49694517254829407
***** iteration: 793 ******
loss: 0.6691930294036865
***** iteration: 794 ******
loss: 0.6658410429954529
***** iteration: 795 ******
loss: 0.5415904521942139
***** iteration: 796 ******
loss: 0.34617623686790466
***** iteration: 797 ******
loss: 0.8540871739387512
***** iteration: 798 ******
loss: 1.3581666946411133
***** iteration: 799 ******
loss: 1.3002759218215942
***** iteration: 800 ******
loss: 0.7518496513366699
***** iteration: 801 ******
loss: 0.3668786585330963
***** iteration: 802 ******
loss: 0.553618311882019
***** iteration: 803 ******
loss: 0.6579106450080872
***** iteration: 804 ******
loss: 0.5801571607589722
***** iteration: 805 ******
loss: 0.385670006275177
***** iteration: 806 ******
loss: 0.7150310277938843
***** iteration: 807 ******
loss: 1.8397151231765747
***** iteration: 808 ******
loss: 1.6364580392837524
***** iteration: 809 ******
loss: 1.3918591737747192
***** iteration: 810 ******
loss: 0.9793983697891235
***** iteration: 811 ******
loss: 0.46438711881637573
***** iteration: 812 ******
loss: 0.6788126826286316
***** iteration: 813 ******
loss: 0.8347681164741516
***** iteration: 814 ******
loss: 0.8025637269020081
***** iteration: 815 ******
loss: 0.6008843183517456
***** iteration: 816 ******
loss: 0.934722363948822
***** iteration: 817 ******
loss: 0.8205251097679138
***** iteration: 818 ******
loss: 0.9364390969276428
***** iteration: 819 ******
loss: 1.083874225616455
***** iteration: 820 ******
loss: 0.7416993379592896
***** iteration: 821 ******
loss: 0.4274477958679199
***** iteration: 822 ******
loss: 0.5091300010681152
***** iteration: 823 ******
loss: 0.5139881372451782
***** iteration: 824 ******
loss: 0.398659884929657
***** iteration: 825 ******
loss: 0.7394891381263733
***** iteration: 826 ******
loss: 0.5249532461166382
***** iteration: 827 ******
loss: 0.6478614211082458
***** iteration: 828 ******
loss: 0.510829508304596
***** iteration: 829 ******
loss: 0.4560414254665375
***** iteration: 830 ******
loss: 0.5046321153640747
***** iteration: 831 ******
loss: 0.43429601192474365
***** iteration: 832 ******
loss: 0.4015336036682129
***** iteration: 833 ******
loss: 0.48569679260253906
***** iteration: 834 ******
loss: 0.5282042026519775
***** iteration: 835 ******
loss: 0.3174802362918854
***** iteration: 836 ******
loss: 0.3179272711277008
***** iteration: 837 ******
loss: 0.300808310508728
***** iteration: 838 ******
loss: 0.28830042481422424
***** iteration: 839 ******
loss: 0.3513920307159424
***** iteration: 840 ******
loss: 0.2711392343044281
***** iteration: 841 ******
loss: 0.28476205468177795
***** iteration: 842 ******
loss: 0.28851208090782166
***** iteration: 843 ******
loss: 0.2643873989582062
***** iteration: 844 ******
loss: 0.28668344020843506
***** iteration: 845 ******
loss: 0.3181253671646118
***** iteration: 846 ******
loss: 0.2921398878097534
***** iteration: 847 ******
loss: 0.4333849847316742
***** iteration: 848 ******
loss: 0.5267215371131897
***** iteration: 849 ******
loss: 0.3046344816684723
***** iteration: 850 ******
loss: 0.43509602546691895
***** iteration: 851 ******
loss: 0.491363525390625
***** iteration: 852 ******
loss: 0.371201753616333
***** iteration: 853 ******
loss: 0.4200165271759033
***** iteration: 854 ******
loss: 0.7341836094856262
***** iteration: 855 ******
loss: 0.5707231760025024
***** iteration: 856 ******
loss: 0.30568280816078186
***** iteration: 857 ******
loss: 0.4237706661224365
***** iteration: 858 ******
loss: 0.9532710909843445
***** iteration: 859 ******
loss: 0.5095303058624268
***** iteration: 860 ******
loss: 0.4565258324146271
***** iteration: 861 ******
loss: 0.3589175045490265
***** iteration: 862 ******
loss: 0.5219875574111938
***** iteration: 863 ******
loss: 0.3978468179702759
***** iteration: 864 ******
loss: 0.3314891457557678
***** iteration: 865 ******
loss: 0.30484122037887573
***** iteration: 866 ******
loss: 0.7409031391143799
***** iteration: 867 ******
loss: 0.44325146079063416
***** iteration: 868 ******
loss: 0.4063921570777893
***** iteration: 869 ******
loss: 0.30228522419929504
***** iteration: 870 ******
loss: 0.30420589447021484
***** iteration: 871 ******
loss: 0.27915361523628235
***** iteration: 872 ******
loss: 0.8916497826576233
***** iteration: 873 ******
loss: 0.4231064021587372
***** iteration: 874 ******
loss: 0.3256646990776062
***** iteration: 875 ******
loss: 0.36654379963874817
***** iteration: 876 ******
loss: 0.36328715085983276
***** iteration: 877 ******
loss: 0.34139779210090637
***** iteration: 878 ******
loss: 0.26705920696258545
***** iteration: 879 ******
loss: 0.311510294675827
***** iteration: 880 ******
loss: 0.2582192122936249
***** iteration: 881 ******
loss: 0.33273017406463623
***** iteration: 882 ******
loss: 0.3374820649623871
***** iteration: 883 ******
loss: 0.24373945593833923
***** iteration: 884 ******
loss: 0.9103880524635315
***** iteration: 885 ******
loss: 1.0286290645599365
***** iteration: 886 ******
loss: 0.9420075416564941
***** iteration: 887 ******
loss: 0.43363136053085327
***** iteration: 888 ******
loss: 0.40124496817588806
***** iteration: 889 ******
loss: 0.5334282517433167
***** iteration: 890 ******
loss: 0.5346749424934387
***** iteration: 891 ******
loss: 0.3952470123767853
***** iteration: 892 ******
loss: 0.39253318309783936
***** iteration: 893 ******
loss: 0.5802392959594727
***** iteration: 894 ******
loss: 0.36336979269981384
***** iteration: 895 ******
loss: 0.38221877813339233
***** iteration: 896 ******
loss: 0.4410448372364044
***** iteration: 897 ******
loss: 0.3477042019367218
***** iteration: 898 ******
loss: 0.44307899475097656
***** iteration: 899 ******
loss: 0.6104745268821716
***** iteration: 900 ******
loss: 0.3711094558238983
***** iteration: 901 ******
loss: 0.3644387125968933
***** iteration: 902 ******
loss: 0.4237591028213501
***** iteration: 903 ******
loss: 0.3652957081794739
***** iteration: 904 ******
loss: 0.36927330493927
***** iteration: 905 ******
loss: 0.4268054664134979
***** iteration: 906 ******
loss: 0.2880476117134094
***** iteration: 907 ******
loss: 0.3687504231929779
***** iteration: 908 ******
loss: 0.3699895441532135
***** iteration: 909 ******
loss: 0.3235137164592743
***** iteration: 910 ******
loss: 0.2771325409412384
***** iteration: 911 ******
loss: 1.7759467363357544
***** iteration: 912 ******
loss: 1.687623143196106
***** iteration: 913 ******
loss: 0.45999836921691895
***** iteration: 914 ******
loss: 0.4255838692188263
***** iteration: 915 ******
loss: 0.3089301586151123
***** iteration: 916 ******
loss: 0.6306149363517761
***** iteration: 917 ******
loss: 0.6164765954017639
***** iteration: 918 ******
loss: 0.6026224493980408
***** iteration: 919 ******
loss: 0.2807008624076843
***** iteration: 920 ******
loss: 0.38191744685173035
***** iteration: 921 ******
loss: 0.40712618827819824
***** iteration: 922 ******
loss: 0.3360251486301422
***** iteration: 923 ******
loss: 0.3588125705718994
***** iteration: 924 ******
loss: 0.44841137528419495
***** iteration: 925 ******
loss: 0.3192152976989746
***** iteration: 926 ******
loss: 0.34928151965141296
***** iteration: 927 ******
loss: 0.352733314037323
***** iteration: 928 ******
loss: 0.3793811500072479
***** iteration: 929 ******
loss: 0.3441654145717621
***** iteration: 930 ******
loss: 0.3113371729850769
***** iteration: 931 ******
loss: 0.36251580715179443
***** iteration: 932 ******
loss: 0.2991102337837219
***** iteration: 933 ******
loss: 0.46406134963035583
***** iteration: 934 ******
loss: 0.6299462914466858
***** iteration: 935 ******
loss: 0.41115066409111023
***** iteration: 936 ******
loss: 0.3312751054763794
***** iteration: 937 ******
loss: 0.3653373718261719
***** iteration: 938 ******
loss: 0.27169346809387207
***** iteration: 939 ******
loss: 0.9939654469490051
***** iteration: 940 ******
loss: 1.239970326423645
***** iteration: 941 ******
loss: 1.189366340637207
***** iteration: 942 ******
loss: 0.9527561664581299
***** iteration: 943 ******
loss: 0.48038291931152344
***** iteration: 944 ******
loss: 0.5289661884307861
***** iteration: 945 ******
loss: 0.6437994241714478
***** iteration: 946 ******
loss: 0.5765411853790283
***** iteration: 947 ******
loss: 0.40365132689476013
***** iteration: 948 ******
loss: 0.7632870674133301
***** iteration: 949 ******
loss: 1.3589080572128296
***** iteration: 950 ******
loss: 1.7484153509140015
***** iteration: 951 ******
loss: 0.9518619775772095
***** iteration: 952 ******
loss: 0.48296335339546204
***** iteration: 953 ******
loss: 0.49991321563720703
***** iteration: 954 ******
loss: 0.6512053608894348
***** iteration: 955 ******
loss: 0.617787778377533
***** iteration: 956 ******
loss: 0.4276065230369568
***** iteration: 957 ******
loss: 0.5169253349304199
***** iteration: 958 ******
loss: 1.1161645650863647
***** iteration: 959 ******
loss: 0.9684798717498779
***** iteration: 960 ******
loss: 0.6474283933639526
***** iteration: 961 ******
loss: 0.39670324325561523
***** iteration: 962 ******
loss: 0.45015522837638855
***** iteration: 963 ******
loss: 0.3834340274333954
***** iteration: 964 ******
loss: 0.5715632438659668
***** iteration: 965 ******
loss: 0.6251946091651917
***** iteration: 966 ******
loss: 0.58818119764328
***** iteration: 967 ******
loss: 0.38159579038619995
***** iteration: 968 ******
loss: 0.49430158734321594
***** iteration: 969 ******
loss: 0.5578747987747192
***** iteration: 970 ******
loss: 0.4623764753341675
***** iteration: 971 ******
loss: 0.36413687467575073
***** iteration: 972 ******
loss: 0.49362558126449585
***** iteration: 973 ******
loss: 0.6316800713539124
***** iteration: 974 ******
loss: 0.39988675713539124
***** iteration: 975 ******
loss: 0.4180970788002014
***** iteration: 976 ******
loss: 0.4702070653438568
***** iteration: 977 ******
loss: 0.36255326867103577
***** iteration: 978 ******
loss: 0.43132713437080383
***** iteration: 979 ******
loss: 0.6016473770141602
***** iteration: 980 ******
loss: 0.6233077645301819
***** iteration: 981 ******
loss: 0.36070266366004944
***** iteration: 982 ******
loss: 0.38434258103370667
***** iteration: 983 ******
loss: 0.45746511220932007
***** iteration: 984 ******
loss: 0.3691829741001129
***** iteration: 985 ******
loss: 0.4331330358982086
***** iteration: 986 ******
loss: 0.4738813638687134
***** iteration: 987 ******
loss: 0.5177032947540283
***** iteration: 988 ******
loss: 0.3205258846282959
***** iteration: 989 ******
loss: 0.4019499123096466
***** iteration: 990 ******
loss: 0.4616488516330719
***** iteration: 991 ******
loss: 0.5048711895942688
***** iteration: 992 ******
loss: 0.30364251136779785
***** iteration: 993 ******
loss: 0.3727900981903076
***** iteration: 994 ******
loss: 0.5337938070297241
***** iteration: 995 ******
loss: 0.3939476013183594
***** iteration: 996 ******
loss: 0.3936510384082794
***** iteration: 997 ******
loss: 0.41597944498062134
***** iteration: 998 ******
loss: 0.3023875653743744
***** iteration: 999 ******
loss: 1.1442439556121826
***** iteration: 1000 ******
loss: 1.3949850797653198
***** iteration: 1001 ******
loss: 1.1332995891571045
***** iteration: 1002 ******
loss: 1.0244051218032837
***** iteration: 1003 ******
loss: 0.5011325478553772
***** iteration: 1004 ******
loss: 0.47384634613990784
***** iteration: 1005 ******
loss: 0.6234156489372253
***** iteration: 1006 ******
loss: 0.6003950238227844
***** iteration: 1007 ******
loss: 0.42366883158683777
***** iteration: 1008 ******
loss: 0.3500272333621979
***** iteration: 1009 ******
loss: 0.6332600712776184
***** iteration: 1010 ******
loss: 0.7525802254676819
***** iteration: 1011 ******
loss: 0.3648678660392761
***** iteration: 1012 ******
loss: 0.32068148255348206
***** iteration: 1013 ******
loss: 0.35203349590301514
***** iteration: 1014 ******
loss: 0.28189441561698914
***** iteration: 1015 ******
loss: 0.6460719704627991
***** iteration: 1016 ******
loss: 0.8392227292060852
***** iteration: 1017 ******
loss: 0.7901098132133484
***** iteration: 1018 ******
loss: 0.4087091088294983
***** iteration: 1019 ******
loss: 0.4027983844280243
***** iteration: 1020 ******
loss: 0.5292962789535522
***** iteration: 1021 ******
loss: 0.5097326040267944
***** iteration: 1022 ******
loss: 0.35525569319725037
***** iteration: 1023 ******
loss: 0.585364043712616
***** iteration: 1024 ******
loss: 0.9492915868759155
***** iteration: 1025 ******
loss: 1.0829912424087524
***** iteration: 1026 ******
loss: 0.7961385250091553
***** iteration: 1027 ******
loss: 0.39691364765167236
***** iteration: 1028 ******
loss: 0.5486717820167542
***** iteration: 1029 ******
loss: 0.6439101099967957
***** iteration: 1030 ******
loss: 0.8508045673370361
***** iteration: 1031 ******
loss: 0.563336968421936
***** iteration: 1032 ******
loss: 0.4289705157279968
***** iteration: 1033 ******
loss: 0.5336600542068481
***** iteration: 1034 ******
loss: 0.8539670705795288
***** iteration: 1035 ******
loss: 0.7246512174606323
***** iteration: 1036 ******
loss: 0.44527578353881836
***** iteration: 1037 ******
loss: 0.5273817777633667
***** iteration: 1038 ******
loss: 0.5491169095039368
***** iteration: 1039 ******
loss: 1.3070695400238037
***** iteration: 1040 ******
loss: 1.2771533727645874
***** iteration: 1041 ******
loss: 0.3277776837348938
***** iteration: 1042 ******
loss: 0.4364757239818573
***** iteration: 1043 ******
loss: 0.5908924341201782
***** iteration: 1044 ******
loss: 0.4540252089500427
***** iteration: 1045 ******
loss: 0.49053463339805603
***** iteration: 1046 ******
loss: 0.5306422710418701
***** iteration: 1047 ******
loss: 0.41890376806259155
***** iteration: 1048 ******
loss: 0.3201534152030945
***** iteration: 1049 ******
loss: 0.5834607481956482
***** iteration: 1050 ******
loss: 1.2218501567840576
***** iteration: 1051 ******
loss: 0.4714009165763855
***** iteration: 1052 ******
loss: 0.334476500749588
***** iteration: 1053 ******
loss: 0.4322107136249542
***** iteration: 1054 ******
loss: 0.45126548409461975
***** iteration: 1055 ******
loss: 0.36559444665908813
***** iteration: 1056 ******
loss: 0.4755776822566986
***** iteration: 1057 ******
loss: 1.2881354093551636
***** iteration: 1058 ******
loss: 1.1122307777404785
***** iteration: 1059 ******
loss: 0.33149293065071106
***** iteration: 1060 ******
loss: 0.4158274829387665
***** iteration: 1061 ******
loss: 0.4889325797557831
***** iteration: 1062 ******
loss: 0.4123919904232025
***** iteration: 1063 ******
loss: 0.5431942939758301
***** iteration: 1064 ******
loss: 0.6639159321784973
***** iteration: 1065 ******
loss: 0.37256166338920593
***** iteration: 1066 ******
loss: 0.38846704363822937
***** iteration: 1067 ******
loss: 0.473151832818985
***** iteration: 1068 ******
loss: 0.43270790576934814
***** iteration: 1069 ******
loss: 0.30045899748802185
***** iteration: 1070 ******
loss: 0.7045073509216309
***** iteration: 1071 ******
loss: 1.0715882778167725
***** iteration: 1072 ******
loss: 0.8793045282363892
***** iteration: 1073 ******
loss: 0.32691314816474915
***** iteration: 1074 ******
loss: 0.4083060026168823
***** iteration: 1075 ******
loss: 0.6452767252922058
***** iteration: 1076 ******
loss: 0.6328579783439636
***** iteration: 1077 ******
loss: 0.5422588586807251
***** iteration: 1078 ******
loss: 0.37061649560928345
***** iteration: 1079 ******
loss: 0.5025995373725891
***** iteration: 1080 ******
loss: 0.8259473443031311
***** iteration: 1081 ******
loss: 0.6673601865768433
***** iteration: 1082 ******
loss: 0.3598540425300598
***** iteration: 1083 ******
loss: 0.4718870222568512
***** iteration: 1084 ******
loss: 0.6223888993263245
***** iteration: 1085 ******
loss: 0.5518706440925598
***** iteration: 1086 ******
loss: 0.437194287776947
***** iteration: 1087 ******
loss: 0.447154700756073
***** iteration: 1088 ******
loss: 0.6527139544487
***** iteration: 1089 ******
loss: 0.46318143606185913
***** iteration: 1090 ******
loss: 0.37059569358825684
***** iteration: 1091 ******
loss: 0.4513821601867676
***** iteration: 1092 ******
loss: 0.443148672580719
***** iteration: 1093 ******
loss: 0.3410014510154724
***** iteration: 1094 ******
loss: 0.8373280763626099
***** iteration: 1095 ******
loss: 1.210330605506897
***** iteration: 1096 ******
loss: 1.0418872833251953
***** iteration: 1097 ******
loss: 0.763766348361969
***** iteration: 1098 ******
loss: 0.3893851041793823
***** iteration: 1099 ******
loss: 0.5121488571166992
***** iteration: 1100 ******
loss: 0.5393729209899902
***** iteration: 1101 ******
loss: 0.7478988170623779
***** iteration: 1102 ******
loss: 0.4858655035495758
***** iteration: 1103 ******
loss: 0.30334940552711487
***** iteration: 1104 ******
loss: 0.5368465185165405
***** iteration: 1105 ******
loss: 0.669664740562439
***** iteration: 1106 ******
loss: 0.3921320140361786
***** iteration: 1107 ******
loss: 0.5136193633079529
***** iteration: 1108 ******
loss: 0.5553229451179504
***** iteration: 1109 ******
loss: 0.5725058317184448
***** iteration: 1110 ******
loss: 0.56741863489151
***** iteration: 1111 ******
loss: 0.3313104212284088
***** iteration: 1112 ******
loss: 0.6120280027389526
***** iteration: 1113 ******
loss: 0.9508557319641113
***** iteration: 1114 ******
loss: 0.7909529209136963
***** iteration: 1115 ******
loss: 0.4818797707557678
***** iteration: 1116 ******
loss: 0.5918567776679993
***** iteration: 1117 ******
loss: 0.6125602126121521
***** iteration: 1118 ******
loss: 0.4850779175758362
***** iteration: 1119 ******
loss: 0.5868298411369324
***** iteration: 1120 ******
loss: 0.5635623931884766
***** iteration: 1121 ******
loss: 0.6558926105499268
***** iteration: 1122 ******
loss: 0.42795342206954956
***** iteration: 1123 ******
loss: 0.49431750178337097
***** iteration: 1124 ******
loss: 0.5659098029136658
***** iteration: 1125 ******
loss: 0.4679567515850067
***** iteration: 1126 ******
loss: 0.4220788776874542
***** iteration: 1127 ******
loss: 0.5340768098831177
***** iteration: 1128 ******
loss: 0.5872478485107422
***** iteration: 1129 ******
loss: 0.37250590324401855
***** iteration: 1130 ******
loss: 0.44941842555999756
***** iteration: 1131 ******
loss: 0.5196910500526428
***** iteration: 1132 ******
loss: 0.46375322341918945
***** iteration: 1133 ******
loss: 0.3330163359642029
***** iteration: 1134 ******
loss: 0.6080881953239441
***** iteration: 1135 ******
loss: 0.8769262433052063
***** iteration: 1136 ******
loss: 0.6857835054397583
***** iteration: 1137 ******
loss: 0.2938874363899231
***** iteration: 1138 ******
loss: 0.48264816403388977
***** iteration: 1139 ******
loss: 0.6317627429962158
***** iteration: 1140 ******
loss: 0.5975742340087891
***** iteration: 1141 ******
loss: 0.40597259998321533
***** iteration: 1142 ******
loss: 0.3427519202232361
***** iteration: 1143 ******
loss: 0.934539258480072
***** iteration: 1144 ******
loss: 1.139839768409729
***** iteration: 1145 ******
loss: 0.7363635301589966
***** iteration: 1146 ******
loss: 0.3994796574115753
***** iteration: 1147 ******
loss: 0.427063912153244
***** iteration: 1148 ******
loss: 0.5370756983757019
***** iteration: 1149 ******
loss: 0.47920864820480347
***** iteration: 1150 ******
loss: 0.33238208293914795
***** iteration: 1151 ******
loss: 0.44073668122291565
***** iteration: 1152 ******
loss: 0.5698217749595642
***** iteration: 1153 ******
loss: 0.3820250928401947
***** iteration: 1154 ******
loss: 0.3609866201877594
***** iteration: 1155 ******
loss: 0.48638486862182617
***** iteration: 1156 ******
loss: 0.44391322135925293
***** iteration: 1157 ******
loss: 0.37001755833625793
***** iteration: 1158 ******
loss: 0.46239203214645386
***** iteration: 1159 ******
loss: 0.5954447984695435
***** iteration: 1160 ******
loss: 0.57781982421875
***** iteration: 1161 ******
loss: 0.3088982105255127
***** iteration: 1162 ******
loss: 0.3649146854877472
***** iteration: 1163 ******
loss: 0.3830522894859314
***** iteration: 1164 ******
loss: 0.2914498746395111
***** iteration: 1165 ******
loss: 0.4761311709880829
***** iteration: 1166 ******
loss: 0.6957065463066101
***** iteration: 1167 ******
loss: 0.4575669467449188
***** iteration: 1168 ******
loss: 0.2520187199115753
***** iteration: 1169 ******
loss: 0.37574052810668945
***** iteration: 1170 ******
loss: 0.2890908122062683
***** iteration: 1171 ******
loss: 0.2771378457546234
***** iteration: 1172 ******
loss: 0.39516034722328186
***** iteration: 1173 ******
loss: 0.36805909872055054
***** iteration: 1174 ******
loss: 0.4276466369628906
***** iteration: 1175 ******
loss: 0.4414619207382202
***** iteration: 1176 ******
loss: 0.3212836682796478
***** iteration: 1177 ******
loss: 1.5758588314056396
***** iteration: 1178 ******
loss: 2.052772283554077
***** iteration: 1179 ******
loss: 1.3353986740112305
***** iteration: 1180 ******
loss: 1.20712149143219
***** iteration: 1181 ******
loss: 0.6354073286056519
***** iteration: 1182 ******
loss: 0.4995717406272888
***** iteration: 1183 ******
loss: 0.6568418145179749
***** iteration: 1184 ******
loss: 0.6348281502723694
***** iteration: 1185 ******
loss: 0.45212113857269287
***** iteration: 1186 ******
loss: 0.4167701303958893
***** iteration: 1187 ******
loss: 1.0011982917785645
***** iteration: 1188 ******
loss: 0.8425835967063904
***** iteration: 1189 ******
loss: 0.695320725440979
***** iteration: 1190 ******
loss: 0.39850783348083496
***** iteration: 1191 ******
loss: 0.5644763112068176
***** iteration: 1192 ******
loss: 0.6816655397415161
***** iteration: 1193 ******
loss: 0.6182113289833069
***** iteration: 1194 ******
loss: 0.40084850788116455
***** iteration: 1195 ******
loss: 0.3958706557750702
***** iteration: 1196 ******
loss: 1.3527898788452148
***** iteration: 1197 ******
loss: 0.9366375207901001
***** iteration: 1198 ******
loss: 0.720668613910675
***** iteration: 1199 ******
loss: 0.3558361530303955
***** iteration: 1200 ******
loss: 0.5313560962677002
***** iteration: 1201 ******
loss: 0.6606506705284119
***** iteration: 1202 ******
loss: 0.6933954954147339
***** iteration: 1203 ******
loss: 0.5150143504142761
***** iteration: 1204 ******
loss: 0.5158916115760803
***** iteration: 1205 ******
loss: 0.3336068391799927
***** iteration: 1206 ******
loss: 0.48665881156921387
***** iteration: 1207 ******
loss: 0.4322766959667206
***** iteration: 1208 ******
loss: 0.4812973439693451
***** iteration: 1209 ******
loss: 0.5211403965950012
***** iteration: 1210 ******
loss: 0.42465120553970337
***** iteration: 1211 ******
loss: 0.40466606616973877
***** iteration: 1212 ******
loss: 1.1501222848892212
***** iteration: 1213 ******
loss: 1.0815742015838623
***** iteration: 1214 ******
loss: 0.4614560008049011
***** iteration: 1215 ******
loss: 0.38983869552612305
***** iteration: 1216 ******
loss: 0.40786534547805786
***** iteration: 1217 ******
loss: 0.33427077531814575
***** iteration: 1218 ******
loss: 1.326722502708435
***** iteration: 1219 ******
loss: 1.6360282897949219
***** iteration: 1220 ******
loss: 1.0158096551895142
***** iteration: 1221 ******
loss: 0.8172143697738647
***** iteration: 1222 ******
loss: 0.4054913818836212
***** iteration: 1223 ******
loss: 0.6034606099128723
***** iteration: 1224 ******
loss: 0.7469751834869385
***** iteration: 1225 ******
loss: 0.7069208025932312
***** iteration: 1226 ******
loss: 0.5018630027770996
***** iteration: 1227 ******
loss: 0.4273374080657959
***** iteration: 1228 ******
loss: 0.7423864603042603
***** iteration: 1229 ******
loss: 0.9933382868766785
***** iteration: 1230 ******
loss: 0.8145344257354736
***** iteration: 1231 ******
loss: 0.350503146648407
***** iteration: 1232 ******
loss: 0.48945704102516174
***** iteration: 1233 ******
loss: 0.623017430305481
***** iteration: 1234 ******
loss: 0.6027823090553284
***** iteration: 1235 ******
loss: 0.5604491233825684
***** iteration: 1236 ******
loss: 0.3159253001213074
***** iteration: 1237 ******
loss: 0.5854059457778931
***** iteration: 1238 ******
loss: 0.9957912564277649
***** iteration: 1239 ******
loss: 0.9121928215026855
***** iteration: 1240 ******
loss: 0.47575247287750244
***** iteration: 1241 ******
loss: 0.4983937740325928
***** iteration: 1242 ******
loss: 0.6085589528083801
***** iteration: 1243 ******
loss: 0.5460995435714722
***** iteration: 1244 ******
loss: 0.5093482732772827
***** iteration: 1245 ******
loss: 0.3713263273239136
***** iteration: 1246 ******
loss: 0.5262126922607422
***** iteration: 1247 ******
loss: 0.5303940176963806
***** iteration: 1248 ******
loss: 0.47206318378448486
***** iteration: 1249 ******
loss: 0.4788605272769928
***** iteration: 1250 ******
loss: 0.3957813084125519
***** iteration: 1251 ******
loss: 0.3658521771430969
***** iteration: 1252 ******
loss: 1.148643970489502
***** iteration: 1253 ******
loss: 0.6278488636016846
***** iteration: 1254 ******
loss: 0.3421928584575653
***** iteration: 1255 ******
loss: 0.39118432998657227
***** iteration: 1256 ******
loss: 0.41244280338287354
***** iteration: 1257 ******
loss: 0.34449735283851624
***** iteration: 1258 ******
loss: 0.3137474060058594
***** iteration: 1259 ******
loss: 1.5696004629135132
***** iteration: 1260 ******
loss: 1.8977155685424805
***** iteration: 1261 ******
loss: 0.4886467456817627
***** iteration: 1262 ******
loss: 0.34845393896102905
***** iteration: 1263 ******
loss: 0.391104131937027
***** iteration: 1264 ******
loss: 0.40764251351356506
***** iteration: 1265 ******
loss: 0.4010932445526123
***** iteration: 1266 ******
loss: 0.31935247778892517
***** iteration: 1267 ******
loss: 0.37306246161460876
***** iteration: 1268 ******
loss: 1.1606481075286865
***** iteration: 1269 ******
loss: 0.7447333931922913
***** iteration: 1270 ******
loss: 0.25839054584503174
***** iteration: 1271 ******
loss: 0.3907199203968048
***** iteration: 1272 ******
loss: 0.4315808117389679
***** iteration: 1273 ******
loss: 0.3271910846233368
***** iteration: 1274 ******
loss: 0.6222312450408936
***** iteration: 1275 ******
loss: 0.846392035484314
***** iteration: 1276 ******
loss: 0.7043575644493103
***** iteration: 1277 ******
loss: 0.2945825755596161
***** iteration: 1278 ******
loss: 0.43533599376678467
***** iteration: 1279 ******
loss: 0.47861671447753906
***** iteration: 1280 ******
loss: 0.4436912536621094
***** iteration: 1281 ******
loss: 0.4452347159385681
***** iteration: 1282 ******
loss: 0.4355390965938568
***** iteration: 1283 ******
loss: 0.36458057165145874
***** iteration: 1284 ******
loss: 0.34356316924095154
***** iteration: 1285 ******
loss: 0.8427869081497192
***** iteration: 1286 ******
loss: 0.8291630744934082
***** iteration: 1287 ******
loss: 0.2534720301628113
***** iteration: 1288 ******
loss: 0.3714463710784912
***** iteration: 1289 ******
loss: 1.0985954999923706
***** iteration: 1290 ******
loss: 0.5686421394348145
***** iteration: 1291 ******
loss: 0.5615301132202148
***** iteration: 1292 ******
loss: 0.5708132386207581
***** iteration: 1293 ******
loss: 0.5107237696647644
***** iteration: 1294 ******
loss: 0.42710304260253906
***** iteration: 1295 ******
loss: 0.3166324496269226
***** iteration: 1296 ******
loss: 2.0367188453674316
***** iteration: 1297 ******
loss: 3.9633166790008545
***** iteration: 1298 ******
loss: 3.5765914916992188
***** iteration: 1299 ******
loss: 1.2045904397964478
***** iteration: 1300 ******
loss: 0.4447824954986572
***** iteration: 1301 ******
loss: 0.43118906021118164
***** iteration: 1302 ******
loss: 0.8747433423995972
***** iteration: 1303 ******
loss: 1.0770726203918457
***** iteration: 1304 ******
loss: 0.7953142523765564
***** iteration: 1305 ******
loss: 0.560401976108551
***** iteration: 1306 ******
loss: 0.6126723885536194
***** iteration: 1307 ******
loss: 0.5102196931838989
***** iteration: 1308 ******
loss: 0.5866089463233948
***** iteration: 1309 ******
loss: 0.617222249507904
***** iteration: 1310 ******
loss: 0.3151523172855377
***** iteration: 1311 ******
loss: 0.3977142572402954
***** iteration: 1312 ******
loss: 0.8154953122138977
***** iteration: 1313 ******
loss: 0.5773845911026001
***** iteration: 1314 ******
loss: 0.520251452922821
***** iteration: 1315 ******
loss: 0.38609394431114197
***** iteration: 1316 ******
loss: 0.7181169390678406
***** iteration: 1317 ******
loss: 1.1225100755691528
***** iteration: 1318 ******
loss: 1.0120500326156616
***** iteration: 1319 ******
loss: 0.49073436856269836
***** iteration: 1320 ******
loss: 0.49515849351882935
***** iteration: 1321 ******
loss: 0.6044886112213135
***** iteration: 1322 ******
loss: 0.5350031852722168
***** iteration: 1323 ******
loss: 0.3391970992088318
***** iteration: 1324 ******
loss: 0.45793426036834717
***** iteration: 1325 ******
loss: 0.7595354318618774
***** iteration: 1326 ******
loss: 0.7194429039955139
***** iteration: 1327 ******
loss: 0.34420308470726013
***** iteration: 1328 ******
loss: 0.40652894973754883
***** iteration: 1329 ******
loss: 0.5074549317359924
***** iteration: 1330 ******
loss: 0.4801517426967621
***** iteration: 1331 ******
loss: 0.32329532504081726
***** iteration: 1332 ******
loss: 0.4112866222858429
***** iteration: 1333 ******
loss: 0.7368203997612
***** iteration: 1334 ******
loss: 0.6681048274040222
***** iteration: 1335 ******
loss: 0.3648033142089844
***** iteration: 1336 ******
loss: 0.4506259858608246
***** iteration: 1337 ******
loss: 0.6194778680801392
***** iteration: 1338 ******
loss: 0.6035293340682983
***** iteration: 1339 ******
loss: 0.5212383270263672
***** iteration: 1340 ******
loss: 0.38180333375930786
***** iteration: 1341 ******
loss: 0.6826784014701843
***** iteration: 1342 ******
loss: 1.024060845375061
***** iteration: 1343 ******
loss: 0.983521044254303
***** iteration: 1344 ******
loss: 0.4686003029346466
***** iteration: 1345 ******
loss: 0.37565720081329346
***** iteration: 1346 ******
loss: 0.45818740129470825
***** iteration: 1347 ******
loss: 0.4097735583782196
***** iteration: 1348 ******
loss: 0.3197743892669678
***** iteration: 1349 ******
loss: 0.48363882303237915
***** iteration: 1350 ******
loss: 0.5449649095535278
***** iteration: 1351 ******
loss: 0.4216821491718292
***** iteration: 1352 ******
loss: 0.4821769893169403
***** iteration: 1353 ******
loss: 0.4907505512237549
***** iteration: 1354 ******
loss: 0.350200355052948
***** iteration: 1355 ******
loss: 1.1195917129516602
***** iteration: 1356 ******
loss: 1.080435037612915
***** iteration: 1357 ******
loss: 1.1143479347229004
***** iteration: 1358 ******
loss: 1.0497690439224243
***** iteration: 1359 ******
loss: 0.5740351676940918
***** iteration: 1360 ******
loss: 0.48339763283729553
***** iteration: 1361 ******
loss: 0.6150726675987244
***** iteration: 1362 ******
loss: 0.5824266672134399
***** iteration: 1363 ******
loss: 0.4173220098018646
***** iteration: 1364 ******
loss: 0.5189967155456543
***** iteration: 1365 ******
loss: 0.733165979385376
***** iteration: 1366 ******
loss: 1.1165523529052734
***** iteration: 1367 ******
loss: 0.7624172568321228
***** iteration: 1368 ******
loss: 0.2759230434894562
***** iteration: 1369 ******
loss: 0.3208143711090088
***** iteration: 1370 ******
loss: 0.2972346246242523
***** iteration: 1371 ******
loss: 0.267162948846817
***** iteration: 1372 ******
loss: 0.27605271339416504
***** iteration: 1373 ******
loss: 0.27943485975265503
***** iteration: 1374 ******
loss: 0.2629927098751068
***** iteration: 1375 ******
loss: 0.306060791015625
***** iteration: 1376 ******
loss: 0.37158817052841187
***** iteration: 1377 ******
loss: 0.2523067891597748
***** iteration: 1378 ******
loss: 0.7993910312652588
***** iteration: 1379 ******
loss: 0.5614998936653137
***** iteration: 1380 ******
loss: 0.5383321046829224
***** iteration: 1381 ******
loss: 0.5014601945877075
***** iteration: 1382 ******
loss: 0.41039130091667175
***** iteration: 1383 ******
loss: 0.4981192648410797
***** iteration: 1384 ******
loss: 0.3848264515399933
***** iteration: 1385 ******
loss: 0.4389997124671936
***** iteration: 1386 ******
loss: 0.4928838610649109
***** iteration: 1387 ******
loss: 0.47485053539276123
***** iteration: 1388 ******
loss: 0.35049158334732056
***** iteration: 1389 ******
loss: 0.49519965052604675
***** iteration: 1390 ******
loss: 0.6280775666236877
***** iteration: 1391 ******
loss: 0.3955208957195282
***** iteration: 1392 ******
loss: 0.4860982298851013
***** iteration: 1393 ******
loss: 0.5355700850486755
***** iteration: 1394 ******
loss: 0.41188210248947144
***** iteration: 1395 ******
loss: 0.6874675154685974
***** iteration: 1396 ******
loss: 0.7356142997741699
***** iteration: 1397 ******
loss: 0.871798038482666
***** iteration: 1398 ******
loss: 0.6598395109176636
***** iteration: 1399 ******
loss: 0.37640881538391113
***** iteration: 1400 ******
loss: 0.4232044517993927
***** iteration: 1401 ******
loss: 0.34472331404685974
***** iteration: 1402 ******
loss: 0.4011470675468445
***** iteration: 1403 ******
loss: 0.5427385568618774
***** iteration: 1404 ******
loss: 0.41537198424339294
***** iteration: 1405 ******
loss: 0.39330074191093445
***** iteration: 1406 ******
loss: 0.3762674629688263
***** iteration: 1407 ******
loss: 0.37255769968032837
***** iteration: 1408 ******
loss: 0.48912835121154785
***** iteration: 1409 ******
loss: 0.5074118375778198
***** iteration: 1410 ******
loss: 0.32363927364349365
***** iteration: 1411 ******
loss: 1.1900092363357544
***** iteration: 1412 ******
loss: 1.815253496170044
***** iteration: 1413 ******
loss: 0.7220962643623352
***** iteration: 1414 ******
loss: 0.7292256951332092
***** iteration: 1415 ******
loss: 0.6814113259315491
***** iteration: 1416 ******
loss: 0.5282211899757385
***** iteration: 1417 ******
loss: 1.1011805534362793
***** iteration: 1418 ******
loss: 1.487208366394043
***** iteration: 1419 ******
loss: 1.3440191745758057
***** iteration: 1420 ******
loss: 0.7483149766921997
***** iteration: 1421 ******
loss: 0.47979530692100525
***** iteration: 1422 ******
loss: 0.5657210350036621
***** iteration: 1423 ******
loss: 0.5248662233352661
***** iteration: 1424 ******
loss: 0.47812700271606445
***** iteration: 1425 ******
loss: 0.35067230463027954
***** iteration: 1426 ******
loss: 0.3782012462615967
***** iteration: 1427 ******
loss: 0.3574940264225006
***** iteration: 1428 ******
loss: 0.3244185149669647
***** iteration: 1429 ******
loss: 0.388034850358963
***** iteration: 1430 ******
loss: 0.3742051422595978
***** iteration: 1431 ******
loss: 0.305138498544693
***** iteration: 1432 ******
loss: 0.304169625043869
***** iteration: 1433 ******
loss: 0.28350144624710083
***** iteration: 1434 ******
loss: 0.2629224359989166
***** iteration: 1435 ******
loss: 0.2898266613483429
***** iteration: 1436 ******
loss: 0.23616379499435425
***** iteration: 1437 ******
loss: 1.2370942831039429
***** iteration: 1438 ******
loss: 0.5372839570045471
***** iteration: 1439 ******
loss: 0.5565465092658997
***** iteration: 1440 ******
loss: 0.4149801731109619
***** iteration: 1441 ******
loss: 0.2865399420261383
***** iteration: 1442 ******
loss: 0.6927507519721985
***** iteration: 1443 ******
loss: 0.7471405267715454
***** iteration: 1444 ******
loss: 0.39458155632019043
***** iteration: 1445 ******
loss: 0.4861244857311249
***** iteration: 1446 ******
loss: 0.5846816301345825
***** iteration: 1447 ******
loss: 0.5050426125526428
***** iteration: 1448 ******
loss: 0.28489282727241516
***** iteration: 1449 ******
loss: 0.7018061876296997
***** iteration: 1450 ******
loss: 1.1598325967788696
***** iteration: 1451 ******
loss: 1.1192179918289185
***** iteration: 1452 ******
loss: 0.620075523853302
***** iteration: 1453 ******
loss: 0.3861329257488251
***** iteration: 1454 ******
loss: 0.4943758249282837
***** iteration: 1455 ******
loss: 0.4292670488357544
***** iteration: 1456 ******
loss: 0.660723090171814
***** iteration: 1457 ******
loss: 0.34874674677848816
***** iteration: 1458 ******
loss: 0.35712045431137085
***** iteration: 1459 ******
loss: 0.2758788466453552
***** iteration: 1460 ******
loss: 0.2575152516365051
***** iteration: 1461 ******
loss: 0.2568529546260834
***** iteration: 1462 ******
loss: 0.31954360008239746
***** iteration: 1463 ******
loss: 0.40958070755004883
***** iteration: 1464 ******
loss: 0.2778308391571045
***** iteration: 1465 ******
loss: 0.3492887616157532
***** iteration: 1466 ******
loss: 0.3696860671043396
***** iteration: 1467 ******
loss: 0.29855039715766907
***** iteration: 1468 ******
loss: 0.3431010842323303
***** iteration: 1469 ******
loss: 0.4489898085594177
***** iteration: 1470 ******
loss: 0.2722146213054657
***** iteration: 1471 ******
loss: 0.40945565700531006
***** iteration: 1472 ******
loss: 0.5727381110191345
***** iteration: 1473 ******
loss: 0.5945722460746765
***** iteration: 1474 ******
loss: 0.44725117087364197
***** iteration: 1475 ******
loss: 0.3378528952598572
***** iteration: 1476 ******
loss: 0.5101926326751709
***** iteration: 1477 ******
loss: 0.5711677074432373
***** iteration: 1478 ******
loss: 0.25965237617492676
***** iteration: 1479 ******
loss: 0.3474874794483185
***** iteration: 1480 ******
loss: 0.3910374343395233
***** iteration: 1481 ******
loss: 0.28731292486190796
***** iteration: 1482 ******
loss: 0.48354944586753845
***** iteration: 1483 ******
loss: 0.8127292394638062
***** iteration: 1484 ******
loss: 0.8187647461891174
***** iteration: 1485 ******
loss: 0.4053177237510681
***** iteration: 1486 ******
loss: 0.3777131140232086
***** iteration: 1487 ******
loss: 0.4753766655921936
***** iteration: 1488 ******
loss: 0.4013577997684479
***** iteration: 1489 ******
loss: 0.8321337699890137
***** iteration: 1490 ******
loss: 0.41685229539871216
***** iteration: 1491 ******
loss: 0.5961082577705383
***** iteration: 1492 ******
loss: 0.4714929461479187
***** iteration: 1493 ******
loss: 0.44711199402809143
***** iteration: 1494 ******
loss: 0.5024136304855347
***** iteration: 1495 ******
loss: 0.42954331636428833
***** iteration: 1496 ******
loss: 0.4546649158000946
***** iteration: 1497 ******
loss: 0.45884454250335693
***** iteration: 1498 ******
loss: 0.26971665024757385
***** iteration: 1499 ******
loss: 0.309938907623291
***** iteration: 1500 ******
loss: 0.3390594720840454
***** iteration: 1501 ******
loss: 0.3287852704524994
***** iteration: 1502 ******
loss: 0.32432734966278076
***** iteration: 1503 ******
loss: 0.2940184473991394
***** iteration: 1504 ******
loss: 0.34196004271507263
***** iteration: 1505 ******
loss: 0.434796541929245
***** iteration: 1506 ******
loss: 0.4069531559944153
***** iteration: 1507 ******
loss: 0.4206216633319855
***** iteration: 1508 ******
loss: 0.465137243270874
***** iteration: 1509 ******
loss: 0.32766321301460266
***** iteration: 1510 ******
loss: 2.1702282428741455
***** iteration: 1511 ******
loss: 2.6393017768859863
***** iteration: 1512 ******
loss: 1.0317049026489258
***** iteration: 1513 ******
loss: 0.8475775122642517
***** iteration: 1514 ******
loss: 0.863070011138916
***** iteration: 1515 ******
loss: 0.709793746471405
***** iteration: 1516 ******
loss: 0.6670106053352356
***** iteration: 1517 ******
loss: 1.070972204208374
***** iteration: 1518 ******
loss: 1.029173493385315
***** iteration: 1519 ******
loss: 0.6505920886993408
***** iteration: 1520 ******
loss: 0.7430540919303894
***** iteration: 1521 ******
loss: 0.8275787234306335
***** iteration: 1522 ******
loss: 0.7366082668304443
***** iteration: 1523 ******
loss: 0.49799248576164246
***** iteration: 1524 ******
loss: 0.559701681137085
***** iteration: 1525 ******
loss: 1.1912320852279663
***** iteration: 1526 ******
loss: 1.6422358751296997
***** iteration: 1527 ******
loss: 1.0168659687042236
***** iteration: 1528 ******
loss: 0.5959544777870178
***** iteration: 1529 ******
loss: 0.40437692403793335
***** iteration: 1530 ******
loss: 0.501099705696106
***** iteration: 1531 ******
loss: 0.4443259537220001
***** iteration: 1532 ******
loss: 0.29891154170036316
***** iteration: 1533 ******
loss: 2.155217170715332
***** iteration: 1534 ******
loss: 3.3121347427368164
***** iteration: 1535 ******
loss: 2.272265672683716
***** iteration: 1536 ******
loss: 1.375977873802185
***** iteration: 1537 ******
loss: 1.064658761024475
***** iteration: 1538 ******
loss: 0.4780678451061249
***** iteration: 1539 ******
loss: 0.6788764595985413
***** iteration: 1540 ******
loss: 0.8939087390899658
***** iteration: 1541 ******
loss: 0.9479789137840271
***** iteration: 1542 ******
loss: 0.8452410697937012
***** iteration: 1543 ******
loss: 0.6120892763137817
***** iteration: 1544 ******
loss: 0.4521440863609314
***** iteration: 1545 ******
loss: 0.6903151869773865
***** iteration: 1546 ******
loss: 0.5972083210945129
***** iteration: 1547 ******
loss: 0.7446536421775818
***** iteration: 1548 ******
loss: 0.6005813479423523
***** iteration: 1549 ******
loss: 0.4501720666885376
***** iteration: 1550 ******
loss: 0.4679255783557892
***** iteration: 1551 ******
loss: 0.40391233563423157
***** iteration: 1552 ******
loss: 0.46971601247787476
***** iteration: 1553 ******
loss: 0.43462586402893066
***** iteration: 1554 ******
loss: 0.4696182310581207
***** iteration: 1555 ******
loss: 0.3718428313732147
***** iteration: 1556 ******
loss: 0.38267767429351807
***** iteration: 1557 ******
loss: 0.36969441175460815
***** iteration: 1558 ******
loss: 0.35470327734947205
***** iteration: 1559 ******
loss: 0.29921042919158936
***** iteration: 1560 ******
loss: 0.47912752628326416
***** iteration: 1561 ******
loss: 0.3284154534339905
***** iteration: 1562 ******
loss: 0.3127652406692505
***** iteration: 1563 ******
loss: 0.3512941896915436
***** iteration: 1564 ******
loss: 0.7036065459251404
***** iteration: 1565 ******
loss: 0.3380832076072693
***** iteration: 1566 ******
loss: 0.2813059091567993
***** iteration: 1567 ******
loss: 1.8626126050949097
***** iteration: 1568 ******
loss: 3.2801826000213623
***** iteration: 1569 ******
loss: 2.4476897716522217
***** iteration: 1570 ******
loss: 0.3957805633544922
***** iteration: 1571 ******
loss: 0.3992830514907837
***** iteration: 1572 ******
loss: 0.6969959735870361
***** iteration: 1573 ******
loss: 0.7999740839004517
***** iteration: 1574 ******
loss: 0.6024947166442871
***** iteration: 1575 ******
loss: 0.7848208546638489
***** iteration: 1576 ******
loss: 0.8591832518577576
***** iteration: 1577 ******
loss: 0.7580651044845581
***** iteration: 1578 ******
loss: 0.527461588382721
***** iteration: 1579 ******
loss: 1.0311145782470703
***** iteration: 1580 ******
loss: 1.3486676216125488
***** iteration: 1581 ******
loss: 1.1628756523132324
***** iteration: 1582 ******
loss: 1.0800212621688843
***** iteration: 1583 ******
loss: 0.820898711681366
***** iteration: 1584 ******
loss: 0.45394447445869446
***** iteration: 1585 ******
loss: 0.5240716934204102
***** iteration: 1586 ******
loss: 0.4405893087387085
***** iteration: 1587 ******
loss: 0.5891569256782532
***** iteration: 1588 ******
loss: 0.8100500702857971
***** iteration: 1589 ******
loss: 0.5904610753059387
***** iteration: 1590 ******
loss: 0.4179058074951172
***** iteration: 1591 ******
loss: 0.4294424057006836
***** iteration: 1592 ******
loss: 0.3091013431549072
***** iteration: 1593 ******
loss: 0.5297653675079346
***** iteration: 1594 ******
loss: 1.1071466207504272
***** iteration: 1595 ******
loss: 0.7668442130088806
***** iteration: 1596 ******
loss: 0.32415130734443665
***** iteration: 1597 ******
loss: 0.3972797393798828
***** iteration: 1598 ******
loss: 0.5011168718338013
***** iteration: 1599 ******
loss: 0.4337591528892517
***** iteration: 1600 ******
loss: 0.3190116286277771
***** iteration: 1601 ******
loss: 0.32407423853874207
***** iteration: 1602 ******
loss: 0.4908895790576935
***** iteration: 1603 ******
loss: 0.4162452220916748
***** iteration: 1604 ******
loss: 0.3218039572238922
***** iteration: 1605 ******
loss: 0.3211315870285034
***** iteration: 1606 ******
loss: 0.3582063913345337
***** iteration: 1607 ******
loss: 0.27258363366127014
***** iteration: 1608 ******
loss: 0.4471786916255951
***** iteration: 1609 ******
loss: 0.46921733021736145
***** iteration: 1610 ******
loss: 0.4444652199745178
***** iteration: 1611 ******
loss: 0.3293083608150482
***** iteration: 1612 ******
loss: 0.6229422092437744
***** iteration: 1613 ******
loss: 0.8980196118354797
***** iteration: 1614 ******
loss: 0.6770049333572388
***** iteration: 1615 ******
loss: 0.2880571186542511
***** iteration: 1616 ******
loss: 0.721439778804779
***** iteration: 1617 ******
loss: 1.034960150718689
***** iteration: 1618 ******
loss: 0.7863252758979797
***** iteration: 1619 ******
loss: 0.8095099925994873
***** iteration: 1620 ******
loss: 0.6748868227005005
***** iteration: 1621 ******
loss: 0.4067345857620239
***** iteration: 1622 ******
loss: 1.023902177810669
***** iteration: 1623 ******
loss: 1.6696534156799316
***** iteration: 1624 ******
loss: 1.760507583618164
***** iteration: 1625 ******
loss: 1.3447983264923096
***** iteration: 1626 ******
loss: 0.5567798018455505
***** iteration: 1627 ******
loss: 0.4540553390979767
***** iteration: 1628 ******
loss: 0.6498461365699768
***** iteration: 1629 ******
loss: 0.7598748803138733
***** iteration: 1630 ******
loss: 0.6944966316223145
***** iteration: 1631 ******
loss: 0.4686814546585083
***** iteration: 1632 ******
loss: 0.2798826992511749
***** iteration: 1633 ******
loss: 0.5612054467201233
***** iteration: 1634 ******
loss: 0.7499855756759644
***** iteration: 1635 ******
loss: 0.35869207978248596
***** iteration: 1636 ******
loss: 0.3161844313144684
***** iteration: 1637 ******
loss: 0.3584897220134735
***** iteration: 1638 ******
loss: 0.31431683897972107
***** iteration: 1639 ******
loss: 0.4678482413291931
***** iteration: 1640 ******
loss: 0.5470888018608093
***** iteration: 1641 ******
loss: 0.3987906873226166
***** iteration: 1642 ******
loss: 0.4775561988353729
***** iteration: 1643 ******
loss: 0.5071230530738831
***** iteration: 1644 ******
loss: 0.7336080074310303
***** iteration: 1645 ******
loss: 0.43708133697509766
***** iteration: 1646 ******
loss: 0.3624434173107147
***** iteration: 1647 ******
loss: 0.3827088177204132
***** iteration: 1648 ******
loss: 0.3611438572406769
***** iteration: 1649 ******
loss: 0.30957233905792236
***** iteration: 1650 ******
loss: 0.325245201587677
***** iteration: 1651 ******
loss: 0.2907938063144684
***** iteration: 1652 ******
loss: 0.44433167576789856
***** iteration: 1653 ******
loss: 0.43624594807624817
***** iteration: 1654 ******
loss: 0.3889732360839844
***** iteration: 1655 ******
loss: 0.2637343108654022
***** iteration: 1656 ******
loss: 0.7423478364944458
***** iteration: 1657 ******
loss: 0.5466106534004211
***** iteration: 1658 ******
loss: 0.40306413173675537
***** iteration: 1659 ******
loss: 0.3287162482738495
***** iteration: 1660 ******
loss: 0.35684287548065186
***** iteration: 1661 ******
loss: 0.33614444732666016
***** iteration: 1662 ******
loss: 0.2818774878978729
***** iteration: 1663 ******
loss: 0.3882073163986206
***** iteration: 1664 ******
loss: 0.692672848701477
***** iteration: 1665 ******
loss: 0.42423179745674133
***** iteration: 1666 ******
loss: 0.3679448366165161
***** iteration: 1667 ******
loss: 0.34850582480430603
***** iteration: 1668 ******
loss: 0.4418296813964844
***** iteration: 1669 ******
loss: 0.29672688245773315
***** iteration: 1670 ******
loss: 0.47624608874320984
***** iteration: 1671 ******
loss: 0.580406665802002
***** iteration: 1672 ******
loss: 0.563518226146698
***** iteration: 1673 ******
loss: 0.4883035123348236
***** iteration: 1674 ******
loss: 0.37476712465286255
***** iteration: 1675 ******
loss: 0.489549458026886
***** iteration: 1676 ******
loss: 0.34428372979164124
***** iteration: 1677 ******
loss: 0.39753013849258423
***** iteration: 1678 ******
loss: 0.6682004928588867
***** iteration: 1679 ******
loss: 0.518392026424408
***** iteration: 1680 ******
loss: 0.42052313685417175
***** iteration: 1681 ******
loss: 0.35730624198913574
***** iteration: 1682 ******
loss: 0.40533846616744995
***** iteration: 1683 ******
loss: 0.32589101791381836
***** iteration: 1684 ******
loss: 0.27396082878112793
***** iteration: 1685 ******
loss: 0.5191914439201355
***** iteration: 1686 ******
loss: 0.3345402479171753
***** iteration: 1687 ******
loss: 0.2663637101650238
***** iteration: 1688 ******
loss: 0.38450872898101807
***** iteration: 1689 ******
loss: 0.40011072158813477
***** iteration: 1690 ******
loss: 0.3740619122982025
***** iteration: 1691 ******
loss: 0.2558123767375946
***** iteration: 1692 ******
loss: 0.2583441734313965
***** iteration: 1693 ******
loss: 0.3378063440322876
***** iteration: 1694 ******
loss: 0.3584015965461731
***** iteration: 1695 ******
loss: 0.28472912311553955
***** iteration: 1696 ******
loss: 0.5613510608673096
***** iteration: 1697 ******
loss: 0.7824009656906128
***** iteration: 1698 ******
loss: 0.5088330507278442
***** iteration: 1699 ******
loss: 0.26654618978500366
***** iteration: 1700 ******
loss: 0.2950505018234253
***** iteration: 1701 ******
loss: 0.28980737924575806
***** iteration: 1702 ******
loss: 0.3079024851322174
***** iteration: 1703 ******
loss: 0.2766161262989044
***** iteration: 1704 ******
loss: 0.32843926548957825
***** iteration: 1705 ******
loss: 0.4198461174964905
***** iteration: 1706 ******
loss: 0.28042590618133545
***** iteration: 1707 ******
loss: 0.29905661940574646
***** iteration: 1708 ******
loss: 0.32571324706077576
***** iteration: 1709 ******
loss: 0.36062610149383545
***** iteration: 1710 ******
loss: 0.3346278965473175
***** iteration: 1711 ******
loss: 0.2748013436794281
***** iteration: 1712 ******
loss: 1.1814115047454834
***** iteration: 1713 ******
loss: 0.8461353182792664
***** iteration: 1714 ******
loss: 0.2805258631706238
***** iteration: 1715 ******
loss: 0.28399422764778137
***** iteration: 1716 ******
loss: 0.27820852398872375
***** iteration: 1717 ******
loss: 0.245658278465271
***** iteration: 1718 ******
loss: 0.3168278932571411
***** iteration: 1719 ******
loss: 0.31305909156799316
***** iteration: 1720 ******
loss: 0.43416827917099
***** iteration: 1721 ******
loss: 0.42362651228904724
***** iteration: 1722 ******
loss: 0.22763387858867645
***** iteration: 1723 ******
loss: 0.9382618069648743
***** iteration: 1724 ******
loss: 0.37442976236343384
***** iteration: 1725 ******
loss: 0.3071863651275635
***** iteration: 1726 ******
loss: 0.392195463180542
***** iteration: 1727 ******
loss: 0.3939913809299469
***** iteration: 1728 ******
loss: 0.47125497460365295
***** iteration: 1729 ******
loss: 0.44923484325408936
***** iteration: 1730 ******
loss: 0.34421685338020325
***** iteration: 1731 ******
loss: 0.29487374424934387
***** iteration: 1732 ******
loss: 0.7033247947692871
***** iteration: 1733 ******
loss: 0.9399175047874451
***** iteration: 1734 ******
loss: 0.6490424275398254
***** iteration: 1735 ******
loss: 0.3841181695461273
***** iteration: 1736 ******
loss: 0.44367021322250366
***** iteration: 1737 ******
loss: 0.5373491644859314
***** iteration: 1738 ******
loss: 0.4549391567707062
***** iteration: 1739 ******
loss: 0.3040383756160736
***** iteration: 1740 ******
loss: 0.3007948100566864
***** iteration: 1741 ******
loss: 0.960115909576416
***** iteration: 1742 ******
loss: 0.9994133114814758
***** iteration: 1743 ******
loss: 0.3235824406147003
***** iteration: 1744 ******
loss: 0.29819074273109436
***** iteration: 1745 ******
loss: 0.5196631550788879
***** iteration: 1746 ******
loss: 0.6365509629249573
***** iteration: 1747 ******
loss: 0.3509998023509979
***** iteration: 1748 ******
loss: 0.3710685968399048
***** iteration: 1749 ******
loss: 0.45603886246681213
***** iteration: 1750 ******
loss: 0.39786240458488464
***** iteration: 1751 ******
loss: 0.2738315761089325
***** iteration: 1752 ******
loss: 0.5700912475585938
***** iteration: 1753 ******
loss: 0.7458996176719666
***** iteration: 1754 ******
loss: 0.41586166620254517
***** iteration: 1755 ******
loss: 0.29224368929862976
***** iteration: 1756 ******
loss: 0.3454805314540863
***** iteration: 1757 ******
loss: 0.2648770809173584
***** iteration: 1758 ******
loss: 0.4879032373428345
***** iteration: 1759 ******
loss: 0.6667985320091248
***** iteration: 1760 ******
loss: 0.46981489658355713
***** iteration: 1761 ******
loss: 0.27291339635849
***** iteration: 1762 ******
loss: 0.34200653433799744
***** iteration: 1763 ******
loss: 0.31058695912361145
***** iteration: 1764 ******
loss: 0.3057076930999756
***** iteration: 1765 ******
loss: 0.29251453280448914
***** iteration: 1766 ******
loss: 0.30571216344833374
***** iteration: 1767 ******
loss: 0.32565999031066895
***** iteration: 1768 ******
loss: 0.29999420046806335
***** iteration: 1769 ******
loss: 0.26477643847465515
***** iteration: 1770 ******
loss: 0.5403991341590881
***** iteration: 1771 ******
loss: 0.44112592935562134
***** iteration: 1772 ******
loss: 0.41958603262901306
***** iteration: 1773 ******
loss: 0.26654794812202454
***** iteration: 1774 ******
loss: 0.7129244804382324
***** iteration: 1775 ******
loss: 1.1108406782150269
***** iteration: 1776 ******
loss: 1.1077264547348022
***** iteration: 1777 ******
loss: 0.6292546987533569
***** iteration: 1778 ******
loss: 0.3161052167415619
***** iteration: 1779 ******
loss: 0.4124087393283844
***** iteration: 1780 ******
loss: 0.34747451543807983
***** iteration: 1781 ******
loss: 0.33252719044685364
***** iteration: 1782 ******
loss: 0.49355703592300415
***** iteration: 1783 ******
loss: 0.47334834933280945
***** iteration: 1784 ******
loss: 0.24959364533424377
***** iteration: 1785 ******
loss: 0.8118717074394226
***** iteration: 1786 ******
loss: 0.6422024965286255
***** iteration: 1787 ******
loss: 0.5576973557472229
***** iteration: 1788 ******
loss: 0.5376685857772827
***** iteration: 1789 ******
loss: 0.37476852536201477
***** iteration: 1790 ******
loss: 0.7947252988815308
***** iteration: 1791 ******
loss: 1.2027093172073364
***** iteration: 1792 ******
loss: 1.0960025787353516
***** iteration: 1793 ******
loss: 0.551530659198761
***** iteration: 1794 ******
loss: 0.3399832248687744
***** iteration: 1795 ******
loss: 0.43961194157600403
***** iteration: 1796 ******
loss: 0.4895075857639313
***** iteration: 1797 ******
loss: 0.3638858199119568
***** iteration: 1798 ******
loss: 0.282406747341156
***** iteration: 1799 ******
loss: 0.37151241302490234
***** iteration: 1800 ******
loss: 0.2664622664451599
***** iteration: 1801 ******
loss: 0.33509689569473267
***** iteration: 1802 ******
loss: 0.4230819642543793
***** iteration: 1803 ******
loss: 0.39216724038124084
***** iteration: 1804 ******
loss: 0.34502071142196655
***** iteration: 1805 ******
loss: 0.36790478229522705
***** iteration: 1806 ******
loss: 0.28039854764938354
***** iteration: 1807 ******
loss: 0.3152112662792206
***** iteration: 1808 ******
loss: 0.2904242277145386
***** iteration: 1809 ******
loss: 0.8518127799034119
***** iteration: 1810 ******
loss: 0.548222005367279
***** iteration: 1811 ******
loss: 0.301300048828125
***** iteration: 1812 ******
loss: 0.29899340867996216
***** iteration: 1813 ******
loss: 0.2599851191043854
***** iteration: 1814 ******
loss: 0.23726679384708405
***** iteration: 1815 ******
loss: 0.28856146335601807
***** iteration: 1816 ******
loss: 0.3280314803123474
***** iteration: 1817 ******
loss: 0.3231768310070038
***** iteration: 1818 ******
loss: 0.3124997019767761
***** iteration: 1819 ******
loss: 0.24977324903011322
***** iteration: 1820 ******
loss: 2.4960248470306396
***** iteration: 1821 ******
loss: 3.274142026901245
***** iteration: 1822 ******
loss: 1.894439935684204
***** iteration: 1823 ******
loss: 0.37654733657836914
***** iteration: 1824 ******
loss: 0.42257454991340637
***** iteration: 1825 ******
loss: 0.4621633291244507
***** iteration: 1826 ******
loss: 0.5590749979019165
***** iteration: 1827 ******
loss: 0.555885374546051
***** iteration: 1828 ******
loss: 0.465724378824234
***** iteration: 1829 ******
loss: 0.41726914048194885
***** iteration: 1830 ******
loss: 0.4037330448627472
***** iteration: 1831 ******
loss: 1.377488374710083
***** iteration: 1832 ******
loss: 1.7464853525161743
***** iteration: 1833 ******
loss: 0.3860900104045868
***** iteration: 1834 ******
loss: 0.40658488869667053
***** iteration: 1835 ******
loss: 0.48689815402030945
***** iteration: 1836 ******
loss: 0.5062133073806763
***** iteration: 1837 ******
loss: 0.6079263687133789
***** iteration: 1838 ******
loss: 0.5032379031181335
***** iteration: 1839 ******
loss: 0.4886118769645691
***** iteration: 1840 ******
loss: 0.4546525478363037
***** iteration: 1841 ******
loss: 0.2978231906890869
***** iteration: 1842 ******
loss: 2.182882070541382
***** iteration: 1843 ******
loss: 3.762354612350464
***** iteration: 1844 ******
loss: 3.122385263442993
***** iteration: 1845 ******
loss: 1.271773099899292
***** iteration: 1846 ******
loss: 0.9028155207633972
***** iteration: 1847 ******
loss: 0.3979029059410095
***** iteration: 1848 ******
loss: 0.6133085489273071
***** iteration: 1849 ******
loss: 0.6987140774726868
***** iteration: 1850 ******
loss: 0.6080369353294373
***** iteration: 1851 ******
loss: 0.8692603707313538
***** iteration: 1852 ******
loss: 0.30274632573127747
***** iteration: 1853 ******
loss: 0.35838761925697327
***** iteration: 1854 ******
loss: 0.38803645968437195
***** iteration: 1855 ******
loss: 0.3318459987640381
***** iteration: 1856 ******
loss: 0.32791534066200256
***** iteration: 1857 ******
loss: 0.46784767508506775
***** iteration: 1858 ******
loss: 0.4593319296836853
***** iteration: 1859 ******
loss: 0.3621322512626648
***** iteration: 1860 ******
loss: 0.39999300241470337
***** iteration: 1861 ******
loss: 0.39324069023132324
***** iteration: 1862 ******
loss: 0.4095090925693512
***** iteration: 1863 ******
loss: 0.41311463713645935
***** iteration: 1864 ******
loss: 0.3268011510372162
***** iteration: 1865 ******
loss: 0.4812006950378418
***** iteration: 1866 ******
loss: 0.36430618166923523
***** iteration: 1867 ******
loss: 0.3477599024772644
***** iteration: 1868 ******
loss: 0.37616807222366333
***** iteration: 1869 ******
loss: 0.3467034697532654
***** iteration: 1870 ******
loss: 0.28456488251686096
***** iteration: 1871 ******
loss: 0.4119870662689209
***** iteration: 1872 ******
loss: 0.3237606883049011
***** iteration: 1873 ******
loss: 0.2971540689468384
***** iteration: 1874 ******
loss: 0.2861838638782501
***** iteration: 1875 ******
loss: 0.2769959270954132
***** iteration: 1876 ******
loss: 0.27080291509628296
***** iteration: 1877 ******
loss: 0.7932032346725464
***** iteration: 1878 ******
loss: 0.3796674609184265
***** iteration: 1879 ******
loss: 0.3421839773654938
***** iteration: 1880 ******
loss: 0.4878796637058258
***** iteration: 1881 ******
loss: 0.5645843744277954
***** iteration: 1882 ******
loss: 0.29754534363746643
***** iteration: 1883 ******
loss: 0.5379547476768494
***** iteration: 1884 ******
loss: 0.6337917447090149
***** iteration: 1885 ******
loss: 0.6809788942337036
***** iteration: 1886 ******
loss: 0.5631074905395508
***** iteration: 1887 ******
loss: 0.32082080841064453
***** iteration: 1888 ******
loss: 1.0525158643722534
***** iteration: 1889 ******
loss: 1.9251861572265625
***** iteration: 1890 ******
loss: 1.9073103666305542
***** iteration: 1891 ******
loss: 1.7308192253112793
***** iteration: 1892 ******
loss: 1.0821785926818848
***** iteration: 1893 ******
loss: 0.35277435183525085
***** iteration: 1894 ******
loss: 0.5051015615463257
***** iteration: 1895 ******
loss: 1.1097385883331299
***** iteration: 1896 ******
loss: 0.9379761815071106
***** iteration: 1897 ******
loss: 0.6699153184890747
***** iteration: 1898 ******
loss: 0.5851595997810364
***** iteration: 1899 ******
loss: 0.3808230459690094
***** iteration: 1900 ******
loss: 1.0129377841949463
***** iteration: 1901 ******
loss: 1.474979281425476
***** iteration: 1902 ******
loss: 1.411749243736267
***** iteration: 1903 ******
loss: 0.8880774974822998
***** iteration: 1904 ******
loss: 0.4186391234397888
***** iteration: 1905 ******
loss: 0.44414690136909485
***** iteration: 1906 ******
loss: 0.554823637008667
***** iteration: 1907 ******
loss: 0.4883165955543518
***** iteration: 1908 ******
loss: 0.28255441784858704
***** iteration: 1909 ******
loss: 0.7245248556137085
***** iteration: 1910 ******
loss: 1.2798845767974854
***** iteration: 1911 ******
loss: 1.3131389617919922
***** iteration: 1912 ******
loss: 1.0160452127456665
***** iteration: 1913 ******
loss: 0.3858570456504822
***** iteration: 1914 ******
loss: 0.4768298864364624
***** iteration: 1915 ******
loss: 0.6530516147613525
***** iteration: 1916 ******
loss: 0.7047736048698425
***** iteration: 1917 ******
loss: 0.5977195501327515
***** iteration: 1918 ******
loss: 0.3953794836997986
***** iteration: 1919 ******
loss: 0.4404819905757904
***** iteration: 1920 ******
loss: 0.8248650431632996
***** iteration: 1921 ******
loss: 0.7955431938171387
***** iteration: 1922 ******
loss: 0.3957289457321167
***** iteration: 1923 ******
loss: 0.36620843410491943
***** iteration: 1924 ******
loss: 0.4640372097492218
***** iteration: 1925 ******
loss: 0.4322052299976349
***** iteration: 1926 ******
loss: 0.3231460154056549
***** iteration: 1927 ******
loss: 0.4638550579547882
***** iteration: 1928 ******
loss: 0.6654433608055115
***** iteration: 1929 ******
loss: 0.4876303970813751
***** iteration: 1930 ******
loss: 0.28472810983657837
***** iteration: 1931 ******
loss: 0.5905416011810303
***** iteration: 1932 ******
loss: 0.5129283666610718
***** iteration: 1933 ******
loss: 0.7857683300971985
***** iteration: 1934 ******
loss: 0.5464892387390137
***** iteration: 1935 ******
loss: 0.41405171155929565
***** iteration: 1936 ******
loss: 0.4425949454307556
***** iteration: 1937 ******
loss: 0.6897783875465393
***** iteration: 1938 ******
loss: 0.5498115420341492
***** iteration: 1939 ******
loss: 0.5192705392837524
***** iteration: 1940 ******
loss: 0.40408268570899963
***** iteration: 1941 ******
loss: 0.43911510705947876
***** iteration: 1942 ******
loss: 0.36456921696662903
***** iteration: 1943 ******
loss: 0.7280272245407104
***** iteration: 1944 ******
loss: 0.9357973337173462
***** iteration: 1945 ******
loss: 0.9979865550994873
***** iteration: 1946 ******
loss: 0.6684423685073853
***** iteration: 1947 ******
loss: 0.44587111473083496
***** iteration: 1948 ******
loss: 0.5052509903907776
***** iteration: 1949 ******
loss: 0.44661059975624084
***** iteration: 1950 ******
loss: 0.34752634167671204
***** iteration: 1951 ******
loss: 0.42433038353919983
***** iteration: 1952 ******
loss: 0.39360666275024414
***** iteration: 1953 ******
loss: 0.32545384764671326
***** iteration: 1954 ******
loss: 0.5406882762908936
***** iteration: 1955 ******
loss: 0.32288363575935364
***** iteration: 1956 ******
loss: 0.3276570737361908
***** iteration: 1957 ******
loss: 0.30256354808807373
***** iteration: 1958 ******
loss: 0.475151002407074
***** iteration: 1959 ******
loss: 0.3958692252635956
***** iteration: 1960 ******
loss: 0.3613203167915344
***** iteration: 1961 ******
loss: 0.265614777803421
***** iteration: 1962 ******
loss: 0.5965964794158936
***** iteration: 1963 ******
loss: 0.3884410858154297
***** iteration: 1964 ******
loss: 0.4453415870666504
***** iteration: 1965 ******
loss: 0.34060415625572205
***** iteration: 1966 ******
loss: 0.3492298424243927
***** iteration: 1967 ******
loss: 0.4079558551311493
***** iteration: 1968 ******
loss: 0.32038813829421997
***** iteration: 1969 ******
loss: 0.6062871217727661
***** iteration: 1970 ******
loss: 0.5344151854515076
***** iteration: 1971 ******
loss: 0.5811766982078552
***** iteration: 1972 ******
loss: 0.5475499629974365
***** iteration: 1973 ******
loss: 0.4039941728115082
***** iteration: 1974 ******
loss: 0.8513752222061157
***** iteration: 1975 ******
loss: 1.052121639251709
***** iteration: 1976 ******
loss: 0.7746447920799255
***** iteration: 1977 ******
loss: 0.5418386459350586
***** iteration: 1978 ******
loss: 0.4267255663871765
***** iteration: 1979 ******
loss: 0.525393009185791
***** iteration: 1980 ******
loss: 0.4768482446670532
***** iteration: 1981 ******
loss: 0.4073246717453003
***** iteration: 1982 ******
loss: 0.48216626048088074
***** iteration: 1983 ******
loss: 0.35869738459587097
***** iteration: 1984 ******
loss: 0.35295388102531433
***** iteration: 1985 ******
loss: 1.4266786575317383
***** iteration: 1986 ******
loss: 1.764878273010254
***** iteration: 1987 ******
loss: 0.38441112637519836
***** iteration: 1988 ******
loss: 0.3321182131767273
***** iteration: 1989 ******
loss: 0.3913094401359558
***** iteration: 1990 ******
loss: 0.39800718426704407
***** iteration: 1991 ******
loss: 0.3661985397338867
***** iteration: 1992 ******
loss: 0.3527827858924866
***** iteration: 1993 ******
loss: 0.2494635432958603
***** iteration: 1994 ******
loss: 0.726970374584198
***** iteration: 1995 ******
loss: 0.561260998249054
***** iteration: 1996 ******
loss: 0.522996187210083
***** iteration: 1997 ******
loss: 0.4783861041069031
***** iteration: 1998 ******
loss: 0.4169575572013855
***** iteration: 1999 ******
loss: 0.5452171564102173
***** iteration: 2000 ******
loss: 0.4070480763912201
***** iteration: 2001 ******
loss: 0.4974747598171234
***** iteration: 2002 ******
loss: 0.4983171224594116
***** iteration: 2003 ******
loss: 0.3925555348396301
***** iteration: 2004 ******
loss: 0.37695005536079407
***** iteration: 2005 ******
loss: 0.5157666802406311
***** iteration: 2006 ******
loss: 0.39424172043800354
***** iteration: 2007 ******
loss: 0.41266781091690063
***** iteration: 2008 ******
loss: 0.4500759541988373
***** iteration: 2009 ******
loss: 0.33906182646751404
***** iteration: 2010 ******
loss: 0.5001567006111145
***** iteration: 2011 ******
loss: 0.865526556968689
***** iteration: 2012 ******
loss: 0.628849983215332
***** iteration: 2013 ******
loss: 0.2842438817024231
***** iteration: 2014 ******
loss: 0.4436791241168976
***** iteration: 2015 ******
loss: 0.5198634266853333
***** iteration: 2016 ******
loss: 0.4332737326622009
***** iteration: 2017 ******
loss: 0.2762882113456726
***** iteration: 2018 ******
loss: 0.5938385128974915
***** iteration: 2019 ******
loss: 0.8347092866897583
***** iteration: 2020 ******
loss: 0.6873201131820679
***** iteration: 2021 ******
loss: 0.34658873081207275
***** iteration: 2022 ******
loss: 0.38933560252189636
***** iteration: 2023 ******
loss: 0.32734453678131104
***** iteration: 2024 ******
loss: 0.8429768681526184
***** iteration: 2025 ******
loss: 0.6676845550537109
***** iteration: 2026 ******
loss: 0.6613088250160217
***** iteration: 2027 ******
loss: 0.464560866355896
***** iteration: 2028 ******
loss: 0.4418758153915405
***** iteration: 2029 ******
loss: 0.5190362334251404
***** iteration: 2030 ******
loss: 0.44642284512519836
***** iteration: 2031 ******
loss: 0.4257946014404297
***** iteration: 2032 ******
loss: 0.44437700510025024
***** iteration: 2033 ******
loss: 0.2757903039455414
***** iteration: 2034 ******
loss: 1.7869354486465454
***** iteration: 2035 ******
loss: 2.5166664123535156
***** iteration: 2036 ******
loss: 1.2701398134231567
***** iteration: 2037 ******
loss: 0.8154293298721313
***** iteration: 2038 ******
loss: 0.8325352668762207
***** iteration: 2039 ******
loss: 0.6813741326332092
***** iteration: 2040 ******
loss: 0.4424682557582855
***** iteration: 2041 ******
loss: 1.406335473060608
***** iteration: 2042 ******
loss: 1.8775122165679932
***** iteration: 2043 ******
loss: 2.0820672512054443
***** iteration: 2044 ******
loss: 1.7661186456680298
***** iteration: 2045 ******
loss: 0.9991026520729065
***** iteration: 2046 ******
loss: 0.3128751218318939
***** iteration: 2047 ******
loss: 0.5319269895553589
***** iteration: 2048 ******
loss: 0.6507165431976318
***** iteration: 2049 ******
loss: 0.5945417881011963
***** iteration: 2050 ******
loss: 0.40158402919769287
***** iteration: 2051 ******
loss: 0.5040748715400696
***** iteration: 2052 ******
loss: 0.8753478527069092
***** iteration: 2053 ******
loss: 0.7911112904548645
***** iteration: 2054 ******
loss: 0.3741217851638794
***** iteration: 2055 ******
loss: 0.4544602930545807
***** iteration: 2056 ******
loss: 0.581190288066864
***** iteration: 2057 ******
loss: 0.5437667965888977
***** iteration: 2058 ******
loss: 0.3712112307548523
***** iteration: 2059 ******
loss: 0.5259892344474792
***** iteration: 2060 ******
loss: 0.88163161277771
***** iteration: 2061 ******
loss: 0.7794902920722961
***** iteration: 2062 ******
loss: 0.3633357584476471
***** iteration: 2063 ******
loss: 0.41822928190231323
***** iteration: 2064 ******
loss: 0.5407112836837769
***** iteration: 2065 ******
loss: 0.5014951229095459
***** iteration: 2066 ******
loss: 0.33724403381347656
***** iteration: 2067 ******
loss: 0.47328081727027893
***** iteration: 2068 ******
loss: 0.8203859925270081
***** iteration: 2069 ******
loss: 0.7146421670913696
***** iteration: 2070 ******
loss: 0.32569676637649536
***** iteration: 2071 ******
loss: 0.37844520807266235
***** iteration: 2072 ******
loss: 0.4963510036468506
***** iteration: 2073 ******
loss: 0.464316189289093
***** iteration: 2074 ******
loss: 0.36398133635520935
***** iteration: 2075 ******
loss: 0.29460054636001587
***** iteration: 2076 ******
loss: 0.5475761294364929
***** iteration: 2077 ******
loss: 0.4262019395828247
***** iteration: 2078 ******
loss: 0.3557218909263611
***** iteration: 2079 ******
loss: 0.4056514799594879
***** iteration: 2080 ******
loss: 0.6918508410453796
***** iteration: 2081 ******
loss: 0.46341103315353394
***** iteration: 2082 ******
loss: 0.3707631230354309
***** iteration: 2083 ******
loss: 0.49951162934303284
***** iteration: 2084 ******
loss: 0.8370535373687744
***** iteration: 2085 ******
loss: 0.9613343477249146
***** iteration: 2086 ******
loss: 0.6304148435592651
***** iteration: 2087 ******
loss: 0.414972186088562
***** iteration: 2088 ******
loss: 0.4557152986526489
***** iteration: 2089 ******
loss: 0.38174211978912354
***** iteration: 2090 ******
loss: 1.566269040107727
***** iteration: 2091 ******
loss: 1.4646046161651611
***** iteration: 2092 ******
loss: 0.7098824977874756
***** iteration: 2093 ******
loss: 0.6129019856452942
***** iteration: 2094 ******
loss: 0.40783634781837463
***** iteration: 2095 ******
loss: 0.5068577527999878
***** iteration: 2096 ******
loss: 0.5292815566062927
***** iteration: 2097 ******
loss: 0.40540456771850586
***** iteration: 2098 ******
loss: 0.40814074873924255
***** iteration: 2099 ******
loss: 0.49459508061408997
***** iteration: 2100 ******
loss: 0.5813751220703125
***** iteration: 2101 ******
loss: 0.36699748039245605
***** iteration: 2102 ******
loss: 0.41960811614990234
***** iteration: 2103 ******
loss: 0.47856470942497253
***** iteration: 2104 ******
loss: 0.3776629865169525
***** iteration: 2105 ******
loss: 0.6671997308731079
***** iteration: 2106 ******
loss: 0.515287458896637
***** iteration: 2107 ******
loss: 0.6423989534378052
***** iteration: 2108 ******
loss: 0.4928191602230072
***** iteration: 2109 ******
loss: 0.3931780457496643
***** iteration: 2110 ******
loss: 0.43606576323509216
***** iteration: 2111 ******
loss: 0.3653883635997772
***** iteration: 2112 ******
loss: 0.344536155462265
***** iteration: 2113 ******
loss: 0.6189939975738525
***** iteration: 2114 ******
loss: 0.6072908043861389
***** iteration: 2115 ******
loss: 0.2896358072757721
***** iteration: 2116 ******
loss: 0.336188942193985
***** iteration: 2117 ******
loss: 0.36609241366386414
***** iteration: 2118 ******
loss: 0.29035425186157227
***** iteration: 2119 ******
loss: 0.46892181038856506
***** iteration: 2120 ******
loss: 0.7184191942214966
***** iteration: 2121 ******
loss: 0.47716471552848816
***** iteration: 2122 ******
loss: 0.30704939365386963
***** iteration: 2123 ******
loss: 0.34999072551727295
***** iteration: 2124 ******
loss: 0.42090943455696106
***** iteration: 2125 ******
loss: 0.285938560962677
***** iteration: 2126 ******
loss: 0.49387067556381226
***** iteration: 2127 ******
loss: 0.6278440356254578
***** iteration: 2128 ******
loss: 0.40159493684768677
***** iteration: 2129 ******
loss: 0.4510851800441742
***** iteration: 2130 ******
loss: 0.5122420787811279
***** iteration: 2131 ******
loss: 0.40749698877334595
***** iteration: 2132 ******
loss: 0.252376914024353
***** iteration: 2133 ******
loss: 1.1704988479614258
***** iteration: 2134 ******
loss: 1.1760567426681519
***** iteration: 2135 ******
loss: 0.41325438022613525
***** iteration: 2136 ******
loss: 0.2943885326385498
***** iteration: 2137 ******
loss: 0.3745027780532837
***** iteration: 2138 ******
loss: 0.353701114654541
***** iteration: 2139 ******
loss: 0.29342055320739746
***** iteration: 2140 ******
loss: 0.4020410478115082
***** iteration: 2141 ******
loss: 0.34104421734809875
***** iteration: 2142 ******
loss: 0.34739112854003906
***** iteration: 2143 ******
loss: 0.35747167468070984
***** iteration: 2144 ******
loss: 0.32553842663764954
***** iteration: 2145 ******
loss: 0.40879619121551514
***** iteration: 2146 ******
loss: 0.3826296925544739
***** iteration: 2147 ******
loss: 0.3840009570121765
***** iteration: 2148 ******
loss: 0.3866942524909973
***** iteration: 2149 ******
loss: 0.29558831453323364
***** iteration: 2150 ******
loss: 1.2424561977386475
***** iteration: 2151 ******
loss: 1.7346652746200562
***** iteration: 2152 ******
loss: 0.46580126881599426
***** iteration: 2153 ******
loss: 0.3369550108909607
***** iteration: 2154 ******
loss: 0.47110414505004883
***** iteration: 2155 ******
loss: 0.5262396335601807
***** iteration: 2156 ******
loss: 0.43699580430984497
***** iteration: 2157 ******
loss: 0.6843670606613159
***** iteration: 2158 ******
loss: 0.8386960029602051
***** iteration: 2159 ******
loss: 0.5401905179023743
***** iteration: 2160 ******
loss: 0.3548966348171234
***** iteration: 2161 ******
loss: 0.4041132628917694
***** iteration: 2162 ******
loss: 0.56232750415802
***** iteration: 2163 ******
loss: 0.42799508571624756
***** iteration: 2164 ******
loss: 0.3080050051212311
***** iteration: 2165 ******
loss: 0.5174478888511658
***** iteration: 2166 ******
loss: 0.5393922924995422
***** iteration: 2167 ******
loss: 0.3219442665576935
***** iteration: 2168 ******
loss: 0.4809514880180359
***** iteration: 2169 ******
loss: 0.5944766998291016
***** iteration: 2170 ******
loss: 0.5913585424423218
***** iteration: 2171 ******
loss: 0.4866862893104553
***** iteration: 2172 ******
loss: 0.3800658881664276
***** iteration: 2173 ******
loss: 0.5251467227935791
***** iteration: 2174 ******
loss: 0.3722308576107025
***** iteration: 2175 ******
loss: 0.3448733687400818
***** iteration: 2176 ******
loss: 0.4742133319377899
***** iteration: 2177 ******
loss: 0.349888414144516
***** iteration: 2178 ******
loss: 0.3081214427947998
***** iteration: 2179 ******
loss: 0.30753472447395325
***** iteration: 2180 ******
loss: 0.31970566511154175
***** iteration: 2181 ******
loss: 0.31930604577064514
***** iteration: 2182 ******
loss: 0.27238330245018005
***** iteration: 2183 ******
loss: 0.49856576323509216
***** iteration: 2184 ******
loss: 0.5594565272331238
***** iteration: 2185 ******
loss: 0.32355934381484985
***** iteration: 2186 ******
loss: 0.3438076078891754
***** iteration: 2187 ******
loss: 0.4190472364425659
***** iteration: 2188 ******
loss: 0.3466758131980896
***** iteration: 2189 ******
loss: 0.2760275900363922
***** iteration: 2190 ******
loss: 0.54669189453125
***** iteration: 2191 ******
loss: 0.3220045566558838
***** iteration: 2192 ******
loss: 0.29882001876831055
***** iteration: 2193 ******
loss: 0.3104855418205261
***** iteration: 2194 ******
loss: 0.3060760796070099
***** iteration: 2195 ******
loss: 0.2849526107311249
***** iteration: 2196 ******
loss: 0.2624092698097229
***** iteration: 2197 ******
loss: 0.24291108548641205
***** iteration: 2198 ******
loss: 0.278572678565979
***** iteration: 2199 ******
loss: 0.4689217507839203
***** iteration: 2200 ******
loss: 0.29534971714019775
***** iteration: 2201 ******
loss: 0.40275120735168457
***** iteration: 2202 ******
loss: 0.37659886479377747
***** iteration: 2203 ******
loss: 0.2884557843208313
***** iteration: 2204 ******
loss: 0.5674052834510803
***** iteration: 2205 ******
loss: 0.2793729305267334
***** iteration: 2206 ******
loss: 0.37751123309135437
***** iteration: 2207 ******
loss: 0.38214823603630066
***** iteration: 2208 ******
loss: 0.4091262221336365
***** iteration: 2209 ******
loss: 0.4088151454925537
***** iteration: 2210 ******
loss: 0.33285027742385864
***** iteration: 2211 ******
loss: 0.27268508076667786
***** iteration: 2212 ******
loss: 2.382899045944214
***** iteration: 2213 ******
loss: 2.9739882946014404
***** iteration: 2214 ******
loss: 1.505835771560669
***** iteration: 2215 ******
loss: 0.5892627239227295
***** iteration: 2216 ******
loss: 0.5598544478416443
***** iteration: 2217 ******
loss: 0.4414612948894501
***** iteration: 2218 ******
loss: 0.5825961828231812
***** iteration: 2219 ******
loss: 0.46584224700927734
***** iteration: 2220 ******
loss: 0.492118239402771
***** iteration: 2221 ******
loss: 0.4847126603126526
***** iteration: 2222 ******
loss: 0.3301704525947571
***** iteration: 2223 ******
loss: 0.5045446157455444
***** iteration: 2224 ******
loss: 1.9526418447494507
***** iteration: 2225 ******
loss: 2.1792714595794678
***** iteration: 2226 ******
loss: 0.7498077750205994
***** iteration: 2227 ******
loss: 0.3471783995628357
***** iteration: 2228 ******
loss: 0.47894421219825745
***** iteration: 2229 ******
loss: 0.6156429648399353
***** iteration: 2230 ******
loss: 0.574837863445282
***** iteration: 2231 ******
loss: 0.43510112166404724
***** iteration: 2232 ******
loss: 0.7059635519981384
***** iteration: 2233 ******
loss: 0.7706844210624695
***** iteration: 2234 ******
loss: 0.5653814673423767
***** iteration: 2235 ******
loss: 0.5253320336341858
***** iteration: 2236 ******
loss: 0.41251930594444275
***** iteration: 2237 ******
loss: 0.43939822912216187
***** iteration: 2238 ******
loss: 0.3450411260128021
***** iteration: 2239 ******
loss: 0.6099709272384644
***** iteration: 2240 ******
loss: 0.7860408425331116
***** iteration: 2241 ******
loss: 0.6539088487625122
***** iteration: 2242 ******
loss: 0.2773652672767639
***** iteration: 2243 ******
loss: 0.4944944381713867
***** iteration: 2244 ******
loss: 0.613440990447998
***** iteration: 2245 ******
loss: 0.5567688941955566
***** iteration: 2246 ******
loss: 0.4050467312335968
***** iteration: 2247 ******
loss: 0.6901131868362427
***** iteration: 2248 ******
loss: 1.356534719467163
***** iteration: 2249 ******
loss: 0.7203095555305481
***** iteration: 2250 ******
loss: 0.9646459221839905
***** iteration: 2251 ******
loss: 0.5513781309127808
***** iteration: 2252 ******
loss: 0.44757696986198425
***** iteration: 2253 ******
loss: 0.48689353466033936
***** iteration: 2254 ******
loss: 0.40324491262435913
***** iteration: 2255 ******
loss: 0.7538371682167053
***** iteration: 2256 ******
loss: 0.9422819018363953
***** iteration: 2257 ******
loss: 0.648985743522644
***** iteration: 2258 ******
loss: 0.301504522562027
***** iteration: 2259 ******
loss: 1.243593454360962
***** iteration: 2260 ******
loss: 1.0711325407028198
***** iteration: 2261 ******
loss: 0.4279463589191437
***** iteration: 2262 ******
loss: 0.3744313716888428
***** iteration: 2263 ******
loss: 0.6078689098358154
***** iteration: 2264 ******
loss: 0.7914047837257385
***** iteration: 2265 ******
loss: 0.528695285320282
***** iteration: 2266 ******
loss: 0.37362396717071533
***** iteration: 2267 ******
loss: 0.3929208517074585
***** iteration: 2268 ******
loss: 0.4734083414077759
***** iteration: 2269 ******
loss: 0.3157517611980438
***** iteration: 2270 ******
loss: 0.3808833658695221
***** iteration: 2271 ******
loss: 0.38308918476104736
***** iteration: 2272 ******
loss: 0.39626821875572205
***** iteration: 2273 ******
loss: 0.32122722268104553
***** iteration: 2274 ******
loss: 0.4120897054672241
***** iteration: 2275 ******
loss: 1.1316428184509277
***** iteration: 2276 ******
loss: 0.5078426599502563
***** iteration: 2277 ******
loss: 0.26995012164115906
***** iteration: 2278 ******
loss: 0.5558291077613831
***** iteration: 2279 ******
loss: 0.517139732837677
***** iteration: 2280 ******
loss: 0.4818335473537445
***** iteration: 2281 ******
loss: 0.3110775351524353
***** iteration: 2282 ******
loss: 0.7339029312133789
***** iteration: 2283 ******
loss: 1.4258040189743042
***** iteration: 2284 ******
loss: 1.2734543085098267
***** iteration: 2285 ******
loss: 0.9355819225311279
***** iteration: 2286 ******
loss: 0.38250887393951416
***** iteration: 2287 ******
loss: 0.6064962148666382
***** iteration: 2288 ******
loss: 0.7879197597503662
***** iteration: 2289 ******
loss: 0.78597092628479
***** iteration: 2290 ******
loss: 0.6190126538276672
***** iteration: 2291 ******
loss: 0.38032424449920654
***** iteration: 2292 ******
loss: 0.6913558840751648
***** iteration: 2293 ******
loss: 1.0878583192825317
***** iteration: 2294 ******
loss: 1.0077338218688965
***** iteration: 2295 ******
loss: 0.5099411606788635
***** iteration: 2296 ******
loss: 0.4001398980617523
***** iteration: 2297 ******
loss: 0.5149048566818237
***** iteration: 2298 ******
loss: 0.503738284111023
***** iteration: 2299 ******
loss: 0.358743280172348
***** iteration: 2300 ******
loss: 0.4567972421646118
***** iteration: 2301 ******
loss: 0.6757274866104126
***** iteration: 2302 ******
loss: 0.4752175807952881
***** iteration: 2303 ******
loss: 0.3009936213493347
***** iteration: 2304 ******
loss: 0.34554409980773926
***** iteration: 2305 ******
loss: 0.3000366985797882
***** iteration: 2306 ******
loss: 0.4389195442199707
***** iteration: 2307 ******
loss: 0.5276272892951965
***** iteration: 2308 ******
loss: 0.33974069356918335
***** iteration: 2309 ******
loss: 0.3161984980106354
***** iteration: 2310 ******
loss: 0.3456000089645386
***** iteration: 2311 ******
loss: 0.4545816481113434
***** iteration: 2312 ******
loss: 0.2942093312740326
***** iteration: 2313 ******
loss: 0.35025402903556824
***** iteration: 2314 ******
loss: 0.3525755703449249
***** iteration: 2315 ******
loss: 0.3884407877922058
***** iteration: 2316 ******
loss: 0.34682098031044006
***** iteration: 2317 ******
loss: 0.6736717820167542
***** iteration: 2318 ******
loss: 0.6101894974708557
***** iteration: 2319 ******
loss: 0.6401179432868958
***** iteration: 2320 ******
loss: 0.4644702970981598
***** iteration: 2321 ******
loss: 0.46426069736480713
***** iteration: 2322 ******
loss: 0.5373727679252625
***** iteration: 2323 ******
loss: 0.4595707952976227
***** iteration: 2324 ******
loss: 0.43393486738204956
***** iteration: 2325 ******
loss: 0.4565902054309845
***** iteration: 2326 ******
loss: 0.2867324948310852
***** iteration: 2327 ******
loss: 0.5344541072845459
***** iteration: 2328 ******
loss: 0.6065789461135864
***** iteration: 2329 ******
loss: 0.6364630460739136
***** iteration: 2330 ******
loss: 0.4986698627471924
***** iteration: 2331 ******
loss: 0.2702881693840027
***** iteration: 2332 ******
loss: 1.7054240703582764
***** iteration: 2333 ******
loss: 2.5595877170562744
***** iteration: 2334 ******
loss: 1.9657939672470093
***** iteration: 2335 ******
loss: 1.9551421403884888
***** iteration: 2336 ******
loss: 1.4790987968444824
***** iteration: 2337 ******
loss: 0.6412994265556335
***** iteration: 2338 ******
loss: 0.5389209389686584
***** iteration: 2339 ******
loss: 0.7729797959327698
***** iteration: 2340 ******
loss: 0.81929612159729
***** iteration: 2341 ******
loss: 0.6962935328483582
***** iteration: 2342 ******
loss: 0.42354661226272583
***** iteration: 2343 ******
loss: 0.4467076063156128
***** iteration: 2344 ******
loss: 1.5828784704208374
***** iteration: 2345 ******
loss: 1.681860089302063
***** iteration: 2346 ******
loss: 1.2941116094589233
***** iteration: 2347 ******
loss: 0.9959158301353455
***** iteration: 2348 ******
loss: 0.4148203134536743
***** iteration: 2349 ******
loss: 0.5813978314399719
***** iteration: 2350 ******
loss: 0.7748973369598389
***** iteration: 2351 ******
loss: 0.7846986651420593
***** iteration: 2352 ******
loss: 0.6287813186645508
***** iteration: 2353 ******
loss: 0.3399072289466858
***** iteration: 2354 ******
loss: 0.8065443634986877
***** iteration: 2355 ******
loss: 1.6041234731674194
***** iteration: 2356 ******
loss: 1.8604974746704102
***** iteration: 2357 ******
loss: 1.9119704961776733
***** iteration: 2358 ******
loss: 1.5276905298233032
***** iteration: 2359 ******
loss: 0.7796885967254639
***** iteration: 2360 ******
loss: 0.4557606279850006
***** iteration: 2361 ******
loss: 0.6347017884254456
***** iteration: 2362 ******
loss: 0.658832848072052
***** iteration: 2363 ******
loss: 0.5178734064102173
***** iteration: 2364 ******
loss: 0.48533326387405396
***** iteration: 2365 ******
loss: 0.5137090682983398
***** iteration: 2366 ******
loss: 0.6889912486076355
***** iteration: 2367 ******
loss: 0.6120299696922302
***** iteration: 2368 ******
loss: 0.4293738603591919
***** iteration: 2369 ******
loss: 0.5163496136665344
***** iteration: 2370 ******
loss: 0.5623421669006348
***** iteration: 2371 ******
loss: 0.5070505142211914
***** iteration: 2372 ******
loss: 0.39151158928871155
***** iteration: 2373 ******
loss: 0.3543298542499542
***** iteration: 2374 ******
loss: 0.41144150495529175
***** iteration: 2375 ******
loss: 0.34128159284591675
***** iteration: 2376 ******
loss: 0.3390115797519684
***** iteration: 2377 ******
loss: 0.6243245601654053
***** iteration: 2378 ******
loss: 0.677680253982544
***** iteration: 2379 ******
loss: 0.32262831926345825
***** iteration: 2380 ******
loss: 0.31253132224082947
***** iteration: 2381 ******
loss: 0.31660544872283936
***** iteration: 2382 ******
loss: 0.3765871524810791
***** iteration: 2383 ******
loss: 0.350763201713562
***** iteration: 2384 ******
loss: 0.32912445068359375
***** iteration: 2385 ******
loss: 0.38009417057037354
***** iteration: 2386 ******
loss: 0.446055144071579
***** iteration: 2387 ******
loss: 0.3025430142879486
***** iteration: 2388 ******
loss: 0.43612179160118103
***** iteration: 2389 ******
loss: 0.5106450319290161
***** iteration: 2390 ******
loss: 0.41806694865226746
***** iteration: 2391 ******
loss: 0.28621694445610046
***** iteration: 2392 ******
loss: 0.4138653874397278
***** iteration: 2393 ******
loss: 0.34237825870513916
***** iteration: 2394 ******
loss: 0.3354479968547821
***** iteration: 2395 ******
loss: 0.33366262912750244
***** iteration: 2396 ******
loss: 0.4715679883956909
***** iteration: 2397 ******
loss: 0.30688250064849854
***** iteration: 2398 ******
loss: 0.2735386788845062
***** iteration: 2399 ******
loss: 0.3193162679672241
***** iteration: 2400 ******
loss: 0.32561159133911133
***** iteration: 2401 ******
loss: 0.2914826273918152
***** iteration: 2402 ******
loss: 0.3136054575443268
***** iteration: 2403 ******
loss: 0.29667070508003235
***** iteration: 2404 ******
loss: 0.30114176869392395
***** iteration: 2405 ******
loss: 0.37569278478622437
***** iteration: 2406 ******
loss: 0.33858153223991394
***** iteration: 2407 ******
loss: 0.5319026708602905
***** iteration: 2408 ******
loss: 0.6107772588729858
***** iteration: 2409 ******
loss: 0.35637521743774414
***** iteration: 2410 ******
loss: 0.35933011770248413
***** iteration: 2411 ******
loss: 0.39916566014289856
***** iteration: 2412 ******
loss: 0.34922459721565247
***** iteration: 2413 ******
loss: 0.34490054845809937
***** iteration: 2414 ******
loss: 0.3446800410747528
***** iteration: 2415 ******
loss: 0.36490297317504883
***** iteration: 2416 ******
loss: 0.4624691307544708
***** iteration: 2417 ******
loss: 0.31152263283729553
***** iteration: 2418 ******
loss: 0.3374898433685303
***** iteration: 2419 ******
loss: 0.34347257018089294
***** iteration: 2420 ******
loss: 0.2986597716808319
***** iteration: 2421 ******
loss: 0.2769802510738373
***** iteration: 2422 ******
loss: 1.2753026485443115
***** iteration: 2423 ******
loss: 1.114295482635498
***** iteration: 2424 ******
loss: 0.5326983332633972
***** iteration: 2425 ******
loss: 0.34558674693107605
***** iteration: 2426 ******
loss: 0.48280584812164307
***** iteration: 2427 ******
loss: 0.5686782598495483
***** iteration: 2428 ******
loss: 0.4827148914337158
***** iteration: 2429 ******
loss: 0.2951667606830597
***** iteration: 2430 ******
loss: 0.6761237978935242
***** iteration: 2431 ******
loss: 2.214193820953369
***** iteration: 2432 ******
loss: 2.0626189708709717
***** iteration: 2433 ******
loss: 0.8749816417694092
***** iteration: 2434 ******
loss: 0.4477408826351166
***** iteration: 2435 ******
loss: 0.438416063785553
***** iteration: 2436 ******
loss: 0.5874783992767334
***** iteration: 2437 ******
loss: 0.5598413348197937
***** iteration: 2438 ******
loss: 0.3841940760612488
***** iteration: 2439 ******
loss: 0.5741247534751892
***** iteration: 2440 ******
loss: 0.8419347405433655
***** iteration: 2441 ******
loss: 0.8150568008422852
***** iteration: 2442 ******
loss: 0.39922401309013367
***** iteration: 2443 ******
loss: 0.3179565966129303
***** iteration: 2444 ******
loss: 0.39062559604644775
***** iteration: 2445 ******
loss: 0.3237634301185608
***** iteration: 2446 ******
loss: 0.38597843050956726
***** iteration: 2447 ******
loss: 0.5195601582527161
***** iteration: 2448 ******
loss: 0.4094517230987549
***** iteration: 2449 ******
loss: 0.2635994553565979
***** iteration: 2450 ******
loss: 0.2878859341144562
***** iteration: 2451 ******
loss: 0.26773902773857117
***** iteration: 2452 ******
loss: 0.28318721055984497
***** iteration: 2453 ******
loss: 0.2904622554779053
***** iteration: 2454 ******
loss: 0.3248494565486908
***** iteration: 2455 ******
loss: 0.2808837294578552
***** iteration: 2456 ******
loss: 0.3512122631072998
***** iteration: 2457 ******
loss: 0.3779677748680115
***** iteration: 2458 ******
loss: 0.3402363359928131
***** iteration: 2459 ******
loss: 0.2458973377943039
***** iteration: 2460 ******
loss: 0.577201783657074
***** iteration: 2461 ******
loss: 0.4388807713985443
***** iteration: 2462 ******
loss: 0.28650832176208496
***** iteration: 2463 ******
loss: 0.40858301520347595
***** iteration: 2464 ******
loss: 0.4711809754371643
***** iteration: 2465 ******
loss: 0.3955991864204407
***** iteration: 2466 ******
loss: 0.2807749807834625
***** iteration: 2467 ******
loss: 0.3878069818019867
***** iteration: 2468 ******
loss: 0.32676762342453003
***** iteration: 2469 ******
loss: 0.3605303466320038
***** iteration: 2470 ******
loss: 0.3604769706726074
***** iteration: 2471 ******
loss: 0.28172311186790466
***** iteration: 2472 ******
loss: 1.4211302995681763
***** iteration: 2473 ******
loss: 1.2744009494781494
***** iteration: 2474 ******
loss: 0.2798321843147278
***** iteration: 2475 ******
loss: 0.3303305506706238
***** iteration: 2476 ******
loss: 0.2930726408958435
***** iteration: 2477 ******
loss: 0.3982897996902466
***** iteration: 2478 ******
loss: 0.46426627039909363
***** iteration: 2479 ******
loss: 0.2700944244861603
***** iteration: 2480 ******
loss: 0.36721184849739075
***** iteration: 2481 ******
loss: 0.465501993894577
***** iteration: 2482 ******
loss: 0.43035340309143066
***** iteration: 2483 ******
loss: 0.2986415922641754
***** iteration: 2484 ******
loss: 0.677299976348877
***** iteration: 2485 ******
loss: 0.9678332209587097
***** iteration: 2486 ******
loss: 0.8814836740493774
***** iteration: 2487 ******
loss: 0.42232415080070496
***** iteration: 2488 ******
loss: 0.3332425653934479
***** iteration: 2489 ******
loss: 0.4463188648223877
***** iteration: 2490 ******
loss: 0.3929762840270996
***** iteration: 2491 ******
loss: 0.3025132417678833
***** iteration: 2492 ******
loss: 0.5190727710723877
***** iteration: 2493 ******
loss: 0.5828859806060791
***** iteration: 2494 ******
loss: 0.3657241761684418
***** iteration: 2495 ******
loss: 0.4804231822490692
***** iteration: 2496 ******
loss: 0.5396842360496521
***** iteration: 2497 ******
loss: 0.43170684576034546
***** iteration: 2498 ******
loss: 1.094739556312561
***** iteration: 2499 ******
loss: 0.8355247378349304
***** iteration: 2500 ******
loss: 0.8178284764289856
***** iteration: 2501 ******
loss: 0.7833237051963806
***** iteration: 2502 ******
loss: 0.4415439963340759
***** iteration: 2503 ******
loss: 0.56782066822052
***** iteration: 2504 ******
loss: 0.6944881677627563
***** iteration: 2505 ******
loss: 0.6469489932060242
***** iteration: 2506 ******
loss: 0.4506152868270874
***** iteration: 2507 ******
loss: 0.4128243625164032
***** iteration: 2508 ******
loss: 0.6200445890426636
***** iteration: 2509 ******
loss: 1.1612404584884644
***** iteration: 2510 ******
loss: 0.7784438729286194
***** iteration: 2511 ******
loss: 0.3645796775817871
***** iteration: 2512 ******
loss: 0.3628264367580414
***** iteration: 2513 ******
loss: 0.3801408112049103
***** iteration: 2514 ******
loss: 0.39722079038619995
***** iteration: 2515 ******
loss: 0.34290528297424316
***** iteration: 2516 ******
loss: 0.8088812828063965
***** iteration: 2517 ******
loss: 0.8020130395889282
***** iteration: 2518 ******
loss: 0.5581659078598022
***** iteration: 2519 ******
loss: 0.5492426753044128
***** iteration: 2520 ******
loss: 0.42274242639541626
***** iteration: 2521 ******
loss: 0.743858277797699
***** iteration: 2522 ******
loss: 1.068451166152954
***** iteration: 2523 ******
loss: 0.9056705832481384
***** iteration: 2524 ******
loss: 0.46209248900413513
***** iteration: 2525 ******
loss: 0.43514981865882874
***** iteration: 2526 ******
loss: 0.5596403479576111
***** iteration: 2527 ******
loss: 0.5372158885002136
***** iteration: 2528 ******
loss: 0.3935616612434387
***** iteration: 2529 ******
loss: 0.6359270811080933
***** iteration: 2530 ******
loss: 1.0158953666687012
***** iteration: 2531 ******
loss: 0.9174473285675049
***** iteration: 2532 ******
loss: 0.4431959390640259
***** iteration: 2533 ******
loss: 0.416808545589447
***** iteration: 2534 ******
loss: 0.5211048722267151
***** iteration: 2535 ******
loss: 0.46538424491882324
***** iteration: 2536 ******
loss: 0.3449822664260864
***** iteration: 2537 ******
loss: 0.4830331802368164
***** iteration: 2538 ******
loss: 0.6199913620948792
***** iteration: 2539 ******
loss: 0.40931591391563416
***** iteration: 2540 ******
loss: 0.3737894296646118
***** iteration: 2541 ******
loss: 0.4027511179447174
***** iteration: 2542 ******
loss: 0.4660766124725342
***** iteration: 2543 ******
loss: 0.2849641740322113
***** iteration: 2544 ******
loss: 0.4730495512485504
***** iteration: 2545 ******
loss: 0.5549252033233643
***** iteration: 2546 ******
loss: 0.43295228481292725
***** iteration: 2547 ******
loss: 0.5520175099372864
***** iteration: 2548 ******
loss: 0.6161574125289917
***** iteration: 2549 ******
loss: 0.5454038977622986
***** iteration: 2550 ******
loss: 0.35043543577194214
***** iteration: 2551 ******
loss: 0.5420225262641907
***** iteration: 2552 ******
loss: 2.071458101272583
***** iteration: 2553 ******
loss: 2.112159490585327
***** iteration: 2554 ******
loss: 1.081264853477478
***** iteration: 2555 ******
loss: 0.6990256905555725
***** iteration: 2556 ******
loss: 0.37328627705574036
***** iteration: 2557 ******
loss: 0.46176666021347046
***** iteration: 2558 ******
loss: 0.4288979172706604
***** iteration: 2559 ******
loss: 0.2858109474182129
***** iteration: 2560 ******
loss: 1.7934868335723877
***** iteration: 2561 ******
loss: 2.705721855163574
***** iteration: 2562 ******
loss: 1.6581610441207886
***** iteration: 2563 ******
loss: 1.5709304809570312
***** iteration: 2564 ******
loss: 1.3113316297531128
***** iteration: 2565 ******
loss: 0.6672456860542297
***** iteration: 2566 ******
loss: 0.63088059425354
***** iteration: 2567 ******
loss: 0.8474347591400146
***** iteration: 2568 ******
loss: 0.8828893303871155
***** iteration: 2569 ******
loss: 0.7630318999290466
***** iteration: 2570 ******
loss: 0.5405124425888062
***** iteration: 2571 ******
loss: 0.8808656930923462
***** iteration: 2572 ******
loss: 1.2518458366394043
***** iteration: 2573 ******
loss: 1.1453187465667725
***** iteration: 2574 ******
loss: 0.6218579411506653
***** iteration: 2575 ******
loss: 0.3856358528137207
***** iteration: 2576 ******
loss: 0.537521481513977
***** iteration: 2577 ******
loss: 0.6481419205665588
***** iteration: 2578 ******
loss: 0.6642407774925232
***** iteration: 2579 ******
loss: 0.5184452533721924
***** iteration: 2580 ******
loss: 0.3674834370613098
***** iteration: 2581 ******
loss: 0.49211809039115906
***** iteration: 2582 ******
loss: 0.5020051598548889
***** iteration: 2583 ******
loss: 0.44356977939605713
***** iteration: 2584 ******
loss: 0.29505470395088196
***** iteration: 2585 ******
loss: 0.29955098032951355
***** iteration: 2586 ******
loss: 0.3322112560272217
***** iteration: 2587 ******
loss: 0.4013895094394684
***** iteration: 2588 ******
loss: 0.3330238461494446
***** iteration: 2589 ******
loss: 0.3278283178806305
***** iteration: 2590 ******
loss: 0.3338353633880615
***** iteration: 2591 ******
loss: 0.37548401951789856
***** iteration: 2592 ******
loss: 0.39347726106643677
***** iteration: 2593 ******
loss: 0.35916754603385925
***** iteration: 2594 ******
loss: 0.3904755115509033
***** iteration: 2595 ******
loss: 0.40183672308921814
***** iteration: 2596 ******
loss: 0.3374754786491394
***** iteration: 2597 ******
loss: 0.4417734444141388
***** iteration: 2598 ******
loss: 0.4951847791671753
***** iteration: 2599 ******
loss: 0.3960321545600891
***** iteration: 2600 ******
loss: 0.4076900780200958
***** iteration: 2601 ******
loss: 0.36471137404441833
***** iteration: 2602 ******
loss: 0.32482337951660156
***** iteration: 2603 ******
loss: 0.38946035504341125
***** iteration: 2604 ******
loss: 0.37115636467933655
***** iteration: 2605 ******
loss: 0.4416908323764801
***** iteration: 2606 ******
loss: 0.4373762607574463
***** iteration: 2607 ******
loss: 0.30202606320381165
***** iteration: 2608 ******
loss: 0.822817862033844
***** iteration: 2609 ******
loss: 1.307843565940857
***** iteration: 2610 ******
loss: 1.3183132410049438
***** iteration: 2611 ******
loss: 1.1531802415847778
***** iteration: 2612 ******
loss: 0.5689828395843506
***** iteration: 2613 ******
loss: 0.4589197635650635
***** iteration: 2614 ******
loss: 0.6288819909095764
***** iteration: 2615 ******
loss: 0.6247010827064514
***** iteration: 2616 ******
loss: 0.46925923228263855
***** iteration: 2617 ******
loss: 0.28444841504096985
***** iteration: 2618 ******
loss: 0.5474538207054138
***** iteration: 2619 ******
loss: 0.7343561053276062
***** iteration: 2620 ******
loss: 0.40286117792129517
***** iteration: 2621 ******
loss: 0.30281031131744385
***** iteration: 2622 ******
loss: 0.31722426414489746
***** iteration: 2623 ******
loss: 0.25656935572624207
***** iteration: 2624 ******
loss: 1.219731330871582
***** iteration: 2625 ******
loss: 1.5064784288406372
***** iteration: 2626 ******
loss: 0.28393834829330444
***** iteration: 2627 ******
loss: 0.3358038365840912
***** iteration: 2628 ******
loss: 0.411893367767334
***** iteration: 2629 ******
loss: 0.45462650060653687
***** iteration: 2630 ******
loss: 0.44726234674453735
***** iteration: 2631 ******
loss: 0.39999130368232727
***** iteration: 2632 ******
loss: 0.30300214886665344
***** iteration: 2633 ******
loss: 3.063305139541626
***** iteration: 2634 ******
loss: 5.303308010101318
***** iteration: 2635 ******
loss: 5.2818779945373535
***** iteration: 2636 ******
loss: 3.2103822231292725
***** iteration: 2637 ******
loss: 0.35863158106803894
***** iteration: 2638 ******
loss: 0.47226962447166443
***** iteration: 2639 ******
loss: 0.5744576454162598
***** iteration: 2640 ******
loss: 0.6697396636009216
***** iteration: 2641 ******
loss: 0.7082235217094421
***** iteration: 2642 ******
loss: 0.7551470398902893
***** iteration: 2643 ******
loss: 0.763605535030365
***** iteration: 2644 ******
loss: 0.7165529131889343
***** iteration: 2645 ******
loss: 0.6699420213699341
***** iteration: 2646 ******
loss: 0.6614462733268738
***** iteration: 2647 ******
loss: 0.5705092549324036
***** iteration: 2648 ******
loss: 0.44313815236091614
***** iteration: 2649 ******
loss: 0.3322969973087311
***** iteration: 2650 ******
loss: 0.8913453817367554
***** iteration: 2651 ******
loss: 2.1404426097869873
***** iteration: 2652 ******
loss: 1.466448187828064
***** iteration: 2653 ******
loss: 0.5533291101455688
***** iteration: 2654 ******
loss: 0.5121528506278992
***** iteration: 2655 ******
loss: 0.39353105425834656
***** iteration: 2656 ******
loss: 0.684741199016571
***** iteration: 2657 ******
loss: 0.7256215214729309
***** iteration: 2658 ******
loss: 0.5671153664588928
***** iteration: 2659 ******
loss: 0.3780026435852051
***** iteration: 2660 ******
loss: 0.36208415031433105
***** iteration: 2661 ******
loss: 0.3012477457523346
***** iteration: 2662 ******
loss: 0.3242628872394562
***** iteration: 2663 ******
loss: 0.32712772488594055
***** iteration: 2664 ******
loss: 0.32165879011154175
***** iteration: 2665 ******
loss: 0.39807018637657166
***** iteration: 2666 ******
loss: 0.40106695890426636
***** iteration: 2667 ******
loss: 0.32277899980545044
***** iteration: 2668 ******
loss: 0.3344154953956604
***** iteration: 2669 ******
loss: 0.3112068772315979
***** iteration: 2670 ******
loss: 0.4086180329322815
***** iteration: 2671 ******
loss: 0.3645946681499481
***** iteration: 2672 ******
loss: 0.3056069016456604
***** iteration: 2673 ******
loss: 0.2989109754562378
***** iteration: 2674 ******
loss: 0.3283211290836334
***** iteration: 2675 ******
loss: 0.2922931909561157
***** iteration: 2676 ******
loss: 0.2944526970386505
***** iteration: 2677 ******
loss: 0.32339754700660706
***** iteration: 2678 ******
loss: 0.2984127998352051
***** iteration: 2679 ******
loss: 0.744295597076416
***** iteration: 2680 ******
loss: 0.26995065808296204
***** iteration: 2681 ******
loss: 0.30081307888031006
***** iteration: 2682 ******
loss: 0.30879294872283936
***** iteration: 2683 ******
loss: 0.32195189595222473
***** iteration: 2684 ******
loss: 0.3062574565410614
***** iteration: 2685 ******
loss: 0.34222501516342163
***** iteration: 2686 ******
loss: 0.41990330815315247
***** iteration: 2687 ******
loss: 0.27942413091659546
***** iteration: 2688 ******
loss: 0.48325201869010925
***** iteration: 2689 ******
loss: 0.4743880331516266
***** iteration: 2690 ******
loss: 0.27209219336509705
***** iteration: 2691 ******
loss: 0.32139837741851807
***** iteration: 2692 ******
loss: 0.35621893405914307
***** iteration: 2693 ******
loss: 0.31580767035484314
***** iteration: 2694 ******
loss: 0.3102605938911438
***** iteration: 2695 ******
loss: 1.3684782981872559
***** iteration: 2696 ******
loss: 1.3303377628326416
***** iteration: 2697 ******
loss: 0.24387739598751068
***** iteration: 2698 ******
loss: 0.2926886975765228
***** iteration: 2699 ******
loss: 0.28236621618270874
***** iteration: 2700 ******
loss: 0.636537492275238
***** iteration: 2701 ******
loss: 0.4535596966743469
***** iteration: 2702 ******
loss: 0.5265507698059082
***** iteration: 2703 ******
loss: 0.2741779088973999
***** iteration: 2704 ******
loss: 0.2716303765773773
***** iteration: 2705 ******
loss: 0.40943023562431335
***** iteration: 2706 ******
loss: 0.4283263385295868
***** iteration: 2707 ******
loss: 0.27571120858192444
***** iteration: 2708 ******
loss: 0.335519939661026
***** iteration: 2709 ******
loss: 0.30521637201309204
***** iteration: 2710 ******
loss: 0.31978413462638855
***** iteration: 2711 ******
loss: 0.2844066619873047
***** iteration: 2712 ******
loss: 0.45961394906044006
***** iteration: 2713 ******
loss: 0.4476672112941742
***** iteration: 2714 ******
loss: 0.4167335629463196
***** iteration: 2715 ******
loss: 0.27087846398353577
***** iteration: 2716 ******
loss: 1.1807373762130737
***** iteration: 2717 ******
loss: 1.5064594745635986
***** iteration: 2718 ******
loss: 1.4210236072540283
***** iteration: 2719 ******
loss: 1.2428085803985596
***** iteration: 2720 ******
loss: 0.6326223611831665
***** iteration: 2721 ******
loss: 0.4766675531864166
***** iteration: 2722 ******
loss: 0.6438120007514954
***** iteration: 2723 ******
loss: 0.631881833076477
***** iteration: 2724 ******
loss: 0.4583985209465027
***** iteration: 2725 ******
loss: 0.34155818819999695
***** iteration: 2726 ******
loss: 0.5138062238693237
***** iteration: 2727 ******
loss: 0.7040497064590454
***** iteration: 2728 ******
loss: 0.3811376094818115
***** iteration: 2729 ******
loss: 0.29971498250961304
***** iteration: 2730 ******
loss: 0.32060977816581726
***** iteration: 2731 ******
loss: 0.3894006013870239
***** iteration: 2732 ******
loss: 0.32470306754112244
***** iteration: 2733 ******
loss: 0.28049275279045105
***** iteration: 2734 ******
loss: 0.4562060832977295
***** iteration: 2735 ******
loss: 0.28574874997138977
***** iteration: 2736 ******
loss: 0.2927459180355072
***** iteration: 2737 ******
loss: 0.3507229685783386
***** iteration: 2738 ******
loss: 0.34006980061531067
***** iteration: 2739 ******
loss: 0.3257640600204468
***** iteration: 2740 ******
loss: 0.277130126953125
***** iteration: 2741 ******
loss: 0.4092400372028351
***** iteration: 2742 ******
loss: 0.3374243676662445
***** iteration: 2743 ******
loss: 0.32076022028923035
***** iteration: 2744 ******
loss: 0.30929267406463623
***** iteration: 2745 ******
loss: 0.46222782135009766
***** iteration: 2746 ******
loss: 0.4722304046154022
***** iteration: 2747 ******
loss: 0.3067086637020111
***** iteration: 2748 ******
loss: 0.41787809133529663
***** iteration: 2749 ******
loss: 0.46393880248069763
***** iteration: 2750 ******
loss: 0.3673989176750183
***** iteration: 2751 ******
loss: 0.3732772171497345
***** iteration: 2752 ******
loss: 0.4661349356174469
***** iteration: 2753 ******
loss: 0.28102973103523254
***** iteration: 2754 ******
loss: 0.6351039409637451
***** iteration: 2755 ******
loss: 0.6222112774848938
***** iteration: 2756 ******
loss: 0.6596914529800415
***** iteration: 2757 ******
loss: 0.5354218482971191
***** iteration: 2758 ******
loss: 0.2878696024417877
***** iteration: 2759 ******
loss: 0.9840665459632874
***** iteration: 2760 ******
loss: 1.7874420881271362
***** iteration: 2761 ******
loss: 1.8225464820861816
***** iteration: 2762 ******
loss: 1.6457704305648804
***** iteration: 2763 ******
loss: 1.0095868110656738
***** iteration: 2764 ******
loss: 0.2860366106033325
***** iteration: 2765 ******
loss: 0.4395115375518799
***** iteration: 2766 ******
loss: 1.3865978717803955
***** iteration: 2767 ******
loss: 1.2499358654022217
***** iteration: 2768 ******
loss: 0.635890007019043
***** iteration: 2769 ******
loss: 0.5663207173347473
***** iteration: 2770 ******
loss: 0.34909138083457947
***** iteration: 2771 ******
loss: 0.7711985111236572
***** iteration: 2772 ******
loss: 1.2706118822097778
***** iteration: 2773 ******
loss: 1.266472578048706
***** iteration: 2774 ******
loss: 0.8098323345184326
***** iteration: 2775 ******
loss: 0.26140695810317993
***** iteration: 2776 ******
loss: 0.44743549823760986
***** iteration: 2777 ******
loss: 0.5583661794662476
***** iteration: 2778 ******
loss: 0.495292603969574
***** iteration: 2779 ******
loss: 0.2937609553337097
***** iteration: 2780 ******
loss: 0.6168408393859863
***** iteration: 2781 ******
loss: 1.1254932880401611
***** iteration: 2782 ******
loss: 1.1034895181655884
***** iteration: 2783 ******
loss: 0.7747136950492859
***** iteration: 2784 ******
loss: 0.29900655150413513
***** iteration: 2785 ******
loss: 0.480251282453537
***** iteration: 2786 ******
loss: 0.9051303863525391
***** iteration: 2787 ******
loss: 0.7767529487609863
***** iteration: 2788 ******
loss: 0.7988548278808594
***** iteration: 2789 ******
loss: 0.7060548663139343
***** iteration: 2790 ******
loss: 0.46709519624710083
***** iteration: 2791 ******
loss: 0.6378669738769531
***** iteration: 2792 ******
loss: 1.1659961938858032
***** iteration: 2793 ******
loss: 1.218299388885498
***** iteration: 2794 ******
loss: 0.8191101551055908
***** iteration: 2795 ******
loss: 0.3168427348136902
***** iteration: 2796 ******
loss: 0.47748154401779175
***** iteration: 2797 ******
loss: 0.9625328183174133
***** iteration: 2798 ******
loss: 0.7982553243637085
***** iteration: 2799 ******
loss: 0.8189823627471924
***** iteration: 2800 ******
loss: 0.6864454746246338
***** iteration: 2801 ******
loss: 0.41833341121673584
***** iteration: 2802 ******
loss: 0.6070905327796936
***** iteration: 2803 ******
loss: 1.179746389389038
***** iteration: 2804 ******
loss: 1.2652740478515625
***** iteration: 2805 ******
loss: 0.9314371943473816
***** iteration: 2806 ******
loss: 0.375245600938797
***** iteration: 2807 ******
loss: 0.37714481353759766
***** iteration: 2808 ******
loss: 0.5128867626190186
***** iteration: 2809 ******
loss: 0.4869786500930786
***** iteration: 2810 ******
loss: 0.33750656247138977
***** iteration: 2811 ******
loss: 0.4336293339729309
***** iteration: 2812 ******
loss: 1.0669381618499756
***** iteration: 2813 ******
loss: 0.9661356806755066
***** iteration: 2814 ******
loss: 0.7688783407211304
***** iteration: 2815 ******
loss: 0.4076715409755707
***** iteration: 2816 ******
loss: 0.5021228194236755
***** iteration: 2817 ******
loss: 0.6421017646789551
***** iteration: 2818 ******
loss: 0.6137707829475403
***** iteration: 2819 ******
loss: 0.4751526117324829
***** iteration: 2820 ******
loss: 0.3320566415786743
***** iteration: 2821 ******
loss: 0.6231569647789001
***** iteration: 2822 ******
loss: 0.8824577927589417
***** iteration: 2823 ******
loss: 0.7115396857261658
***** iteration: 2824 ******
loss: 0.49089497327804565
***** iteration: 2825 ******
loss: 0.5822821855545044
***** iteration: 2826 ******
loss: 0.6128436923027039
***** iteration: 2827 ******
loss: 0.48503977060317993
***** iteration: 2828 ******
loss: 0.6683327555656433
***** iteration: 2829 ******
loss: 0.818443775177002
***** iteration: 2830 ******
loss: 0.6733328104019165
***** iteration: 2831 ******
loss: 0.6847824454307556
***** iteration: 2832 ******
loss: 0.3968919515609741
***** iteration: 2833 ******
loss: 0.4535399079322815
***** iteration: 2834 ******
loss: 0.5603272914886475
***** iteration: 2835 ******
loss: 0.5086990594863892
***** iteration: 2836 ******
loss: 0.33309441804885864
***** iteration: 2837 ******
loss: 0.6601651310920715
***** iteration: 2838 ******
loss: 1.180789828300476
***** iteration: 2839 ******
loss: 1.4330484867095947
***** iteration: 2840 ******
loss: 1.3594002723693848
***** iteration: 2841 ******
loss: 0.8333162069320679
***** iteration: 2842 ******
loss: 0.41263800859451294
***** iteration: 2843 ******
loss: 0.5096516013145447
***** iteration: 2844 ******
loss: 0.502701997756958
***** iteration: 2845 ******
loss: 0.36939969658851624
***** iteration: 2846 ******
loss: 0.388862669467926
***** iteration: 2847 ******
loss: 0.5488843321800232
***** iteration: 2848 ******
loss: 0.6148386001586914
***** iteration: 2849 ******
loss: 0.38606247305870056
***** iteration: 2850 ******
loss: 0.4210265278816223
***** iteration: 2851 ******
loss: 0.48546552658081055
***** iteration: 2852 ******
loss: 0.39526331424713135
***** iteration: 2853 ******
loss: 0.4216223359107971
***** iteration: 2854 ******
loss: 0.4734036922454834
***** iteration: 2855 ******
loss: 0.5530121326446533
***** iteration: 2856 ******
loss: 0.4186449944972992
***** iteration: 2857 ******
loss: 0.3602338135242462
***** iteration: 2858 ******
loss: 0.3886512815952301
***** iteration: 2859 ******
loss: 0.3135048747062683
***** iteration: 2860 ******
loss: 0.510391354560852
***** iteration: 2861 ******
loss: 0.7043083906173706
***** iteration: 2862 ******
loss: 0.6761454343795776
***** iteration: 2863 ******
loss: 0.3743177354335785
***** iteration: 2864 ******
loss: 0.440563827753067
***** iteration: 2865 ******
loss: 0.5292736291885376
***** iteration: 2866 ******
loss: 0.6113801598548889
***** iteration: 2867 ******
loss: 0.4170074760913849
***** iteration: 2868 ******
loss: 0.315183162689209
***** iteration: 2869 ******
loss: 0.5169312357902527
***** iteration: 2870 ******
loss: 0.7816697955131531
***** iteration: 2871 ******
loss: 0.5133087635040283
***** iteration: 2872 ******
loss: 0.29529377818107605
***** iteration: 2873 ******
loss: 0.40004226565361023
***** iteration: 2874 ******
loss: 0.4895946979522705
***** iteration: 2875 ******
loss: 0.4590417742729187
***** iteration: 2876 ******
loss: 0.3282095789909363
***** iteration: 2877 ******
loss: 0.593548595905304
***** iteration: 2878 ******
loss: 0.8807178139686584
***** iteration: 2879 ******
loss: 0.8330748677253723
***** iteration: 2880 ******
loss: 0.427167683839798
***** iteration: 2881 ******
loss: 0.30900633335113525
***** iteration: 2882 ******
loss: 0.38818272948265076
***** iteration: 2883 ******
loss: 0.3383348882198334
***** iteration: 2884 ******
loss: 0.3038271367549896
***** iteration: 2885 ******
loss: 0.32361629605293274
***** iteration: 2886 ******
loss: 0.3584810495376587
***** iteration: 2887 ******
loss: 0.3419722020626068
***** iteration: 2888 ******
loss: 0.3253021836280823
***** iteration: 2889 ******
loss: 0.3490082621574402
***** iteration: 2890 ******
loss: 0.42065364122390747
***** iteration: 2891 ******
loss: 0.2944873571395874
***** iteration: 2892 ******
loss: 0.31364116072654724
***** iteration: 2893 ******
loss: 0.33891570568084717
***** iteration: 2894 ******
loss: 0.2998916804790497
***** iteration: 2895 ******
loss: 0.32360658049583435
***** iteration: 2896 ******
loss: 0.33140984177589417
***** iteration: 2897 ******
loss: 0.23794402182102203
***** iteration: 2898 ******
loss: 0.29267629981040955
***** iteration: 2899 ******
loss: 0.27068352699279785
***** iteration: 2900 ******
loss: 0.34533756971359253
***** iteration: 2901 ******
loss: 0.9162848591804504
***** iteration: 2902 ******
loss: 0.3309451937675476
***** iteration: 2903 ******
loss: 0.28339090943336487
***** iteration: 2904 ******
loss: 0.32970792055130005
***** iteration: 2905 ******
loss: 0.2947549819946289
***** iteration: 2906 ******
loss: 0.35276326537132263
***** iteration: 2907 ******
loss: 0.9903380870819092
***** iteration: 2908 ******
loss: 0.5770389437675476
***** iteration: 2909 ******
loss: 0.3003919720649719
***** iteration: 2910 ******
loss: 0.36045604944229126
***** iteration: 2911 ******
loss: 0.3201374113559723
***** iteration: 2912 ******
loss: 0.3788158595561981
***** iteration: 2913 ******
loss: 0.7011556029319763
***** iteration: 2914 ******
loss: 0.3280833065509796
***** iteration: 2915 ******
loss: 0.34729957580566406
***** iteration: 2916 ******
loss: 0.37895140051841736
***** iteration: 2917 ******
loss: 0.3100144565105438
***** iteration: 2918 ******
loss: 0.2911565899848938
***** iteration: 2919 ******
loss: 0.3290491998195648
***** iteration: 2920 ******
loss: 0.30848270654678345
***** iteration: 2921 ******
loss: 0.30220574140548706
***** iteration: 2922 ******
loss: 0.27592960000038147
***** iteration: 2923 ******
loss: 0.4368261396884918
***** iteration: 2924 ******
loss: 0.4534013569355011
***** iteration: 2925 ******
loss: 0.25243547558784485
***** iteration: 2926 ******
loss: 0.5130119919776917
***** iteration: 2927 ******
loss: 0.4237242043018341
***** iteration: 2928 ******
loss: 0.37128540873527527
***** iteration: 2929 ******
loss: 0.3466871678829193
***** iteration: 2930 ******
loss: 0.38376718759536743
***** iteration: 2931 ******
loss: 0.3797951936721802
***** iteration: 2932 ******
loss: 0.34060782194137573
***** iteration: 2933 ******
loss: 0.3215659260749817
***** iteration: 2934 ******
loss: 0.46764057874679565
***** iteration: 2935 ******
loss: 0.498881459236145
***** iteration: 2936 ******
loss: 0.31806913018226624
***** iteration: 2937 ******
loss: 0.40921124815940857
***** iteration: 2938 ******
loss: 0.43174654245376587
***** iteration: 2939 ******
loss: 0.3482455611228943
***** iteration: 2940 ******
loss: 0.4088062345981598
***** iteration: 2941 ******
loss: 0.9782951474189758
***** iteration: 2942 ******
loss: 0.46823883056640625
***** iteration: 2943 ******
loss: 0.3383300006389618
***** iteration: 2944 ******
loss: 0.37027451395988464
***** iteration: 2945 ******
loss: 0.3610855042934418
***** iteration: 2946 ******
loss: 0.31218311190605164
***** iteration: 2947 ******
loss: 0.5560927391052246
***** iteration: 2948 ******
loss: 0.7948680520057678
***** iteration: 2949 ******
loss: 0.3214976489543915
***** iteration: 2950 ******
loss: 0.28631865978240967
***** iteration: 2951 ******
loss: 0.29892435669898987
***** iteration: 2952 ******
loss: 0.2638795077800751
***** iteration: 2953 ******
loss: 0.25756800174713135
***** iteration: 2954 ******
loss: 0.7175741791725159
***** iteration: 2955 ******
loss: 0.324505478143692
***** iteration: 2956 ******
loss: 0.2518773674964905
***** iteration: 2957 ******
loss: 0.8087056279182434
***** iteration: 2958 ******
loss: 0.8023430109024048
***** iteration: 2959 ******
loss: 0.5522666573524475
***** iteration: 2960 ******
loss: 0.5224701762199402
***** iteration: 2961 ******
loss: 0.361713707447052
***** iteration: 2962 ******
loss: 0.9277222752571106
***** iteration: 2963 ******
loss: 1.3038461208343506
***** iteration: 2964 ******
loss: 1.1765050888061523
***** iteration: 2965 ******
loss: 0.6516591310501099
***** iteration: 2966 ******
loss: 0.4132757782936096
***** iteration: 2967 ******
loss: 0.5617600679397583
***** iteration: 2968 ******
loss: 0.6268303394317627
***** iteration: 2969 ******
loss: 0.5235856771469116
***** iteration: 2970 ******
loss: 0.3180124759674072
***** iteration: 2971 ******
loss: 0.9073744416236877
***** iteration: 2972 ******
loss: 1.6052311658859253
***** iteration: 2973 ******
loss: 1.4106714725494385
***** iteration: 2974 ******
loss: 1.1406217813491821
***** iteration: 2975 ******
loss: 0.5247766971588135
***** iteration: 2976 ******
loss: 0.5019209980964661
***** iteration: 2977 ******
loss: 0.6799143552780151
***** iteration: 2978 ******
loss: 0.678393542766571
***** iteration: 2979 ******
loss: 0.5149966478347778
***** iteration: 2980 ******
loss: 0.2736109793186188
***** iteration: 2981 ******
loss: 1.8743910789489746
***** iteration: 2982 ******
loss: 3.3402106761932373
***** iteration: 2983 ******
loss: 2.734271764755249
***** iteration: 2984 ******
loss: 1.8953346014022827
***** iteration: 2985 ******
loss: 1.79695725440979
***** iteration: 2986 ******
loss: 1.1369725465774536
***** iteration: 2987 ******
loss: 0.452043741941452
***** iteration: 2988 ******
loss: 0.7593133449554443
***** iteration: 2989 ******
loss: 0.9872690439224243
***** iteration: 2990 ******
loss: 1.0303525924682617
***** iteration: 2991 ******
loss: 0.9071286916732788
***** iteration: 2992 ******
loss: 0.6342566013336182
***** iteration: 2993 ******
loss: 0.29685986042022705
***** iteration: 2994 ******
loss: 1.1862939596176147
***** iteration: 2995 ******
loss: 1.9733967781066895
***** iteration: 2996 ******
loss: 2.2323033809661865
***** iteration: 2997 ******
loss: 2.0473592281341553
***** iteration: 2998 ******
loss: 1.4204444885253906
***** iteration: 2999 ******
loss: 0.5014216303825378
======== testing rumen/g__Prevotella_test_wrapper =========
x shape is: (127, 16384)
features shape is: (127, 16384)
subtypes shape is: (127,)
x_test.shape is: (127, 5) y_test.len is: 127
out_test is:
tensor([[ 3.1659, -2.5629, -5.5790],
        [ 3.3423, -2.6460, -5.6094],
        [ 3.5110, -2.7410, -5.5843],
        [ 3.0466, -2.5102, -5.5963],
        [ 3.3460, -2.6667, -5.6294],
        [ 3.3683, -2.6562, -5.5881],
        [ 2.8576, -2.3718, -5.5989],
        [ 3.0593, -2.4928, -5.5690],
        [ 3.1875, -2.5674, -5.6363],
        [ 2.6456, -2.2386, -5.6537],
        [ 3.2308, -2.6129, -5.6323],
        [ 3.0102, -2.4420, -5.6428],
        [ 3.0911, -2.4783, -5.7153],
        [ 3.1888, -2.5637, -5.6480],
        [ 2.9640, -2.3887, -5.6986],
        [ 3.1388, -2.5383, -5.6359],
        [ 2.7739, -2.2939, -5.7017],
        [ 3.6338, -2.8585, -5.5722],
        [ 3.1442, -2.5619, -5.5825],
        [ 3.2481, -2.5878, -5.6880],
        [ 3.3848, -2.6363, -5.6581],
        [ 3.3728, -2.6470, -5.6447],
        [ 2.9938, -2.4332, -5.5671],
        [ 3.5137, -2.7440, -5.6742],
        [ 3.2775, -2.6163, -5.5821],
        [ 3.2611, -2.5996, -5.5974],
        [ 3.3915, -2.6673, -5.6057],
        [ 3.3032, -2.6536, -5.5582],
        [ 3.2172, -2.5576, -5.6385],
        [ 3.4575, -2.7210, -5.5812],
        [ 2.7813, -2.3114, -5.6649],
        [ 3.4234, -2.6763, -5.6675],
        [ 2.6875, -2.2623, -5.5929],
        [ 3.1113, -2.5168, -5.6281],
        [ 3.5815, -2.7805, -5.6340],
        [ 3.3236, -2.5862, -5.6657],
        [ 2.7037, -2.2679, -5.6647],
        [ 3.2235, -2.5801, -5.5770],
        [ 2.9991, -2.4748, -5.6111],
        [ 3.2717, -2.6306, -5.6410],
        [ 3.1862, -2.5475, -5.6573],
        [ 3.3103, -2.6422, -5.6617],
        [ 3.1943, -2.5820, -5.5860],
        [ 3.2763, -2.6040, -5.5564],
        [ 3.1228, -2.5047, -5.6698],
        [ 2.7631, -2.3400, -5.6283],
        [ 2.9844, -2.4584, -5.6886],
        [ 3.3460, -2.6478, -5.5300],
        [ 3.2224, -2.5154, -5.6032],
        [ 3.0661, -2.4937, -5.5747],
        [ 2.9791, -2.4635, -5.6735],
        [ 3.2591, -2.5764, -5.6221],
        [ 3.1193, -2.4902, -5.6518],
        [ 3.1874, -2.5630, -5.6174],
        [ 3.1975, -2.5367, -5.6377],
        [ 3.3011, -2.6178, -5.6603],
        [ 3.3541, -2.6772, -5.7139],
        [ 3.3061, -2.5988, -5.6087],
        [ 3.3876, -2.6454, -5.6484],
        [ 3.2650, -2.5895, -5.6236],
        [ 3.2536, -2.5995, -5.5428],
        [ 3.4867, -2.7259, -5.6993],
        [ 3.2801, -2.6030, -5.6322],
        [ 3.2745, -2.6207, -5.5958],
        [ 2.7712, -2.2968, -5.6506],
        [ 2.8681, -2.3465, -5.7039],
        [ 2.7850, -2.2856, -5.7356],
        [ 3.5077, -2.7579, -5.5785],
        [ 3.1279, -2.5566, -5.6242],
        [ 3.1848, -2.5605, -5.5441],
        [ 2.7825, -2.3068, -5.6957],
        [ 3.6558, -2.8793, -5.4846],
        [ 3.5912, -2.8035, -5.6222],
        [ 3.3358, -2.6523, -5.6189],
        [ 3.3334, -2.6228, -5.6038],
        [ 3.3077, -2.5938, -5.6255],
        [ 2.7607, -2.2339, -5.6928],
        [ 3.2205, -2.5846, -5.6307],
        [ 2.9581, -2.4557, -5.5923],
        [ 3.5195, -2.5561, -5.7437],
        [ 2.8901, -2.3666, -5.6723],
        [ 3.2968, -2.6054, -5.5446],
        [ 3.3389, -2.6533, -5.6246],
        [ 3.2116, -2.5582, -5.6412],
        [ 3.1070, -2.4290, -5.7271],
        [ 2.7018, -2.2711, -5.6648],
        [ 3.3503, -2.6492, -5.5363],
        [ 3.1933, -2.5497, -5.5815],
        [ 3.2964, -2.6316, -5.6322],
        [ 2.9509, -2.3885, -5.7118],
        [ 3.4935, -2.7939, -5.5369],
        [ 2.9195, -2.4055, -5.5575],
        [ 2.9962, -2.4582, -5.6258],
        [ 2.8874, -2.3739, -5.6445],
        [ 3.1991, -2.5671, -5.6768],
        [ 3.3220, -2.6356, -5.6568],
        [ 2.3280, -2.0524, -5.7887],
        [ 2.7970, -2.2949, -5.7032],
        [ 3.3333, -2.6143, -5.6526],
        [ 2.9151, -2.4005, -5.6758],
        [ 3.1974, -2.5807, -5.6965],
        [ 3.3984, -2.6780, -5.6157],
        [ 3.2361, -2.5956, -5.7133],
        [ 3.2264, -2.6194, -5.6120],
        [ 3.2786, -2.6124, -5.6649],
        [ 3.3632, -2.6928, -5.5181],
        [ 3.3529, -2.6521, -5.6745],
        [ 3.1810, -2.5533, -5.6843],
        [ 3.8304, -2.9358, -5.6389],
        [ 3.3348, -2.6502, -5.6265],
        [ 3.3295, -2.6518, -5.5609],
        [ 3.2136, -2.5697, -5.6934],
        [ 2.9730, -2.4627, -5.6196],
        [ 3.1299, -2.5242, -5.6538],
        [ 3.0706, -2.4526, -5.6685],
        [ 3.2360, -2.5727, -5.7655],
        [ 3.3102, -2.6136, -5.6882],
        [ 2.8814, -2.3749, -5.6350],
        [ 3.1430, -2.5159, -5.6506],
        [ 2.7671, -2.3024, -5.5902],
        [ 3.1182, -2.5121, -5.6087],
        [ 3.1118, -2.5035, -5.6873],
        [ 2.7259, -2.3240, -5.6848],
        [ 3.3005, -2.6286, -5.6164],
        [ 3.0245, -2.4672, -5.6019],
        [ 3.4864, -2.7431, -5.6419],
        [ 3.3934, -2.6718, -5.6160]], grad_fn=<AddmmBackward>)
Number of rejected data: 0.00% (0/127)
Accuracy of non-rejected data: 0.00 % (0/127)
Test empirical 0-1-c risk: 1.000000
======== testing rumen/g__RC9_test_wrapper =========
x shape is: (52, 16384)
features shape is: (52, 16384)
subtypes shape is: (52,)
x_test.shape is: (52, 5) y_test.len is: 52
out_test is:
tensor([[ 3.0695, -2.4073, -5.7340],
        [ 3.0269, -2.4226, -5.9406],
        [ 2.9438, -2.4219, -5.7603],
        [ 2.8846, -2.2933, -5.7681],
        [ 3.1418, -2.5777, -5.7727],
        [ 3.0953, -2.4403, -5.7995],
        [ 2.8991, -2.3907, -5.8388],
        [ 3.2777, -2.6261, -5.7178],
        [ 3.3090, -2.5997, -5.9127],
        [ 3.1195, -2.5732, -5.7321],
        [ 3.1533, -2.5123, -5.8993],
        [ 3.2493, -2.6359, -5.6548],
        [ 3.2513, -2.6043, -5.8261],
        [ 3.0346, -2.4595, -5.7353],
        [ 3.6116, -2.8144, -5.6085],
        [ 2.3356, -2.1308, -5.7488],
        [ 3.0721, -2.5329, -5.7541],
        [ 3.3368, -2.6788, -5.6162],
        [ 3.1099, -2.5484, -5.6458],
        [ 3.1950, -2.5421, -5.8317],
        [ 3.0613, -2.4575, -5.9603],
        [ 2.9855, -2.4609, -5.6775],
        [ 2.9912, -2.4026, -5.7060],
        [ 3.2429, -2.6233, -5.6066],
        [ 2.9525, -2.4806, -5.7740],
        [ 2.4900, -2.2220, -5.6109],
        [ 3.0763, -2.4543, -5.8580],
        [ 3.2461, -2.5325, -5.7694],
        [ 2.6638, -2.3351, -5.7512],
        [ 3.1870, -2.5676, -5.7340],
        [ 3.1645, -2.5123, -5.8417],
        [ 2.8868, -2.3451, -5.8030],
        [ 3.2418, -2.5440, -5.6957],
        [ 3.4374, -2.6833, -5.7017],
        [ 3.1273, -2.4716, -5.7261],
        [ 3.0603, -2.4633, -5.7618],
        [ 3.3775, -2.6464, -5.6872],
        [ 3.5598, -2.7481, -5.7502],
        [ 2.8517, -2.3274, -5.8754],
        [ 3.1842, -2.5321, -5.8998],
        [ 2.7154, -2.3161, -5.7986],
        [ 3.1055, -2.4554, -5.7393],
        [ 3.5255, -2.7568, -5.7256],
        [ 2.9782, -2.5131, -5.6272],
        [ 3.2061, -2.5838, -5.7201],
        [ 3.0726, -2.4781, -5.8515],
        [ 3.1315, -2.5747, -5.7161],
        [ 3.6605, -2.8474, -5.5821],
        [ 3.1803, -2.5535, -5.8770],
        [ 2.8559, -2.3282, -5.8985],
        [ 3.3436, -2.6436, -5.7224],
        [ 2.2112, -2.1095, -5.8315]], grad_fn=<AddmmBackward>)
Number of rejected data: 0.00% (0/52)
Accuracy of non-rejected data: 0.00 % (0/52)
Test empirical 0-1-c risk: 1.000000
======== testing rumen/g__C941_test_wrapper =========
x shape is: (35, 16384)
features shape is: (35, 16384)
subtypes shape is: (35,)
x_test.shape is: (35, 5) y_test.len is: 35
out_test is:
tensor([[ 3.1287, -2.5353, -5.6805],
        [ 3.0509, -2.4508, -5.6616],
        [ 2.4676, -2.1489, -5.7316],
        [ 3.2384, -2.5912, -5.6737],
        [ 3.1000, -2.5215, -5.5458],
        [ 2.5198, -2.1388, -5.7744],
        [ 2.6591, -2.2720, -5.7625],
        [ 3.1092, -2.5406, -5.5161],
        [ 2.6607, -2.2391, -5.6933],
        [ 3.4986, -2.7565, -5.7005],
        [ 3.2524, -2.5863, -5.7268],
        [ 2.6585, -2.1797, -5.6128],
        [ 2.6135, -2.0016, -5.8785],
        [ 3.1020, -2.5341, -5.7064],
        [ 2.7061, -2.2724, -5.7799],
        [ 2.6650, -2.2496, -5.7194],
        [ 2.9946, -2.4690, -5.7200],
        [ 3.0257, -2.4679, -5.6311],
        [ 3.0067, -2.4629, -5.6779],
        [ 3.0808, -2.5136, -5.6939],
        [ 2.9936, -2.4350, -5.7612],
        [ 2.9024, -2.3813, -5.8256],
        [ 2.8484, -2.3672, -5.6867],
        [ 3.0855, -2.4814, -5.7640],
        [ 2.7523, -2.2728, -5.6620],
        [ 3.0814, -2.4996, -5.7181],
        [ 3.0046, -2.4460, -5.7116],
        [ 3.0036, -2.4536, -5.7756],
        [ 2.4806, -2.1031, -5.7950],
        [ 3.0355, -2.4829, -5.6710],
        [ 3.2874, -2.5601, -5.8381],
        [ 2.9731, -2.4531, -5.6792],
        [ 2.4696, -2.0827, -6.0049],
        [ 2.9487, -2.4225, -5.5545],
        [ 3.0902, -2.5308, -5.6192]], grad_fn=<AddmmBackward>)
Number of rejected data: 0.00% (0/35)
Accuracy of non-rejected data: 0.00 % (0/35)
Test empirical 0-1-c risk: 1.000000
======== testing rumen/f__Lachnospiraceae_test_wrapper =========
x shape is: (131, 16384)
features shape is: (131, 16384)
subtypes shape is: (131,)
x_test.shape is: (131, 5) y_test.len is: 131
out_test is:
tensor([[ 2.6217, -2.2551, -5.6840],
        [ 3.3532, -2.6152, -5.4570],
        [ 3.7687, -2.8108, -5.7712],
        [ 3.0756, -2.4538, -5.7747],
        [ 3.3330, -2.5691, -5.7973],
        [ 2.8726, -2.3613, -5.7463],
        [ 3.7532, -2.7563, -5.9471],
        [ 2.6993, -2.2627, -5.8370],
        [ 3.5535, -2.6699, -5.7744],
        [ 3.3634, -2.5832, -5.7734],
        [ 3.3027, -2.5263, -5.8020],
        [ 3.3212, -2.5153, -6.0067],
        [ 3.3132, -2.4618, -5.6986],
        [ 4.1029, -3.0176, -5.7709],
        [ 3.6244, -2.8249, -5.4522],
        [ 3.1291, -2.4940, -5.7772],
        [ 3.6946, -2.7697, -5.6625],
        [ 3.0874, -2.4655, -5.8502],
        [ 3.0461, -2.3592, -5.9716],
        [ 3.0708, -2.4581, -5.2707],
        [ 2.9822, -2.3551, -5.9367],
        [ 3.2899, -2.5446, -5.6101],
        [ 3.6405, -2.7518, -5.8697],
        [ 3.6006, -2.6279, -5.7565],
        [ 3.2825, -2.5624, -5.9802],
        [ 3.1844, -2.4866, -5.8133],
        [ 3.4274, -2.5757, -5.6878],
        [ 3.2852, -2.5788, -5.7923],
        [ 3.3550, -2.5696, -5.7211],
        [ 3.1295, -2.4874, -5.9649],
        [ 3.2844, -2.5439, -5.8152],
        [ 3.7204, -2.8277, -5.6301],
        [ 3.7515, -2.7617, -5.7142],
        [ 3.6168, -2.7113, -5.9215],
        [ 3.3533, -2.6560, -5.4548],
        [ 4.1719, -3.0611, -5.8982],
        [ 4.6858, -3.3740, -6.0375],
        [ 3.4605, -2.6952, -5.7389],
        [ 2.9873, -2.3675, -5.7667],
        [ 3.5288, -2.6726, -5.9456],
        [ 3.4587, -2.6564, -5.7246],
        [ 3.6549, -2.7849, -5.5701],
        [ 3.4074, -2.6532, -5.8744],
        [ 3.5698, -2.7332, -5.7394],
        [ 3.7302, -2.7458, -5.8461],
        [ 3.8962, -2.8742, -5.8355],
        [ 3.1887, -2.5794, -5.6163],
        [ 3.3887, -2.6565, -5.3627],
        [ 2.9472, -2.3366, -5.9825],
        [ 3.3253, -2.6027, -5.8416],
        [ 2.8460, -2.3381, -5.8589],
        [ 3.4011, -2.6065, -5.9434],
        [ 3.3612, -2.6204, -5.6330],
        [ 3.1249, -2.5377, -5.3151],
        [ 3.5570, -2.5887, -5.8285],
        [ 3.5221, -2.7138, -5.5586],
        [ 3.6003, -2.7333, -5.4546],
        [ 3.4757, -2.7379, -5.4337],
        [ 2.6503, -2.2359, -5.7952],
        [ 2.7186, -2.3189, -5.8117],
        [ 3.6423, -2.7696, -5.8569],
        [ 3.1774, -2.5417, -6.0378],
        [ 3.3881, -2.5434, -5.9857],
        [ 3.4846, -2.6876, -5.7120],
        [ 3.5999, -2.7605, -5.7882],
        [ 3.5779, -2.6557, -5.8798],
        [ 3.1566, -2.4762, -5.8684],
        [ 3.2841, -2.4879, -5.7379],
        [ 3.1393, -2.4785, -5.7703],
        [ 2.9699, -2.4229, -5.7379],
        [ 3.4679, -2.6387, -5.7825],
        [ 2.6805, -2.2350, -5.4445],
        [ 3.5082, -2.6673, -5.7001],
        [ 3.2847, -2.5543, -5.9963],
        [ 3.2733, -2.6125, -5.7561],
        [ 3.0290, -2.4326, -5.7568],
        [ 3.0358, -2.4269, -5.8866],
        [ 3.6491, -2.7812, -5.8756],
        [ 3.3310, -2.6391, -5.7777],
        [ 3.4741, -2.6610, -5.8641],
        [ 3.2821, -2.5597, -5.8341],
        [ 3.3050, -2.5618, -6.0481],
        [ 3.2461, -2.5030, -5.9370],
        [ 3.3920, -2.6183, -5.7988],
        [ 3.6464, -2.7339, -5.6608],
        [ 2.9720, -2.3939, -5.9185],
        [ 3.4325, -2.6856, -5.5084],
        [ 3.3983, -2.6273, -5.9430],
        [ 3.0689, -2.4294, -5.7060],
        [ 3.4361, -2.6285, -5.9623],
        [ 3.6642, -2.7880, -5.8527],
        [ 3.1864, -2.5204, -5.9554],
        [ 3.5549, -2.7274, -5.8730],
        [ 3.5462, -2.6433, -5.8039],
        [ 3.6241, -2.7616, -5.8972],
        [ 3.2111, -2.5503, -5.7857],
        [ 4.3109, -3.1610, -5.9195],
        [ 3.1888, -2.4619, -5.6093],
        [ 3.0969, -2.4758, -5.8891],
        [ 3.5336, -2.6882, -5.6659],
        [ 3.4753, -2.6528, -5.6675],
        [ 3.1665, -2.5026, -5.7241],
        [ 2.8157, -2.3535, -5.8483],
        [ 3.6718, -2.7284, -5.8022],
        [ 3.5884, -2.7557, -5.8532],
        [ 2.7534, -2.3221, -5.7638],
        [ 4.3926, -3.2717, -5.8818],
        [ 3.1141, -2.4187, -5.5610],
        [ 3.0074, -2.4204, -5.8180],
        [ 3.2936, -2.5226, -6.0429],
        [ 3.6072, -2.7192, -5.9471],
        [ 3.8417, -2.8699, -5.4961],
        [ 3.5252, -2.7586, -5.8015],
        [ 3.0672, -2.4048, -5.7516],
        [ 2.8462, -2.3507, -5.3622],
        [ 3.1701, -2.5306, -5.8723],
        [ 3.5732, -2.7233, -5.8748],
        [ 3.5218, -2.6527, -5.9771],
        [ 3.6515, -2.7625, -5.7032],
        [ 3.5469, -2.6782, -5.6465],
        [ 3.2175, -2.5642, -5.7459],
        [ 3.2253, -2.5521, -5.8690],
        [ 3.4720, -2.6846, -5.8241],
        [ 3.2382, -2.4821, -6.0162],
        [ 3.3886, -2.5907, -5.8876],
        [ 2.8110, -2.3266, -5.8982],
        [ 3.5262, -2.6993, -5.6950],
        [ 2.7642, -2.2523, -5.8242],
        [ 3.6855, -2.7725, -5.9778],
        [ 3.4914, -2.6854, -5.8657],
        [ 3.1487, -2.4400, -5.8225]], grad_fn=<AddmmBackward>)
Number of rejected data: 0.00% (0/131)
Accuracy of non-rejected data: 100.00 % (131/131)
Test empirical 0-1-c risk: 0.000000
======== testing rumen/g__Ruminococcus_E_test_wrapper =========
x shape is: (58, 16384)
features shape is: (58, 16384)
subtypes shape is: (58,)
x_test.shape is: (58, 5) y_test.len is: 58
out_test is:
tensor([[ 1.7257, -1.6774, -5.7351],
        [ 1.5907, -1.6178, -5.6912],
        [ 1.2531, -1.3609, -5.9664],
        [ 2.2467, -1.9870, -5.7814],
        [ 1.3021, -1.3932, -6.1247],
        [ 2.4183, -2.1007, -5.6865],
        [ 1.8304, -1.7136, -5.7179],
        [ 2.0057, -1.7901, -5.6761],
        [ 2.5901, -2.1887, -5.2649],
        [ 1.4970, -1.5126, -6.1041],
        [ 1.9231, -1.7985, -5.6161],
        [ 2.2559, -2.0231, -5.7469],
        [ 1.2098, -1.3042, -6.2376],
        [ 1.8804, -1.7254, -5.6324],
        [ 2.5568, -2.1607, -5.6018],
        [ 1.8561, -1.7344, -5.9563],
        [ 1.5358, -1.5397, -6.0238],
        [ 2.1312, -1.8868, -6.0321],
        [ 1.4573, -1.4994, -5.7319],
        [ 1.4972, -1.5343, -5.6889],
        [ 1.6565, -1.6444, -5.9466],
        [ 2.7534, -2.3003, -5.7934],
        [ 2.1204, -1.8970, -5.5737],
        [ 3.7033, -2.7555, -5.7113],
        [ 2.0044, -1.8647, -5.6812],
        [ 2.0945, -1.8905, -5.7688],
        [ 1.8150, -1.7441, -5.8063],
        [ 1.2170, -1.3164, -6.1288],
        [ 1.7216, -1.6234, -6.0996],
        [ 1.8547, -1.7659, -5.8168],
        [ 2.0321, -1.8001, -5.7188],
        [ 1.6330, -1.5797, -6.1201],
        [ 1.8023, -1.7357, -5.7505],
        [ 2.4959, -2.1193, -5.5638],
        [ 1.6506, -1.6322, -5.6655],
        [ 2.5115, -2.1351, -5.6604],
        [ 1.7734, -1.6898, -5.6513],
        [ 1.4543, -1.4357, -6.1935],
        [ 1.8130, -1.7199, -5.6872],
        [ 1.7554, -1.7264, -5.8714],
        [ 1.7989, -1.7241, -5.7077],
        [ 2.0829, -1.8974, -5.5454],
        [ 1.5101, -1.5383, -6.1190],
        [ 2.0932, -1.8714, -5.6669],
        [ 2.9010, -2.3484, -5.6903],
        [ 1.7011, -1.6233, -6.1415],
        [ 1.6783, -1.5686, -6.1604],
        [ 1.8762, -1.7965, -5.6980],
        [ 2.1037, -1.9065, -5.8425],
        [ 1.4297, -1.4254, -6.2770],
        [ 1.3703, -1.4362, -6.1508],
        [ 1.7503, -1.7041, -5.6388],
        [ 1.8781, -1.7431, -5.7569],
        [ 1.8733, -1.7125, -5.7077],
        [ 1.7365, -1.7073, -5.7816],
        [ 1.3402, -1.3923, -6.1610],
        [ 1.2599, -1.3607, -6.2035],
        [ 2.2578, -1.9655, -5.5511]], grad_fn=<AddmmBackward>)
Number of rejected data: 0.00% (0/58)
Accuracy of non-rejected data: 100.00 % (58/58)
Test empirical 0-1-c risk: 0.000000
